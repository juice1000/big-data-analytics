# Tag 1: Big Data fundamentals + Multi-source data integration

---

## ğŸ¯ **Ziele und Erwartungen der Teilnehmenden**

### **Lernziel-Framework fÃ¼r Banking Analytics**

```
Business Goals â†’ Technical Skills â†’ Practical Application
     â†“                â†“                    â†“
- Fraud Detection   - Spark Mastery    - Real Banking Data
- Customer Insights - ML Algorithms    - Production Pipeline
- Risk Assessment   - LLM Integration  - Cloud Deployment

```

### **Erwartungsmanagement**

- **Praxisrelevanz:** Jede Ãœbung mit Banking-Kontext
- **Skalierbarkeit:** Von lokaler Entwicklung bis Cloud-Production

---

## ğŸ“Š **Grundlagen von Big Data**

### **Die Evolution der Datenlandschaft**

### **Traditionelle Datenverarbeitung (1990er-2000er)**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   OLTP      â”‚    â”‚ Data Warehouse â”‚    â”‚  Reporting  â”‚
â”‚ (Transact.) â”‚ -> â”‚  (Nightly ETL) â”‚ -> â”‚ (Business)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   â€¢ MB-GB Daten     â€¢ Strukturiert      â€¢ Batch-Reports
   â€¢ Einzelne Server â€¢ Feste Schemas     â€¢ Historische Sicht

```

OTLP = Online Transaction Processing (executing a large number of small, concurrent transactions, typically in real-time)

OLTP = Online Transaction Processing - das System, das Ihre tÃ¤glichen BankgeschÃ¤fte wie Ãœberweisungen und KontostÃ¤nde in Echtzeit verarbeitet. Denken Sie an das System, das lÃ¤uft, wenn Sie eine Karte in den Geldautomaten stecken.

Data Warehouse = Ein zentraler Speicherort fÃ¼r bereinigte, strukturierte Daten aus verschiedenen Quellen. Stellen Sie sich vor wie eine gut organisierte Bibliothek, wo alle BÃ¼cher (Daten) katalogisiert und an festen PlÃ¤tzen stehen.

ETL = Extract, Transform, Load - der traditionelle Prozess zur Datenverarbeitung:
â€¢ Extract: Daten aus verschiedenen Quellen sammeln
â€¢ Transform: Daten bereinigen und in das gewÃ¼nschte Format bringen
â€¢ Load: Bereinigte Daten in das Data Warehouse laden

### **Big Data Ã„ra (2010er-heute)**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Multi-Sourceâ”‚    â”‚  Data Lake  â”‚    â”‚ Real-time   â”‚
â”‚ (Streaming) â”‚ -> â”‚ (All Data)  â”‚ -> â”‚ Analytics   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   â€¢ TB-PB Daten     â€¢ Alle Formate     â€¢ Live Insights
   â€¢ Verteilte Sys.  â€¢ Schema-on-Read   â€¢ Predictive

```

Data Lake = Ein riesiger Speicher fÃ¼r Daten in ihrem ursprÃ¼nglichen Format. Stellen Sie sich vor wie ein groÃŸer See, in den verschiedene FlÃ¼sse (Datenquellen) mÃ¼nden. Die Daten werden erst spÃ¤ter strukturiert, wenn man sie braucht.

Schema-on-Read = Die Struktur wird erst beim Lesen der Daten festgelegt, nicht beim Speichern. Das ist wie ein flexibles Lagersystem - Sie kÃ¶nnen verschiedene GegenstÃ¤nde einlagern und spÃ¤ter entscheiden, wie Sie sie sortieren mÃ¶chten.

## **ğŸ“ Grundlegende Begriffe einfach erklÃ¤rt**

Bevor wir in die technischen Details eintauchen, klÃ¤ren wir die wichtigsten Begriffe mit einfachen Analogien:

### **ğŸ“– Big Data Glossar**

- Data Warehouse = Eine gut organisierte Bibliothek, wo alle BÃ¼cher (Daten) katalogisiert und an festen PlÃ¤tzen stehen. Perfekt fÃ¼r geplante Recherche, aber unflexibel fÃ¼r spontane Fragen.
- Data Lake = Ein riesiger Speicher fÃ¼r Daten in ihrem ursprÃ¼nglichen Format. Wie ein groÃŸer See, in den verschiedene FlÃ¼sse (Datenquellen) mÃ¼nden. Die Daten werden erst spÃ¤ter strukturiert, wenn man sie braucht.
- ETL (Extract, Transform, Load) = Traditioneller Prozess: Daten sammeln â†’ bereinigen â†’ in Warehouse laden. Wie BÃ¼cher sortieren, bevor sie ins Regal kommen.
- Schema-on-Read vs Schema-on-Write = Bei Schema-on-Write definieren Sie die Struktur beim Speichern (wie vorgedruckte Formulare). Bei Schema-on-Read entscheiden Sie erst beim Lesen, wie Sie die Daten interpretieren (wie leere NotizblÃ¤tter, die Sie spÃ¤ter unterschiedlich verwenden).
- Verteilte Systeme = Statt einem groÃŸen Computer nutzen Sie viele kleine Computer zusammen. Wie ein Orchester: Jeder Musiker spielt seinen Teil, zusammen entsteht die Symphonie.

### **ğŸ¦ Banking-spezifische Begriffe**

- OLTP (Online Transaction Processing) = Das System fÃ¼r tÃ¤gliche BankgeschÃ¤fte wie Ãœberweisungen und KontostÃ¤nde. Optimiert fÃ¼r viele kleine, schnelle Transaktionen - wie ein Supermarkt-Kassensystem.
- OLAP (Online Analytical Processing) = Das System fÃ¼r Analysen und Berichte. Optimiert fÃ¼r komplexe Fragen Ã¼ber groÃŸe Datenmengen - wie ein Forschungslabor.
- Fraud Detection = Betrugserkennung. Systeme, die verdÃ¤chtige Transaktionen in Echtzeit identifizieren - wie ein digitaler Detektiv, der 24/7 auf VerdÃ¤chtiges achtet.
- Risk Assessment = Risikobewertung. Berechnung der Wahrscheinlichkeit, dass ein Kredit nicht zurÃ¼ckgezahlt wird - wie ein digitaler Wahrsager fÃ¼r Kreditentscheidungen.

### **Die 5 V's von Big Data**

Beschreiben Charakteristiken, die Big Data innehat.

![image.png](Tag%201%20Big%20Data%20fundamentals%20+%20Multi-source%20data%20in%2024a7a6147595806a8bfeec3805da3e08/image.png)

### **1. Volume (Datenmenge)**

**Problem:** Exponentielles Datenwachstum

**Banking-Beispiele:**

- **Deutsche Bank:** ~1 Milliarde Transaktionen/Tag
- **PayPal:** 19 Millionen Transaktionen/Tag
- **Visa:** 65.000 Transaktionen/Sekunde

**Technische Herausforderungen:**

```python
# Traditionelle Limits
pandas_limit = "~8GB RAM Limit"
single_machine = "Vertical Scaling Costs"

# Big Data LÃ¶sung
spark_distributed = "Horizontal Scaling"
cloud_elastic = "Pay-as-you-scale"

```

Stellen Sie sich vor: Wenn Sie mit Excel arbeiten, werden alle Daten in den Arbeitsspeicher (RAM) Ihres Computers geladen. Moderne Computer haben oft 8-16GB RAM, was bedeutet, dass Sie maximal 2-3GB an Daten verarbeiten kÃ¶nnen.

Bei Big Data haben wir aber DatensÃ¤tze, die 1000x grÃ¶ÃŸer sind! Da mÃ¼ssen wir die Daten auf viele Computer verteilen - so wie ein groÃŸes Lager, das auf mehrere GebÃ¤ude aufgeteilt wird.

Cloud-Elastic = Sie zahlen nur fÃ¼r die Rechenleistung, die Sie gerade brauchen. Wie bei einem Taxi - Sie zahlen nur fÃ¼r die gefahrenen Kilometer, nicht fÃ¼r das ganze Auto.

Pandas = Eine Python-Bibliothek fÃ¼r Datenanalyse, die in den Arbeitsspeicher (RAM) eines einzelnen Computers geladen wird. Bei 8GB RAM sind etwa 2-3GB fÃ¼r Daten verfÃ¼gbar - das reicht fÃ¼r kleine bis mittlere Datasets.

Vertical Scaling (Vertikal skalieren) = Den Computer stÃ¤rker machen: Mehr RAM, schnellere CPU, grÃ¶ÃŸere Festplatte. Wird schnell sehr teuer.

Horizontal Scaling (Horizontal skalieren) = Mehr Computer verwenden: Statt einem 32GB-Computer nutzen Sie vier 8GB-Computer zusammen. GÃ¼nstiger und flexibler.

### **2. Velocity (Geschwindigkeit)**

**Problem:** Real-time Verarbeitung wird zur GeschÃ¤ftsanforderung

**Banking Real-time Requirements:**

- **Fraud Detection:** < 100ms Entscheidung
- **Credit Approval:** < 5 Sekunden
- **Market Trading:** < 1ms (High-Frequency Trading)
- **Mobile Banking:** < 200ms Response

**Architektur-Implikationen:**

```
Batch Processing    vs.    Stream Processing
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚Data arrives â”‚           â”‚Data arrives â”‚
â”‚in batches   â”‚           â”‚continuously â”‚
â”‚             â”‚           â”‚             â”‚
â”‚Process everyâ”‚           â”‚Process in   â”‚
â”‚X hours      â”‚           â”‚real-time    â”‚
â”‚             â”‚           â”‚             â”‚
â”‚High latency â”‚           â”‚Low latency  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

```

### **3. Variety (Vielfalt)**

**Problem:** Daten kommen in verschiedenen Formaten

**Banking Data Sources:**

```
Strukturiert (20%):        Semi-strukturiert (30%):    Unstrukturiert (50%):
â€¢ Transaktionsdaten      â€¢ JSON APIs                 â€¢ E-Mails
â€¢ Kundenstammdaten       â€¢ XML Nachrichten           â€¢ Chat-Logs
â€¢ KontostÃ¤nde            â€¢ Log-Dateien               â€¢ Dokumente
â€¢ Kredithistorie         â€¢ Web-Clickstreams          â€¢ Audio-Calls

```

**Schema Evolution Challenge:**

```sql
-- Traditional: Schema-on-Write
CREATE TABLE transactions (
    id INTEGER,
    amount DECIMAL(10,2),
    currency CHAR(3)
);

-- Big Data: Schema-on-Read
-- Flexibel: Neue Felder ohne Schema-Ã„nderung
{
  "id": 123,
  "amount": 99.99,
  "currency": "EUR",
  "metadata": {...}  // Neue Felder jederzeit mÃ¶glich
}

```

### **4. Veracity (Wahrhaftigkeit)**

**Problem:** DatenqualitÃ¤t variiert stark

**Banking Data Quality Issues:**

- **Incomplete Data:** 15-20% fehlende Werte in Customer Data
- **Inconsistent Formats:** Datum, WÃ¤hrung, Namen
- **Duplicate Records:** 5-10% bei KundenzusammenfÃ¼hrungen
- **Outdated Information:** Adressen, Telefonnummern

**Quality Impact on Business:**

```
Schlechte DatenqualitÃ¤t â†’ Falsche Insights â†’ Schlechte Entscheidungen

Beispiel: Credit Scoring
â€¢ 30% fehlerhafte Einkommensdaten
â€¢ 15% veraltete BeschÃ¤ftigungsinformationen
â†’ 25% falsche Kreditentscheidungen
â†’ â‚¬50M jÃ¤hrlicher Verlust (GroÃŸbank)

```

### **5. Value (Wert)**

**Problem:** Nur ein Bruchteil der Daten ist wertvoll

**Banking Value Extraction:**

```
Raw Data (100%) â†’ Processed (30%) â†’ Insights (5%) â†’ Action (1%)

Beispiel: Fraud Detection
â€¢ 1M tÃ¤gliche Transaktionen
â€¢ 10,000 verdÃ¤chtige Muster
â€¢ 100 echte BetrugsfÃ¤lle
â€¢ 50 verhinderte Verluste = â‚¬500,000 tÃ¤glich gerettete Summe

```

### **Herausforderungen bei der Big Data Verarbeitung**

- **Speicherung:**Â Ein Server kann maximal 100TB speichern
- **Geschwindigkeit:**Â Ein Computer prÃ¼ft 10.000 DatensÃ¤tze/Sekunde
- **FlexibilitÃ¤t:**Â Schema-Ã„nderungen dauern Monate
- **Kosten:**Â StÃ¤rkere Hardware wird exponentiell teurer

LÃ¶sungen:

- **Verteilte Speicherung:**Â 1000 Server mit je 100TB = 100PB
- **Parallele Verarbeitung:**Â 1000 Computer prÃ¼fen 10M DatensÃ¤tze/Sekunde
- **Schema-on-Read:**Â Struktur beim Lesen bestimmen
- **Horizontal Skalierung:**Â Mehr gÃ¼nstige Computer statt teure Hardware

### **1. Speicherung**

**Problem:** Petabyte-Scale Storage Management

**Traditionelle Limits:**

- **Single Server:** Maximale FestplattenkapazitÃ¤t
- **SAN/NAS Architekturen:** Kostenexplosion bei groÃŸen Datenmengen
- **Backup:** UnmÃ¶gliche Backup-Zeiten

SAN = Storage Area Network (unabhÃ¤ngiges Speichernetzwerk, wird Ã¼ber Switch)

![image.png](Tag%201%20Big%20Data%20fundamentals%20+%20Multi-source%20data%20in%2024a7a6147595806a8bfeec3805da3e08/image%201.png)

NAS = Network Attached Storage (Teil des Netzwerkes)

![image.png](Tag%201%20Big%20Data%20fundamentals%20+%20Multi-source%20data%20in%2024a7a6147595806a8bfeec3805da3e08/image%202.png)

DAS = Direct Attached Storage (z.B. an den PC)

![image.png](Tag%201%20Big%20Data%20fundamentals%20+%20Multi-source%20data%20in%2024a7a6147595806a8bfeec3805da3e08/image%203.png)

**Big Data LÃ¶sung:**

```
Distributed File Systems (HDFS)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Node 1  â”‚ â”‚ Node 2  â”‚ â”‚ Node 3  â”‚ â”‚ Node 4  â”‚
â”‚ 10TB    â”‚ â”‚ 10TB    â”‚ â”‚ 10TB    â”‚ â”‚ 10TB    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚           â”‚           â”‚           â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚           â”‚
        Replicated Data (3x Copy)

```

### **2. Verarbeitungsgeschwindigkeit**

**Problem:** Traditionelle Algorithmen skalieren nicht

**Complexity Analysis:**

**Denken Sie an Algorithmus-Effizienz wie beim AufrÃ¤umen einer Bibliothek:**

â€¢ **Linear (O(n)):** Sie schauen jedes Buch einzeln durch - bei 1 Million BÃ¼chern schauen Sie 1 Million mal
â€¢ **Quadratisch (O(nÂ²)):** Sie vergleichen jedes Buch mit jedem anderen - bei 1 Million BÃ¼chern sind das 1 Billion Vergleiche! Das dauert ewig.
â€¢ **Logarithmisch (O(log n)):** Sie nutzen ein intelligentes Ordnungssystem - selbst bei 1 Million BÃ¼chern brauchen Sie nur 20 Schritte

**Big Data LÃ¶sung:** Statt einen Bibliothekar alle BÃ¼cher durchsuchen zu lassen, teilen Sie die Arbeit auf 1000 Bibliothekare auf. Jeder sucht nur 1000 BÃ¼cher durch - parallel und viel schneller!

### **3. DatenqualitÃ¤t bei Scale**

**Problem:** Quality Checks werden zum Bottleneck

**Traditional Quality Checks:**

```python
# Single-threaded: 10,000 records/second
def validate_record(record):
    check_completeness(record)
    check_format(record)
    check_business_rules(record)
    return is_valid

# Time fÃ¼r 1B records: 100,000 seconds (28 hours!)

```

**Stellen Sie sich vor: Ein Sicherheitsbeamter Ã¼berprÃ¼ft jeden Besucher einzeln**

Traditionelle DatenqualitÃ¤t ist wie ein einzelner Sicherheitsbeamter, der jeden Besucher eines riesigen Konzerts einzeln Ã¼berprÃ¼ft. Bei 10.000 Besuchern schafft er vielleicht 1000 pro Stunde - das dauert 10 Stunden!

**Big Data LÃ¶sung: 100 Sicherheitsbeamte arbeiten parallel**

Spark ist wie 100 Sicherheitsbeamte, die gleichzeitig arbeiten. Jeder Ã¼berprÃ¼ft 100 Besucher - das dauert nur 6 Minuten! So kann Spark 1 Million DatensÃ¤tze pro Sekunde Ã¼berprÃ¼fen, wÃ¤hrend ein traditionelles System nur 10.000 schafft.

**Distributed Quality Checks:**

```python
# Spark: 1,000,000 records/second (1000x faster)
df.filter(col("amount") > 0)\
  .filter(col("currency").isin(["EUR", "USD", "GBP"]))\
  .filter(col("timestamp").isNotNull())

```

## **Chancen und Potenziale von Big Data**

### **1. Erkenntnisgewinn durch Skalierung**

**Statistical Power durch grÃ¶ÃŸere Samples:**

```
Sample Size Impact auf Confidence:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Sample Size  â”‚ Margin Errorâ”‚ Confidence   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1,000        â”‚ Â±3.1%       â”‚ 95%          â”‚
â”‚ 10,000       â”‚ Â±1.0%       â”‚ 95%          â”‚
â”‚ 1,000,000    â”‚ Â±0.1%       â”‚ 95%          â”‚
â”‚ 100,000,000  â”‚ Â±0.01%      â”‚ 95%          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

```

**Banking Insights Examples:**

- **Micro-Segmentation:** 10,000 Customer segments statt 10
- **Real-time Patterns:** Fraud detection in Millisekunden
- **Predictive Accuracy:** 95% Genauigkeit bei KreditausfÃ¤llen

### **2. Verbesserte Entscheidungsfindung**

**From Reactive to Predictive:**

```
Traditional Banking          Big Data Banking
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Historical      â”‚   â†’     â”‚ Predictive      â”‚
â”‚ Analysis        â”‚         â”‚ Analytics       â”‚
â”‚                 â”‚         â”‚                 â”‚
â”‚ "What happened?"â”‚         â”‚ "What will      â”‚
â”‚                 â”‚         â”‚  happen?"       â”‚
â”‚                 â”‚         â”‚                 â”‚
â”‚ Monthly Reports â”‚         â”‚ Real-time       â”‚
â”‚                 â”‚         â”‚ Dashboards      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

```

**Decision Speed Impact:**

- **Credit Decisions:** 2 Wochen â†’ 2 Minuten
- **Fraud Detection:** 24 Stunden â†’ 100 Millisekunden
- **Market Response:** 1 Woche â†’ Real-time

### **3. Personalisierte Dienstleistungen**

**Hyper-Personalization durch Big Data:**

```python
# Traditional: Segment-based
if customer.age > 65:
    offer_pension_products()

# Big Data: Individual-based
if customer.spending_pattern.matches("young_family") and \
   customer.savings_rate > 0.15 and \
   customer.location.near("good_schools"):
    offer_education_savings_plan()

```

---

## ğŸ—ï¸ **Big Data-Technologien und -Plattformen**

### **Hadoop-Ã–kosystem: Die Foundation**

**Stellen Sie sich Hadoop wie ein riesiges Lager vor:**

- **HDFS (Hadoop Distributed File System)** = Das Lagersystem selbst. Statt einem riesigen Lagerhaus haben Sie 1000 kleine Lagerhallen, die alle miteinander verbunden sind. Wenn eine abbrennt, haben Sie die Waren trotzdem noch in 2 anderen Hallen (automatische Sicherheitskopien).
- **MapReduce** = Die Arbeiter im Lager. Jeder Arbeiter bekommt einen kleinen Teil der Aufgabe (Map), dann werden alle Ergebnisse zusammengefÃ¼gt (Reduce). Wie eine Inventur: 1000 Arbeiter zÃ¤hlen je ein Regal, dann wird alles zusammengerechnet.
- **YARN** = Der Lagerverwalter. Er entscheidet, welcher Arbeiter welche Aufgabe bekommt und sorgt dafÃ¼r, dass alles effizient lÃ¤uft.

### **Hadoop Architektur Overview**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                Hadoop Ecosystem                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Applications: Hive, Pig, Mahout, HBase, Flume     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Processing: MapReduce, Spark, Tez                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Resource Management: YARN                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Storage: HDFS (Hadoop Distributed File System)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

```

### **HDFS (Hadoop Distributed File System)**

**Das verteilte Dateisystem fÃ¼r Big Data**

**Kernprinzipien:**

```
1. Massive Skalierung:
   â€¢ Designed fÃ¼r Petabytes
   â€¢ Commodity Hardware
   â€¢ Linear scaling

2. Fault Tolerance:
   â€¢ Automatische Replikation (3x default)
   â€¢ Self-healing bei Node-AusfÃ¤llen
   â€¢ No Single Point of Failure

3. High Throughput:
   â€¢ Optimiert fÃ¼r groÃŸe Files (>64MB)
   â€¢ Sequential Access Pattern
   â€¢ Bandwidth over Latency

```

**HDFS Architektur:**

```
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚ NameNode    â”‚ â† Metadaten Master
                â”‚ (Master)    â”‚   (Filesystem Namespace)
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚              â”‚              â”‚
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚ DataNode 1  â”‚ â”‚ DataNode 2  â”‚ â”‚ DataNode 3  â”‚
 â”‚             â”‚ â”‚             â”‚ â”‚             â”‚
 â”‚ Block A-1   â”‚ â”‚ Block A-2   â”‚ â”‚ Block A-3   â”‚
 â”‚ Block B-1   â”‚ â”‚ Block B-2   â”‚ â”‚ Block C-1   â”‚
 â”‚ Block C-2   â”‚ â”‚ Block C-3   â”‚ â”‚ Block B-3   â”‚
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

```

**Banking Use Case:**

```python
# Speicherung von 5 Jahren Transaktionsdaten
# 1 Milliarde Transaktionen/Tag Ã— 365 Tage Ã— 5 Jahre = 1.8 Trillion Records
# @ 200 Bytes/Record = 360 TB Raw Data

hdfs_storage = {
    "raw_transactions": "120 TB",
    "processed_data": "90 TB",
    "ml_features": "60 TB",
    "models_backups": "30 TB",
    "total_with_replication": "900 TB"  # 3x replication
}

```

### **MapReduce-Programmierung**

**Das ursprÃ¼ngliche Big Data Processing Model**

**MapReduce Paradigma:**

```python
Input â†’ MAP â†’ Shuffle & Sort â†’ REDUCE â†’ Output

Beispiel: Fraud Detection Count
Input: [transaction1, transaction2, ..., transactionN]

MAP Phase:
transaction1 â†’ ("fraud", 1) if is_suspicious(transaction1) else ("normal", 1)
transaction2 â†’ ("fraud", 1) if is_suspicious(transaction2) else ("normal", 1)

REDUCE Phase:
("fraud", [1,1,1,...]) â†’ ("fraud", total_count)
("normal", [1,1,1,...]) â†’ ("normal", total_count)

```

**MapReduce Limitations (Warum Spark besser ist):**

```
MapReduce Problems:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. Disk-based: Slow I/O between jobs    â”‚
â”‚ 2. Batch-only: No real-time processing  â”‚
â”‚ 3. Complex: Multi-step jobs difficult   â”‚
â”‚ 4. No Interactive: Each query from disk â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Spark Solutions:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. In-Memory: 10-100x faster                   â”‚
â”‚ 2. Unified: Batch + Stream + ML                â”‚
â”‚ 3. Simple: High-level APIs                     â”‚
â”‚ 4. Interactive: REPL (Read-Evaluate-Print-Loop)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

```

## **Apache Spark: Die moderne Big Data Engine**

### **ğŸ† Apache Spark: Unified Framework**

*Data Collection â†’ Cleaning â†’ Analysis â†’ Feature Engineering â†’ 
Model Training â†’ Evaluation & Validation â†’ Serving*

**Spark ist wie ein modernes Hochleistungs-Logistikzentrum:**

- **In-Memory Computing** = Statt jedes Paket zwischenzulagern, behalten die Arbeiter alles im Kopf (Arbeitsspeicher). 10-100x schneller als stÃ¤ndiges Hin- und HerrÃ¤umen.
- **Unified Analytics** = Ein System fÃ¼r alles: normale Lagerarbeit, EchtzeitÃ¼berwachung, QualitÃ¤tskontrolle und Vorhersagen. FrÃ¼her brauchten Sie 4 verschiedene Systeme.
- **Spark SQL** = Sie kÃ¶nnen mit normaler Sprache (SQL) fragen: 'Zeig mir alle roten Produkte aus 2024' statt komplizierte Arbeitsbefehle zu geben.
- **Spark Streaming** = Ãœberwachung in Echtzeit. Jedes neue Paket wird sofort registriert und verarbeitet, nicht erst am Ende des Tages.

### **Warum Spark MapReduce ablÃ¶st**

**Performance Comparison:**

```
Benchmark: 100GB Daten sortieren

MapReduce:    1600 Sekunden
Spark (Disk): 200 Sekunden  (8x schneller)
Spark (Memory): 23 Sekunden (70x schneller)

Iterative ML Algorithm (10 Iterations):
MapReduce:    30 Minuten (jede Iteration from disk)
Spark:        2 Minuten (data cached in memory)

```

### Best of Both Worlds

Eine Kombination aus beidem ist auch mÃ¶glich

```python
                   +---------------------------+
                   |        Clients            |
                   |  (Spark jobs / Notebooks) |
                   +-------------+-------------+
                                 |
                                 v
+---------------------------------------------------------------+
|                        Resource Layer                         |
|                    (YARN / Kubernetes*)                       |
|   Schedulers allocate executors & containers for Spark jobs   |
+---------------------------+-----------------------------------+
                            |
                            v
+---------------------------------------------------------------+
|                         Compute Layer                         |
|                         Apache Spark                          |
|  - Spark SQL / DataFrames  - MLlib  - Structured Streaming    |
|  - Executors run close to data (data locality)                |
+---------------------------+-----------------------------------+
                            |
                            v
+---------------------------------------------------------------+
|                         Storage Layer                         |
|     HDFS  (or Ozone / S3-compatible object storage**)         |
|  - Parquet / ORC / Avro files                                 |
|  - Replication & fault-tolerance                              |
+---------------------------+-----------------------------------+
                            |
                            v
+---------------------------------------------------------------+
|                      Metastore & Tables                       |
|        Apache Hive Metastore (Glue OK too)                    |
|  - External tables over Parquet/ORC in HDFS                   |
|  - Accessed via Spark SQL (Hive catalog)                      |
+---------------------------+-----------------------------------+
                            |
                            v
+---------------------------------------------------------------+
|                     NoSQL / Serving (optional)                |
|        HBase / Cassandra / Elasticsearch                      |
|  - Low-latency lookups / serving layer                        |
+---------------------------------------------------------------+
```

### **Spark Core Concepts**

**1. In-Memory Computing:**

```
Traditional MapReduce:          Apache Spark:
â”Œâ”€â”€â”€â”€â”€â”                        â”Œâ”€â”€â”€â”€â”€â”
â”‚ Job1â”‚ â†’ Disk â†’ â”Œâ”€â”€â”€â”€â”€â”       â”‚ Job1â”‚ â†˜
â””â”€â”€â”€â”€â”€â”˜          â”‚ Job2â”‚       â””â”€â”€â”€â”€â”€â”˜  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”
                 â””â”€â”€â”€â”€â”€â”˜                â”‚ Memory â”‚
                      â†“                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              Disk â†’ â”Œâ”€â”€â”€â”€â”€â”                 â†—
                     â”‚ Job3â”‚       â”Œâ”€â”€â”€â”€â”€â” â†—
                     â””â”€â”€â”€â”€â”€â”˜       â”‚ Job2â”‚
                                   â””â”€â”€â”€â”€â”€â”˜

```

Spark ist ein Lazy-Excecution Framework.

**2. Unified Analytics Platform:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                Spark Applications                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Spark SQL â”‚ Streaming â”‚ MLlib â”‚ GraphX â”‚ R/Python   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚              Spark Core Engine                      â”‚
â”‚         (RDDs, DAG Scheduler, Memory Mgmt)          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚           Cluster Resource Manager                  â”‚
â”‚      (YARN, Mesos, Kubernetes, Standalone)         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

```

### Programmiersprachen mit Spark

![Screenshot 2025-08-10 at 18.52.48.png](Tag%201%20Big%20Data%20fundamentals%20+%20Multi-source%20data%20in%2024a7a6147595806a8bfeec3805da3e08/Screenshot_2025-08-10_at_18.52.48.png)

### Spark-Integrationen

![Screenshot 2025-08-10 at 18.49.04.png](Tag%201%20Big%20Data%20fundamentals%20+%20Multi-source%20data%20in%2024a7a6147595806a8bfeec3805da3e08/Screenshot_2025-08-10_at_18.49.04.png)

### **Spark SQL: Distributed SQL**

**SQL-Interface fÃ¼r Big Data**

```python
# Spark SQL Example: Banking Fraud Analysis
from pyspark.sql import SparkSession
from pyspark.sql.functions import *

spark = SparkSession.builder.appName("FraudDetection").getOrCreate()

# Load banking data
transactions = spark.read.parquet("hdfs://banking/transactions/")

# Register as SQL table
transactions.createOrReplaceTempView("transactions")

# Complex fraud detection query
fraud_patterns = spark.sql("""
    SELECT
        customer_id,
        COUNT(*) as transaction_count,
        SUM(amount) as total_amount,
        AVG(amount) as avg_amount,
        STDDEV(amount) as amount_volatility,
        COUNT(DISTINCT merchant) as unique_merchants,
        COUNT(DISTINCT HOUR(timestamp)) as active_hours
    FROM transactions
    WHERE date(timestamp) = current_date()
    GROUP BY customer_id
    HAVING
        transaction_count > 50 OR
        amount_volatility > 1000 OR
        unique_merchants > 20
    ORDER BY amount_volatility DESC
""")

```

**Apache Pig:**

ETL Pipelines auf semi/unstructured Daten

```sql
-- Pig Latin: Dataflow Language
transactions = LOAD 'hdfs://transactions.csv' USING PigStorage(',')
    AS (id:chararray, customer:chararray, amount:double, timestamp:chararray);

filtered = FILTER transactions BY amount > 1000.0;
grouped = GROUP filtered BY customer;
aggregated = FOREACH grouped GENERATE
    group AS customer,
    COUNT(filtered) AS high_value_count;

STORE aggregated INTO 'hdfs://high_value_customers';

```

### **Spark Streaming: Real-time Processing**

**Micro-Batch Architecture fÃ¼r Low-Latency**

```python
# Spark Streaming: Real-time Fraud Detection
from pyspark.streaming import StreamingContext

# Create streaming context (2 second batches)
ssc = StreamingContext(spark.sparkContext, 2)

# Connect to Kafka stream
kafka_stream = KafkaUtils.createStream(ssc,
    zkQuorum="localhost:2181",
    groupId="fraud-detection",
    topics={"transactions": 1})

# Process each micro-batch
def process_transactions(time, rdd):
    if not rdd.isEmpty():
        # Convert to DataFrame
        df = spark.read.json(rdd)

        # Apply fraud detection model
        predictions = fraud_model.transform(df)

        # Filter high-risk transactions
        high_risk = predictions.filter(col("fraud_probability") > 0.8)

        # Send alerts
        high_risk.foreach(send_fraud_alert)

kafka_stream.foreachRDD(process_transactions)
ssc.start()
ssc.awaitTermination()

```

### **Apache Flink: Stream-First Processing**

### **Flink vs. Spark Streaming**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     Aspect      â”‚ Spark Streaming â”‚ Apache Flink    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Processing Modelâ”‚ Micro-batches   â”‚ True Streaming  â”‚
â”‚ Latency         â”‚ 0.5-2 seconds   â”‚ <100ms          â”‚
â”‚ Throughput      â”‚ Very High       â”‚ High            â”‚
â”‚ Exactly-once    â”‚ Yes             â”‚ Yes             â”‚
â”‚ Complex Events  â”‚ Limited         â”‚ Excellent (CEP) â”‚
â”‚ Windowing       â”‚ Good            â”‚ Very Advanced   â”‚
â”‚ Ecosystem       â”‚ Mature          â”‚ Growing         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

```

**Flink Banking Use Case:**

```java
// Flink: Real-time Transaction Monitoring
DataStream<Transaction> transactions = env
    .addSource(new KafkaSource<>("transactions"))
    .keyBy(Transaction::getCustomerId)
    .window(TumblingEventTimeWindows.of(Time.minutes(5)))
    .aggregate(new TransactionAggregator())
    .filter(new FraudDetectionFilter());

transactions.addSink(new AlertSink());

```

## **ğŸ† Cloud-Plattformen fÃ¼r Big Data**

Moderne Banken nutzen Cloud-Plattformen, um Big Data zu verarbeiten. Das ist wie der Umzug von einem eigenen Rechenzentrum in ein hochmodernes, geteiltes Datacenter:

### **â˜ï¸ Google Cloud + Databricks: Das Power-Duo**

- **Google Cloud Platform** = Das Rechenzentrum. Unbegrenzte Rechenleistung und Speicherplatz, den Sie nach Bedarf mieten.
- **Databricks** = Die Entwicklungsumgebung. Ein modernes Labor, wo Data Scientists und Entwickler zusammenarbeiten kÃ¶nnen.
- **BigQuery** = Der Hochgeschwindigkeits-Abfrage-Service. Stellen Sie Fragen an Milliarden von DatensÃ¤tzen und bekommen Antworten in Sekunden.
- **Vertex AI** = Die KÃ¼nstliche Intelligenz. Wie ein sehr kluger Assistent, der Muster in Daten erkennt und Vorhersagen macht.

### **GCP Databricks: Unified Analytics Platform**

**Databricks Architektur:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                Databricks Workspace                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Notebooks â”‚ Jobs â”‚ Models â”‚ SQL â”‚ Dashboards        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚            Apache Spark Runtime                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚              GCP Infrastructure                     â”‚
â”‚  Compute Engine â”‚ Cloud Storage â”‚ BigQuery          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

```

### Databricks - Unified Analytics Platform

![image.png](Tag%201%20Big%20Data%20fundamentals%20+%20Multi-source%20data%20in%2024a7a6147595806a8bfeec3805da3e08/image%204.png)

**Key Databricks Features:**

- **Collaborative Notebooks:** Multi-language (Python, Scala, SQL, R)
- **Auto-scaling Clusters:** Pay-per-use compute
- **MLflow Integration:** ML lifecycle management
- **Delta Lake:** ACID transactions for data lakes
- **Security:** Enterprise-grade access controls

### **Google Cloud Integration**

```python
# Databricks + GCP Services Integration
from pyspark.sql import SparkSession

# Session Initialization
spark = SparkSession.builder \
    .appName("Banking Analytics") \
    .getOrCreate()

# Read directly from BigQuery
transactions = spark.read \
    .format("bigquery") \
    .option("table", "banking-project.transactions.daily") \
    .load()

# Process with Spark
fraud_scores = transactions \
    .select("customer_id", "amount", "merchant") \
    .groupBy("customer_id") \
    .agg({"amount": "avg", "merchant": "count"})

# Write back to BigQuery
fraud_scores.write \
    .format("bigquery") \
    .option("table", "banking-project.analytics.fraud_scores") \
    .mode("overwrite") \
    .save()

```

## ğŸ¦ **Banking-Spezifische Big Data Anwendungen**

### **Fraud Detection mit Big Data**

**Traditionell:** Ein Bankmitarbeiter Ã¼berprÃ¼ft morgens die Transaktionen vom Vortag. BetrÃ¼ger haben 24 Stunden Vorsprung.

**Big Data:** Ein intelligentes System analysiert jede Transaktion in Echtzeit:

- **Ist das normal fÃ¼r diesen Kunden?** (Verhaltensanalyse)
- **Passt der Ort zur gewohnten Route?** (Geo-Analyse)
- **Gibt es Ã¤hnliche Betrugsmuster?** (Machine Learning)
- **Entscheidung in <100ms:** Transaktion genehmigen oder blockieren

### **Traditional vs. Big Data Approach**

```
Traditional Fraud Detection:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ â€¢ Rule-based (if amount > â‚¬5000...)     â”‚
â”‚ â€¢ Batch processing (daily updates)      â”‚
â”‚ â€¢ Limited features (transaction only)   â”‚
â”‚ â€¢ High false positives (>30%)           â”‚
â”‚ â€¢ 24-48 hour detection delay            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Big Data Fraud Detection:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ â€¢ ML-based (complex pattern detection)  â”‚
â”‚ â€¢ Real-time processing (<100ms)         â”‚
â”‚ â€¢ Multi-modal (transaction + behavior)  â”‚
â”‚ â€¢ Low false positives (<5%)             â”‚
â”‚ â€¢ Immediate detection & blocking        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

```

### **Multi-Layer Fraud Detection Architecture**

```
Layer 1: Real-time Rules (< 10ms)
â”œâ”€ Amount limits
â”œâ”€ Geographic checks
â””â”€ Merchant validation

Layer 2: ML Scoring (< 100ms)
â”œâ”€ Behavior patterns
â”œâ”€ Network analysis
â””â”€ Anomaly detection

Layer 3: Deep Analysis (< 1s)
â”œâ”€ Text analysis (LLM)
â”œâ”€ Cross-reference external data
â””â”€ Human expert review queue

```

## ğŸ“ **Datenbeschaffung und -integration**

### **Interne Datenquellen**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Core Banking System                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ Transaktionsdaten (Payment Processing)            â”‚
â”‚ â€¢ Kundenstammdaten (CRM)                            â”‚
â”‚ â€¢ Kontoinformationen (Account Management)           â”‚
â”‚ â€¢ Kredithistorie (Risk Management)                  â”‚
â”‚ â€¢ Produktportfolio (Product Database)               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 Digital Channels                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ Mobile Banking Logs                               â”‚
â”‚ â€¢ Online Banking Clickstreams                       â”‚
â”‚ â€¢ ATM Transaction Logs                              â”‚
â”‚ â€¢ Call Center Interactions                          â”‚
â”‚ â€¢ Chat/Support Communications                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

```

### **Externe Datenquellen**

```python
# External Data Integration fÃ¼r Enhanced Analytics
external_data_sources = {
    "market_data": {
        "provider": "Bloomberg/Reuters",
        "frequency": "real_time",
        "data_types": ["stock_prices", "fx_rates", "interest_rates"],
        "volume": "1M records/day"
    },
    "credit_bureaus": {
        "provider": "Schufa/Experian",
        "frequency": "on_demand",
        "data_types": ["credit_scores", "payment_history"],
        "volume": "100K requests/day"
    },
    "economic_indicators": {
        "provider": "Government APIs",
        "frequency": "monthly",
        "data_types": ["inflation", "unemployment", "gdp"],
        "volume": "1K indicators"
    },
    "social_sentiment": {
        "provider": "Social Media APIs",
        "frequency": "hourly",
        "data_types": ["brand_mentions", "sentiment_scores"],
        "volume": "50K mentions/day"
    }
}

```

## **Datenextraktion Strategien**

In der Banking-Welt mÃ¼ssen wir verschiedene DatenextraktionsansÃ¤tze verstehen, da nicht alle Daten gleich behandelt werden kÃ¶nnen. Hier sind die drei Hauptstrategien:

### **1. Batch ETL fÃ¼r Historical Data**

Warum Batch ETL?

- GroÃŸe Datenmengen verarbeiten (TB-Level)
- Komplexe Transformationen mit hoher Rechenleistung
- ZeitkritikalitÃ¤t nicht gegeben (tÃ¤glich, wÃ¶chentlich)
- Kostenoptimierung durch Off-Peak-Verarbeitung

Banking-Beispiel fÃ¼r Batch ETL:

TÃ¤gliche Verarbeitung von Transaktionsdaten:

Schritt 1: Extract (22:00 Uhr)
â€¢ Alle Transaktionen des Tages aus Core Banking System
â€¢ Kundenstammdaten-Updates
â€¢ Wechselkurs-Snapshots
â€¢ Merchant-Kategorisierungen

Schritt 2: Transform (22:30-04:00 Uhr)
â€¢ Datenbereinigung (Duplikate, Formate)
â€¢ Anreicherung mit externen Daten
â€¢ Fraud-Score-Berechnung (historisch)
â€¢ Customer-Segmentierung
â€¢ Compliance-Checks

Schritt 3: Load (04:00-05:00 Uhr)
â€¢ Data Warehouse Update
â€¢ Analytische DatenbankbefÃ¼llung
â€¢ Dashboard-Datenmart Refresh

Typische Batch-GrÃ¶ÃŸen im Banking:

- Transaktionen: 1-10 Millionen Records/Tag
- Datenvolumen: 100GB-2TB/Tag
- Verarbeitungszeit: 4-6 Stunden
- ParallelitÃ¤t: 50-200 Spark Executors

### **2. Real-time Streaming fÃ¼r Live Data**

Wann brauchen wir Real-time?

- Fraud Detection: Transaktion muss in <100ms bewertet werden
- Risk Monitoring: Portfoliowerte Ã¤ndern sich sekundÃ¤r
- Customer Experience: Sofortige Empfehlungen
- Regulatory Compliance: Sofortige Meldungen

Streaming-Architektur im Banking:

ATM/POS â†’ Kafka â†’ Spark Streaming â†’ Fraud Engine â†’ Sofortige Entscheidung
    â†“           â†“           â†“              â†“              â†“
  <1ms      <10ms      <50ms         <30ms         <100ms

Parallele Verarbeitung:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Transaction       â”‚â†’ â”‚             Kafka             â”‚â†’â”‚           Multiple          â”‚
â”‚             Stream          â”‚    â”‚             Topics            â”‚    â”‚       Consumers        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚                    â”‚
                           â”Œâ”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”                     â”‚
                           â”‚                â”‚                â”‚                     â–¼
              â”Œâ”€â”€â”€â”€â–¼â”€â”€â” â”Œâ”€â–¼â”€â”€â” â”Œâ”€â–¼â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚      Fraud    â”‚ â”‚  Risk  â”‚ â”‚    Rec  â”‚   â”‚      Database     â”‚
              â”‚    Engine    â”‚ â”‚ Mgmtâ”‚ â”‚  Eng   â”‚   â”‚      Updates       â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Streaming vs. Batch Entscheidungsmatrix:

Streaming wÃ¤hlen, wenn:
â€¢ Latenz < 1 Sekunde erforderlich
â€¢ Daten kontinuierlich ankommen
â€¢ Sofortige Aktionen nÃ¶tig (Alerts, Blocking)
â€¢ Event-driven Architecture

Batch wÃ¤hlen, wenn:
â€¢ Komplexe Joins zwischen groÃŸen DatensÃ¤tzen
â€¢ Historische Analysen Ã¼ber Jahre
â€¢ Kostenoptimierung wichtig
â€¢ Verarbeitungszeit > 1 Stunde akzeptabel

### **3. API Integration fÃ¼r External Data**

Warum APIs kritisch fÃ¼r Banking sind:

- Regulatorische Anforderungen (Open Banking, PSD2)
- Real-time Marktdaten fÃ¼r Trading
- Kreditscoring mit externen Datenquellen
- KYC/AML-Compliance Checks

Typische Banking API-Integrations-Szenarien:

Szenario 1: KreditprÃ¼fung in Echtzeit
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Kunde     â”‚ â†’ â”‚ Bank System   â”‚ â†’ â”‚ Multiple APIs   â”‚
â”‚ Antrag    â”‚    â”‚ (Spark Job)   â”‚    â”‚ (Parallel)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                              â”‚
                               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                               â”‚          â”‚          â”‚
                         â”Œâ”€â”€â”€â”€â–¼â”€â”€â” â”Œâ”€â–¼â”€â”€â” â”Œâ”€â–¼â”€â”€â”€â”€â”
                         â”‚Schufa â”‚ â”‚BaFinâ”‚ â”‚Socialâ”‚
                         â”‚ API   â”‚ â”‚ API â”‚ â”‚ Data â”‚
                         â””â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”˜
                             â”‚       â”‚       â”‚
                             â–¼       â–¼       â–¼
                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                         â”‚   ML-Model fÃ¼r Credit   â”‚
                         â”‚   Scoring (< 3 Sek)     â”‚
                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

API-Integration Herausforderungen:

- Rate Limiting: APIs haben oft BeschrÃ¤nkungen (z.B. 1000 Calls/Minute)
- Authentication: OAuth, API Keys, Zertifikate verwalten
- Error Handling: Netzwerkfehler, Timeouts, Service-Downtimes
- Data Consistency: Verschiedene APIs liefern unterschiedliche Formate
- Kosten: Pay-per-call Models kÃ¶nnen teuer werden

### **Datenintegration Herausforderungen**

Selbst mit den besten Extraktionsstrategien stehen wir vor zwei grundlegenden Herausforderungen:

### **1. Schema Evolution Management**

Das Problem: Datenstrukturen Ã¤ndern sich stÃ¤ndig

Warum Schema Evolution in Banking kritisch ist:

- Regulatorische Ã„nderungen: Neue Compliance-Felder (z.B. DSGVO, Basel III)
- Produktinnovationen: Neue Bankprodukte brauchen neue Datenfelder
- API-Updates: Externe Datenanbieter Ã¤ndern ihre Strukturen
- System-Upgrades: Core Banking Systeme werden modernisiert

Schema Evolution Beispiel - Banking Transaction Table:

Version 1.0 (2020):
â€¢ transaction_id
â€¢ customer_id
â€¢ amount
â€¢ currency
â€¢ timestamp

Version 2.0 (2021 - DSGVO):
+ privacy_consent_flag
+ data_retention_date
+ anonymization_level

Version 3.0 (2022 - Open Banking):
+ third_party_provider_id
+ consent_scope
+ api_version

Version 4.0 (2024 - ESG Compliance):
+ carbon_footprint_score
+ sustainable_investment_flag
+ esg_rating

Das Problem: Wie behandeln wir alte Daten mit neuen Schemas?

LÃ¶sungsansÃ¤tze fÃ¼r Schema Evolution:

- Schema Registry: Zentrale Versionierung aller Schemas (Confluent Schema Registry)
- Delta Lake: ACID-Transaktionen fÃ¼r Schema-Ã„nderungen
- Backwards Compatibility: Neue Felder als optional mit Default-Werten
- Data Versioning: Separate Pipelines fÃ¼r verschiedene Schema-Versionen

### Praktisches Beispiel: Schema Evolution Workflow

Schritt 1: Schema-Ã„nderung erkennen
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Neues Feld: esg_rating      â”‚
â”‚ Typ: INTEGER               â”‚
â”‚ Default: NULL              â”‚
â”‚ Required: false            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Schritt 2: Schema Registry Update
â€¢ Version 4.0 registrieren
â€¢ Backward Compatibility prÃ¼fen
â€¢ Consumer benachrichtigen

Schritt 3: Graduelle Migration
â€¢ Neue Daten: Schema 4.0
â€¢ Alte Daten: Schema 1.0-3.0 (NULL-Werte fÃ¼r neue Felder)
â€¢ Queries: Beide Versionen unterstÃ¼tzen

### **2. Data Quality Across Sources**

Das Problem: Jede Datenquelle hat ihre eigenen QualitÃ¤tsstandards

Multi-Source Data Quality Herausforderungen:

Banking Data Quality Matrix (typische Probleme):

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Datenquelle      â”‚ Typical Issues   â”‚ Quality Score    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Core Banking     â”‚ 5% Duplicates    â”‚ 95% (Excellent)  â”‚
â”‚ Mobile App       â”‚ 10% Missing      â”‚ 85% (Good)       â”‚
â”‚ Web Portal       â”‚ 15% Inconsistent â”‚ 80% (Fair)       â”‚
â”‚ ATM Logs         â”‚ 20% Format Issuesâ”‚ 75% (Poor)       â”‚
â”‚ External APIs    â”‚ 30% Stale Data   â”‚ 60% (Critical)   â”‚
â”‚ Social Media     â”‚ 50% Unreliable   â”‚ 40% (Unusable)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Spezifische QualitÃ¤tsprobleme nach Quelle:

Core Banking System (95% Quality):
â€¢ Sehr hohe Genauigkeit
â€¢ VollstÃ¤ndige Transaktionsdaten
â€¢ Problem: Legacy-Formate, langsame Updates

Mobile/Web Channels (80-85% Quality):
â€¢ Real-time Daten
â€¢ Gute User Experience Metriken
â€¢ Problem: UnvollstÃ¤ndige Sessions, Device-Unterschiede

External APIs (60% Quality):
â€¢ Wertvolle Anreicherung
â€¢ Aktuelle Marktdaten
â€¢ Problem: Rate Limits, Ausfallzeiten, verschiedene Formate

Data Quality Management Strategien:

1. Source-Level Quality Gates (PrÃ¤ventive MaÃŸnahmen):

- Input Validation: Daten sofort bei Eingabe prÃ¼fen
- API Contracts: Klare SLAs mit externen Anbietern
- Real-time Monitoring: Sofortige Alerts bei QualitÃ¤tsabfall

2. Processing-Level Quality Control (Reaktive MaÃŸnahmen):

- Data Profiling: Automatische Erkennung von Anomalien
- Cross-Source Validation: Daten zwischen Quellen abgleichen
- Machine Learning Quality Scoring: Automatische QualitÃ¤tsbewertung

3. Unified Data Quality Framework:

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            Data Quality Monitoring Dashboard           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Source        Quality Score    Last Updated   Status â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Core Banking  95%             2min ago       ğŸŸ¢     â”‚
â”‚ Mobile App    87%             5min ago       ğŸŸ¢     â”‚
â”‚ Web Portal    78%             1min ago       ğŸŸ¡     â”‚
â”‚ External API  45%             15min ago      ğŸ”´     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Praktische Workshop-Ãœbung: Data Quality Assessment

Im Workshop werden wir folgende praktische Ãœbungen durchfÃ¼hren:

- Batch ETL: Historische Transaktionsdaten mit Spark verarbeiten
- Streaming: Live-Simulation von Kreditkarten-Transaktionen
- API Integration: Externe Wechselkurse in Echtzeit abrufen
- Schema Evolution: Neue Compliance-Felder hinzufÃ¼gen ohne Downtime
- Quality Monitoring: Automatische Erkennung von DatenqualitÃ¤tsproblemen

## **ğŸ”„ Datenbeschaffung und -integration verstehen**

In der Banking-Welt kommen Daten aus vielen verschiedenen Quellen. Stellen Sie sich vor, Sie mÃ¼ssen ein vollstÃ¤ndiges Bild eines Kunden erstellen:

### **ğŸ¯ DatenqualitÃ¤t: Das A und O**

Schlechte DatenqualitÃ¤t ist wie ein kaputtes NavigationsgerÃ¤t - es fÃ¼hrt Sie in die falsche Richtung, egal wie gut Ihr Auto ist.

****Die 6 Dimensionen der DatenqualitÃ¤t:****

- **VollstÃ¤ndigkeit** = Sind alle Puzzle-Teile da? (Wie vollstÃ¤ndig ist ein Kundenprofil?)
- **Genauigkeit** = Stimmen die Informationen? (Ist die E-Mail-Adresse korrekt?)
- **Konsistenz** = Sagen alle Quellen dasselbe? (HeiÃŸt der Kunde Ã¼berall gleich?)
- **AktualitÃ¤t** = Sind die Daten frisch genug? (Ist die Adresse noch aktuell?)
- **GÃ¼ltigkeit** = ErfÃ¼llen Daten die GeschÃ¤ftsregeln? (Ist das Geburtsdatum realistisch?)
- **Eindeutigkeit** = Gibt es Duplikate? (Existiert derselbe Kunde mehrfach?)

## **ğŸ© Mehr Banking-Use Cases in der Praxis**

Schauen wir uns konkrete Beispiele an, wie Big Data das Banking verÃ¤ndert hat:

### **ğŸ“Š Kreditentscheidungen: Der intelligente Berater**

**Traditionell:** Kreditantrag â†’ Manuelle PrÃ¼fung â†’ 2 Wochen warten â†’ Entscheidung

**Big Data:** Automatische Analyse in 2 Minuten:

- **Finanzhistorie:** Wie zuverlÃ¤ssig war der Kunde bisher?
- **Verhaltensanalyse:** Wie verwaltet er sein Geld?
- **Externe Faktoren:** Wie stabil ist sein Job/Branche?
- **Resultat:** 95% Genauigkeit bei Ausfallprognosen

### **ğŸ›ï¸ Personalisierte Angebote: Der persÃ¶nliche Bankberater**

**Traditionell:** Alle Kunden bekommen dieselben Standard-Angebote per Post.

**Big Data:** Jeder Kunde bekommt maÃŸgeschneiderte Angebote:

- **Junge Familie mit steigendem Einkommen** â†’ Bauspardarlehen-Angebot
- **Student mit regelmÃ¤ÃŸigen Einzahlungen** â†’ Investmentfonds-Angebot
- **Senior mit hohen Ersparnissen** â†’ Altersvorsorge-Optimierung

## **ğŸŒˆ Zusammenfassung: Die Big Data Revolution**

**Tag 1 hat gezeigt, wie sich die Datenwelt fundamental verÃ¤ndert hat:**

- **Von klein zu riesig:** MB â†’ Petabytes (das Millionenfache)
- **Von langsam zu sofort:** TÃ¤gliche Berichte â†’ Echtzeit-Entscheidungen
- **Von starr zu flexibel:** Feste Schemas â†’ Beliebige Datenformate
- **Von teuer zu erschwinglich:** Spezialhardware â†’ Cloud-Services

**Die wichtigsten Erkenntnisse:**

- **Big Data ist kein Hype** - es lÃ¶st echte GeschÃ¤ftsprobleme
- **Technologie ist Mittel zum Zweck** - der Fokus liegt auf Mehrwert fÃ¼r Kunden
- **DatenqualitÃ¤t ist entscheidend** - schlechte Daten fÃ¼hren zu schlechten Entscheidungen
- **Integration ist komplex** - aber die Tools werden immer

## Databricks Tutorial

File Upload oder Daten aus verschiedenen Quellen beziehen

![Screenshot 2025-08-11 at 15.26.20.png](Tag%201%20Big%20Data%20fundamentals%20+%20Multi-source%20data%20in%2024a7a6147595806a8bfeec3805da3e08/Screenshot_2025-08-11_at_15.26.20.png)