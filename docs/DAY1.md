# Tag 1: Big Data fundamentals + Multi-source data integration

## ğŸ¯ **Ziele und Erwartungen der Teilnehmenden**

### **Warum diese Session kritisch ist**

Die KlÃ¤rung individueller Lernziele bildet das Fundament fÃ¼r einen erfolgreichen Workshop. Bei nur 2 Teilnehmern kÃ¶nnen wir den Inhalt optimal an die spezifischen BedÃ¼rfnisse anpassen.

### **Lernziel-Framework fÃ¼r Banking Analytics**

```
Business Goals â†’ Technical Skills â†’ Practical Application
     â†“                â†“                    â†“
- Fraud Detection   - Spark Mastery    - Real Banking Data
- Customer Insights - ML Algorithms    - Production Pipeline
- Risk Assessment   - LLM Integration  - Cloud Deployment

```

### **Erwartungsmanagement**

- **70% Hands-On:** Direktes Arbeiten mit Code und Daten
- **30% Theorie:** Nur das Wesentliche fÃ¼r das VerstÃ¤ndnis
- **Praxisrelevanz:** Jede Ãœbung mit Banking-Kontext
- **Skalierbarkeit:** Von lokaler Entwicklung bis Cloud-Production

---

## ğŸ“Š **Grundlagen von Big Data**

### **Die Evolution der Datenlandschaft**

### **Traditionelle Datenverarbeitung (1990er-2000er)**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   OLTP      â”‚    â”‚ Data Warehouse â”‚    â”‚  Reporting  â”‚
â”‚ (Transact.) â”‚ -> â”‚  (Nightly ETL) â”‚ -> â”‚ (Business)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   â€¢ MB-GB Daten     â€¢ Strukturiert      â€¢ Batch-Reports
   â€¢ Einzelne Server  â€¢ Feste Schemas     â€¢ Historische Sicht

```

### **Big Data Ã„ra (2010er-heute)**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Multi-Sourceâ”‚    â”‚  Data Lake  â”‚    â”‚ Real-time   â”‚
â”‚ (Streaming) â”‚ -> â”‚ (All Data)  â”‚ -> â”‚ Analytics   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   â€¢ TB-PB Daten     â€¢ Alle Formate     â€¢ Live Insights
   â€¢ Verteilte Sys.  â€¢ Schema-on-Read   â€¢ Predictive

```

### **Die 5 V's von Big Data - Detaillierte Analyse**

### **1. Volume (Datenmenge)**

**Problem:** Exponentielles Datenwachstum

**Banking-Beispiele:**

- **Deutsche Bank:** ~1 Milliarde Transaktionen/Tag
- **PayPal:** 19 Millionen Transaktionen/Tag
- **Visa:** 65.000 Transaktionen/Sekunde

**Technische Herausforderungen:**

```python
# Traditionelle Limits
pandas_limit = "~8GB RAM Limit"
single_machine = "Vertical Scaling Costs"

# Big Data LÃ¶sung
spark_distributed = "Horizontal Scaling"
cloud_elastic = "Pay-as-you-scale"

```

### **2. Velocity (Geschwindigkeit)**

**Problem:** Real-time Verarbeitung wird zur GeschÃ¤ftsanforderung

**Banking Real-time Requirements:**

- **Fraud Detection:** < 100ms Entscheidung
- **Credit Approval:** < 5 Sekunden
- **Market Trading:** < 1ms (High-Frequency Trading)
- **Mobile Banking:** < 200ms Response

**Architektur-Implikationen:**

```
Batch Processing    vs.    Stream Processing
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚Data arrives â”‚           â”‚Data arrives â”‚
â”‚in batches   â”‚           â”‚continuously â”‚
â”‚             â”‚           â”‚             â”‚
â”‚Process everyâ”‚           â”‚Process in   â”‚
â”‚X hours      â”‚           â”‚real-time    â”‚
â”‚             â”‚           â”‚             â”‚
â”‚High latency â”‚           â”‚Low latency  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

```

### **3. Variety (Vielfalt)**

**Problem:** Daten kommen in verschiedenen Formaten

**Banking Data Sources:**

```
Strukturiert (20%):        Semi-strukturiert (30%):    Unstrukturiert (50%):
â€¢ Transaktionsdaten       â€¢ JSON APIs                 â€¢ E-Mails
â€¢ Kundenstammdaten       â€¢ XML Nachrichten           â€¢ Chat-Logs
â€¢ KontostÃ¤nde            â€¢ Log-Dateien               â€¢ Dokumente
â€¢ Kredithistorie         â€¢ Web-Clickstreams          â€¢ Audio-Calls

```

**Schema Evolution Challenge:**

```sql
-- Traditional: Schema-on-Write
CREATE TABLE transactions (
    id INTEGER,
    amount DECIMAL(10,2),
    currency CHAR(3)
);

-- Big Data: Schema-on-Read
-- Flexibel: Neue Felder ohne Schema-Ã„nderung
{
  "id": 123,
  "amount": 99.99,
  "currency": "EUR",
  "metadata": {...}  // Neue Felder jederzeit mÃ¶glich
}

```

### **4. Veracity (Wahrhaftigkeit)**

**Problem:** DatenqualitÃ¤t variiert stark

**Banking Data Quality Issues:**

- **Incomplete Data:** 15-20% fehlende Werte in Customer Data
- **Inconsistent Formats:** Datum, WÃ¤hrung, Namen
- **Duplicate Records:** 5-10% bei KundenzusammenfÃ¼hrungen
- **Outdated Information:** Adressen, Telefonnummern

**Quality Impact on Business:**

```
Schlechte DatenqualitÃ¤t â†’ Falsche Insights â†’ Schlechte Entscheidungen

Beispiel: Credit Scoring
â€¢ 30% fehlerhafte Einkommensdaten
â€¢ 15% veraltete BeschÃ¤ftigungsinformationen
â†’ 25% falsche Kreditentscheidungen
â†’ â‚¬50M jÃ¤hrlicher Verlust (GroÃŸbank)

```

### **5. Value (Wert)**

**Problem:** Nur ein Bruchteil der Daten ist wertvoll

**Banking Value Extraction:**

```
Raw Data (100%) â†’ Processed (30%) â†’ Insights (5%) â†’ Action (1%)

Beispiel: Fraud Detection
â€¢ 1M tÃ¤gliche Transaktionen
â€¢ 10,000 verdÃ¤chtige Muster
â€¢ 100 echte BetrugsfÃ¤lle
â€¢ 50 verhinderte Verluste = â‚¬500,000 tÃ¤glich gerettete Summe

```

### **Herausforderungen bei der Big Data Verarbeitung**

### **1. Speicherung**

**Problem:** Petabyte-Scale Storage Management

**Traditionelle Limits:**

- **Single Server:** Maximale FestplattenkapazitÃ¤t
- **SAN/NAS:** Kostenexplosion bei groÃŸen Datenmengen
- **Backup:** UnmÃ¶gliche Backup-Zeiten

**Big Data LÃ¶sung:**

```
Distributed File Systems (HDFS)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Node 1  â”‚ â”‚ Node 2  â”‚ â”‚ Node 3  â”‚ â”‚ Node 4  â”‚
â”‚ 10TB    â”‚ â”‚ 10TB    â”‚ â”‚ 10TB    â”‚ â”‚ 10TB    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚           â”‚           â”‚           â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚           â”‚
        Replicated Data (3x Copy)

```

### **2. Verarbeitungsgeschwindigkeit**

**Problem:** Traditionelle Algorithmen skalieren nicht

**Complexity Analysis:**

```python
# O(n) - Linear: Akzeptabel bis ~1M Records
for transaction in transactions:
    process(transaction)

# O(nÂ²) - Quadratic: UnmÃ¶glich bei Big Data
for t1 in transactions:
    for t2 in transactions:
        compare(t1, t2)  # 1M Ã— 1M = 1 Trillion operations!

# O(log n) - Distributed: Big Data Solution
spark.sql("SELECT * FROM transactions WHERE suspicious = true")

```

### **3. DatenqualitÃ¤t bei Scale**

**Problem:** Quality Checks werden zum Bottleneck

**Traditional Quality Checks:**

```python
# Single-threaded: 10,000 records/second
def validate_record(record):
    check_completeness(record)
    check_format(record)
    check_business_rules(record)
    return is_valid

# Time fÃ¼r 1B records: 100,000 seconds (28 hours!)

```

**Distributed Quality Checks:**

```python
# Spark: 1,000,000 records/second (1000x faster)
df.filter(col("amount") > 0)\
  .filter(col("currency").isin(["EUR", "USD", "GBP"]))\
  .filter(col("timestamp").isNotNull())

```

### **Chancen und Potenziale von Big Data**

### **1. Erkenntnisgewinn durch Skalierung**

**Statistical Power durch grÃ¶ÃŸere Samples:**

```
Sample Size Impact auf Confidence:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Sample Size  â”‚ Margin Errorâ”‚ Confidence   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1,000        â”‚ Â±3.1%       â”‚ 95%          â”‚
â”‚ 10,000       â”‚ Â±1.0%       â”‚ 95%          â”‚
â”‚ 1,000,000    â”‚ Â±0.1%       â”‚ 95%          â”‚
â”‚ 100,000,000  â”‚ Â±0.01%      â”‚ 95%          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

```

**Banking Insights Examples:**

- **Micro-Segmentation:** 10,000 Customer segments statt 10
- **Real-time Patterns:** Fraud detection in Millisekunden
- **Predictive Accuracy:** 95% Genauigkeit bei KreditausfÃ¤llen

### **2. Verbesserte Entscheidungsfindung**

**From Reactive to Predictive:**

```
Traditional Banking          Big Data Banking
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Historical      â”‚   â†’     â”‚ Predictive      â”‚
â”‚ Analysis        â”‚         â”‚ Analytics       â”‚
â”‚                 â”‚         â”‚                 â”‚
â”‚ "What happened?"â”‚         â”‚ "What will      â”‚
â”‚                 â”‚         â”‚  happen?"       â”‚
â”‚                 â”‚         â”‚                 â”‚
â”‚ Monthly Reports â”‚         â”‚ Real-time       â”‚
â”‚                 â”‚         â”‚ Dashboards      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

```

**Decision Speed Impact:**

- **Credit Decisions:** 2 Wochen â†’ 2 Minuten
- **Fraud Detection:** 24 Stunden â†’ 100 Millisekunden
- **Market Response:** 1 Woche â†’ Real-time

### **3. Personalisierte Dienstleistungen**

**Hyper-Personalization durch Big Data:**

```python
# Traditional: Segment-based
if customer.age > 65:
    offer_pension_products()

# Big Data: Individual-based
if customer.spending_pattern.matches("young_family") and \
   customer.savings_rate > 0.15 and \
   customer.location.near("good_schools"):
    offer_education_savings_plan()

```

---

## ğŸ—ï¸ **Big Data-Technologien und -Plattformen**

### **Hadoop-Ã–kosystem: Die Foundation**

### **Hadoop Architektur Overview**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                Hadoop Ecosystem                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Applications: Hive, Pig, Mahout, HBase, Flume     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Processing: MapReduce, Spark, Tez                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Resource Management: YARN                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Storage: HDFS (Hadoop Distributed File System)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

```

### **HDFS (Hadoop Distributed File System)**

**Das verteilte Dateisystem fÃ¼r Big Data**

**Kernprinzipien:**

```
1. Massive Skalierung:
   â€¢ Designed fÃ¼r Petabytes
   â€¢ Commodity Hardware
   â€¢ Linear scaling

2. Fault Tolerance:
   â€¢ Automatische Replikation (3x default)
   â€¢ Self-healing bei Node-AusfÃ¤llen
   â€¢ No Single Point of Failure

3. High Throughput:
   â€¢ Optimiert fÃ¼r groÃŸe Files (>64MB)
   â€¢ Sequential Access Pattern
   â€¢ Bandwidth over Latency

```

**HDFS Architektur:**

```
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚ NameNode    â”‚ â† Metadaten Master
                â”‚ (Master)    â”‚   (Filesystem Namespace)
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚              â”‚              â”‚
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚ DataNode 1  â”‚ â”‚ DataNode 2  â”‚ â”‚ DataNode 3  â”‚
 â”‚             â”‚ â”‚             â”‚ â”‚             â”‚
 â”‚ Block A-1   â”‚ â”‚ Block A-2   â”‚ â”‚ Block A-3   â”‚
 â”‚ Block B-1   â”‚ â”‚ Block B-2   â”‚ â”‚ Block C-1   â”‚
 â”‚ Block C-2   â”‚ â”‚ Block C-3   â”‚ â”‚ Block B-3   â”‚
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

```

**Banking Use Case:**

```python
# Speicherung von 5 Jahren Transaktionsdaten
# 1 Milliarde Transaktionen/Tag Ã— 365 Tage Ã— 5 Jahre = 1.8 Trillion Records
# @ 200 Bytes/Record = 360 TB Raw Data

hdfs_storage = {
    "raw_transactions": "120 TB",
    "processed_data": "90 TB",
    "ml_features": "60 TB",
    "models_backups": "30 TB",
    "total_with_replication": "900 TB"  # 3x replication
}

```

### **MapReduce-Programmierung**

**Das ursprÃ¼ngliche Big Data Processing Model**

**MapReduce Paradigma:**

```
Input â†’ MAP â†’ Shuffle & Sort â†’ REDUCE â†’ Output

Beispiel: Fraud Detection Count
Input: [transaction1, transaction2, ..., transactionN]

MAP Phase:
transaction1 â†’ ("fraud", 1) if is_suspicious(transaction1) else ("normal", 1)
transaction2 â†’ ("fraud", 1) if is_suspicious(transaction2) else ("normal", 1)

REDUCE Phase:
("fraud", [1,1,1,...]) â†’ ("fraud", total_count)
("normal", [1,1,1,...]) â†’ ("normal", total_count)

```

**MapReduce Limitations (Warum Spark besser ist):**

```
MapReduce Problems:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. Disk-based: Slow I/O between jobs   â”‚
â”‚ 2. Batch-only: No real-time processing â”‚
â”‚ 3. Complex: Multi-step jobs difficult  â”‚
â”‚ 4. No Interactive: Each query from diskâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Spark Solutions:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. In-Memory: 10-100x faster          â”‚
â”‚ 2. Unified: Batch + Stream + ML       â”‚
â”‚ 3. Simple: High-level APIs             â”‚
â”‚ 4. Interactive: REPL for exploration   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

```

### **Hive & Pig: SQL-Ã¤hnliche Abfragesprachen**

**Apache Hive:**

```sql
-- Hive: SQL-on-Hadoop
CREATE TABLE transactions (
    transaction_id STRING,
    customer_id STRING,
    amount DECIMAL(10,2),
    timestamp TIMESTAMP,
    merchant STRING
) STORED AS PARQUET;

-- Complex Analytics Query
SELECT
    customer_id,
    COUNT(*) as transaction_count,
    AVG(amount) as avg_amount,
    STDDEV(amount) as amount_volatility
FROM transactions
WHERE timestamp >= '2024-01-01'
GROUP BY customer_id
HAVING COUNT(*) > 100;

```

**Apache Pig:**

```
-- Pig Latin: Dataflow Language
transactions = LOAD 'hdfs://transactions.csv' USING PigStorage(',')
    AS (id:chararray, customer:chararray, amount:double, timestamp:chararray);

filtered = FILTER transactions BY amount > 1000.0;
grouped = GROUP filtered BY customer;
aggregated = FOREACH grouped GENERATE
    group AS customer,
    COUNT(filtered) AS high_value_count;

STORE aggregated INTO 'hdfs://high_value_customers';

```

### **Apache Spark: Die moderne Big Data Engine**

### **Warum Spark MapReduce ablÃ¶st**

**Performance Comparison:**

```
Benchmark: 100GB Daten sortieren

MapReduce:    1600 Sekunden
Spark (Disk): 200 Sekunden  (8x schneller)
Spark (Memory): 23 Sekunden (70x schneller)

Iterative ML Algorithm (10 Iterations):
MapReduce:    30 Minuten (jede Iteration from disk)
Spark:        2 Minuten (data cached in memory)

```

### **Spark Core Concepts**

**1. In-Memory Computing:**

```
Traditional MapReduce:          Apache Spark:
â”Œâ”€â”€â”€â”€â”€â”                        â”Œâ”€â”€â”€â”€â”€â”
â”‚ Job1â”‚ â†’ Disk â†’ â”Œâ”€â”€â”€â”€â”€â”       â”‚ Job1â”‚ â†˜
â””â”€â”€â”€â”€â”€â”˜          â”‚ Job2â”‚       â””â”€â”€â”€â”€â”€â”˜  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”
                 â””â”€â”€â”€â”€â”€â”˜                â”‚ Memory â”‚
                      â†“                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              Disk â†’ â”Œâ”€â”€â”€â”€â”€â”                 â†—
                     â”‚ Job3â”‚       â”Œâ”€â”€â”€â”€â”€â” â†—
                     â””â”€â”€â”€â”€â”€â”˜       â”‚ Job2â”‚
                                   â””â”€â”€â”€â”€â”€â”˜

```

**2. Unified Analytics Platform:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                Spark Applications                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Spark SQL â”‚ Streaming â”‚ MLlib â”‚ GraphX â”‚ R/Python   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚              Spark Core Engine                      â”‚
â”‚         (RDDs, DAG Scheduler, Memory Mgmt)          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚           Cluster Resource Manager                  â”‚
â”‚      (YARN, Mesos, Kubernetes, Standalone)         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

```

### **Spark SQL: Verteiltes SQL**

**SQL-Interface fÃ¼r Big Data**

```python
# Spark SQL Example: Banking Fraud Analysis
from pyspark.sql import SparkSession
from pyspark.sql.functions import *

spark = SparkSession.builder.appName("FraudDetection").getOrCreate()

# Load banking data
transactions = spark.read.parquet("hdfs://banking/transactions/")

# Register as SQL table
transactions.createOrReplaceTempView("transactions")

# Complex fraud detection query
fraud_patterns = spark.sql("""
    SELECT
        customer_id,
        COUNT(*) as transaction_count,
        SUM(amount) as total_amount,
        AVG(amount) as avg_amount,
        STDDEV(amount) as amount_volatility,
        COUNT(DISTINCT merchant) as unique_merchants,
        COUNT(DISTINCT HOUR(timestamp)) as active_hours
    FROM transactions
    WHERE date(timestamp) = current_date()
    GROUP BY customer_id
    HAVING
        transaction_count > 50 OR
        amount_volatility > 1000 OR
        unique_merchants > 20
    ORDER BY amount_volatility DESC
""")

```

### **Spark Streaming: Real-time Processing**

**Micro-Batch Architecture fÃ¼r Low-Latency**

```python
# Spark Streaming: Real-time Fraud Detection
from pyspark.streaming import StreamingContext

# Create streaming context (2 second batches)
ssc = StreamingContext(spark.sparkContext, 2)

# Connect to Kafka stream
kafka_stream = KafkaUtils.createStream(ssc,
    zkQuorum="localhost:2181",
    groupId="fraud-detection",
    topics={"transactions": 1})

# Process each micro-batch
def process_transactions(time, rdd):
    if not rdd.isEmpty():
        # Convert to DataFrame
        df = spark.read.json(rdd)

        # Apply fraud detection model
        predictions = fraud_model.transform(df)

        # Filter high-risk transactions
        high_risk = predictions.filter(col("fraud_probability") > 0.8)

        # Send alerts
        high_risk.foreach(send_fraud_alert)

kafka_stream.foreachRDD(process_transactions)
ssc.start()
ssc.awaitTermination()

```

### **Apache Flink: Stream-First Processing**

### **Flink vs. Spark Streaming**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     Aspect      â”‚ Spark Streaming â”‚ Apache Flink    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Processing Modelâ”‚ Micro-batches   â”‚ True Streaming  â”‚
â”‚ Latency         â”‚ 0.5-2 seconds   â”‚ <100ms          â”‚
â”‚ Throughput      â”‚ Very High       â”‚ High            â”‚
â”‚ Exactly-once    â”‚ Yes             â”‚ Yes             â”‚
â”‚ Complex Events  â”‚ Limited         â”‚ Excellent (CEP) â”‚
â”‚ Windowing       â”‚ Good            â”‚ Very Advanced   â”‚
â”‚ Ecosystem       â”‚ Mature          â”‚ Growing         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

```

**Flink Banking Use Case:**

```java
// Flink: Real-time Transaction Monitoring
DataStream<Transaction> transactions = env
    .addSource(new KafkaSource<>("transactions"))
    .keyBy(Transaction::getCustomerId)
    .window(TumblingEventTimeWindows.of(Time.minutes(5)))
    .aggregate(new TransactionAggregator())
    .filter(new FraudDetectionFilter());

transactions.addSink(new AlertSink());

```

### **Cloud Platforms fÃ¼r Big Data**

### **GCP Databricks: Unified Analytics Platform**

**Databricks Architektur:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                Databricks Workspace                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Notebooks â”‚ Jobs â”‚ Models â”‚ SQL â”‚ Dashboards       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚            Apache Spark Runtime                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚              GCP Infrastructure                     â”‚
â”‚  Compute Engine â”‚ Cloud Storage â”‚ BigQuery         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

```

**Key Databricks Features:**

- **Collaborative Notebooks:** Multi-language (Python, Scala, SQL, R)
- **Auto-scaling Clusters:** Pay-per-use compute
- **MLflow Integration:** ML lifecycle management
- **Delta Lake:** ACID transactions for data lakes
- **Security:** Enterprise-grade access controls

### **Google Cloud Integration**

```python
# Databricks + GCP Services Integration
from pyspark.sql import SparkSession

# BigQuery Integration
spark = SparkSession.builder \
    .appName("Banking Analytics") \
    .config("spark.jars", "gs://spark-lib/bigquery/spark-bigquery-latest.jar") \
    .getOrCreate()

# Read from BigQuery
transactions = spark.read \
    .format("bigquery") \
    .option("table", "banking-project.transactions.daily") \
    .load()

# Process with Spark
fraud_scores = transactions \
    .select("customer_id", "amount", "merchant") \
    .groupBy("customer_id") \
    .agg({"amount": "avg", "merchant": "count"})

# Write back to BigQuery
fraud_scores.write \
    .format("bigquery") \
    .option("table", "banking-project.analytics.fraud_scores") \
    .mode("overwrite") \
    .save()

```

### **Vertex AI fÃ¼r LLM Integration**

**Google's AI Platform fÃ¼r Banking Analytics**

```python
# Vertex AI + LLM fÃ¼r Transaction Analysis
from google.cloud import aiplatform
from vertexai.language_models import TextGenerationModel

# Initialize Vertex AI
aiplatform.init(project="banking-analytics", location="us-central1")

# Load PaLM model
model = TextGenerationModel.from_pretrained("text-bison@001")

# Analyze transaction descriptions
def analyze_transaction_text(description):
    prompt = f"""
    Analyze this banking transaction description for fraud indicators:
    Transaction: "{description}"

    Consider:
    - Unusual merchant names
    - Suspicious transaction patterns
    - Geographic inconsistencies

    Provide a fraud risk score (0-100) and explanation.
    """

    response = model.predict(
        prompt=prompt,
        temperature=0.1,
        max_output_tokens=256
    )

    return response.text

# Apply to transaction stream
transactions_with_ai = spark_df.withColumn(
    "ai_fraud_analysis",
    udf(analyze_transaction_text)(col("description"))
)

```

---

## ğŸ¦ **Banking-Spezifische Big Data Anwendungen**

### **Fraud Detection mit Big Data**

### **Traditional vs. Big Data Approach**

```
Traditional Fraud Detection:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ â€¢ Rule-based (if amount > â‚¬5000...)     â”‚
â”‚ â€¢ Batch processing (daily updates)      â”‚
â”‚ â€¢ Limited features (transaction only)   â”‚
â”‚ â€¢ High false positives (>30%)          â”‚
â”‚ â€¢ 24-48 hour detection delay           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Big Data Fraud Detection:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ â€¢ ML-based (complex pattern detection)  â”‚
â”‚ â€¢ Real-time processing (<100ms)         â”‚
â”‚ â€¢ Multi-modal (transaction + behavior)  â”‚
â”‚ â€¢ Low false positives (<5%)            â”‚
â”‚ â€¢ Immediate detection & blocking        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

```

### **Multi-Layer Fraud Detection Architecture**

```
Layer 1: Real-time Rules (< 10ms)
â”œâ”€ Amount limits
â”œâ”€ Geographic checks
â””â”€ Merchant validation

Layer 2: ML Scoring (< 100ms)
â”œâ”€ Behavior patterns
â”œâ”€ Network analysis
â””â”€ Anomaly detection

Layer 3: Deep Analysis (< 1s)
â”œâ”€ Text analysis (LLM)
â”œâ”€ Cross-reference external data
â””â”€ Human expert review queue

```

### **Customer Analytics & Segmentation**

### **Traditional Segmentation**

```sql
-- Simple demographic segmentation
SELECT
    CASE
        WHEN age < 25 THEN 'Young'
        WHEN age < 45 THEN 'Middle'
        ELSE 'Senior'
    END as age_group,
    COUNT(*) as customer_count
FROM customers
GROUP BY age_group;

```

### **Big Data Behavioral Segmentation**

```python
# Advanced behavioral segmentation with Spark MLlib
from pyspark.ml.clustering import KMeans
from pyspark.ml.feature import VectorAssembler

# Create feature vectors
feature_cols = [
    "avg_transaction_amount", "transaction_frequency",
    "product_diversity", "digital_engagement_score",
    "customer_service_interactions", "account_balance_volatility"
]

assembler = VectorAssembler(inputCols=feature_cols, outputCol="features")
customer_features = assembler.transform(customer_data)

# Apply K-means clustering
kmeans = KMeans(k=8, featuresCol="features", predictionCol="segment")
model = kmeans.fit(customer_features)

# Generate segments
segmented_customers = model.transform(customer_features)

# Business interpretation
segment_profiles = segmented_customers.groupBy("segment").agg(
    avg("avg_transaction_amount").alias("avg_spend"),
    avg("transaction_frequency").alias("frequency"),
    avg("product_diversity").alias("products"),
    count("*").alias("customer_count")
)

```

### **Portfolio Optimization mit Big Data**

### **Risk Assessment at Scale**

```python
# Portfolio risk calculation with distributed computing
def calculate_var_at_risk(portfolio_data, confidence_level=0.95):
    """
    Value at Risk calculation for large portfolios
    Using Monte Carlo simulation on Spark
    """

    # Generate random scenarios (10M simulations)
    scenarios = spark.range(10000000).select(
        (rand() * 2 - 1).alias("market_shock"),
        (rand() * 0.5).alias("credit_shock"),
        (rand() * 0.3).alias("operational_shock")
    )

    # Apply shocks to portfolio
    portfolio_scenarios = portfolio_data.crossJoin(scenarios).select(
        "*",
        (col("position_value") *
         (1 + col("market_shock") * col("beta") +
          col("credit_shock") * col("credit_risk") +
          col("operational_shock") * 0.1)).alias("shocked_value")
    )

    # Calculate portfolio P&L for each scenario
    portfolio_pnl = portfolio_scenarios.groupBy("market_shock", "credit_shock", "operational_shock").agg(
        sum("shocked_value").alias("total_portfolio_value")
    ).withColumn(
        "portfolio_pnl",
        col("total_portfolio_value") - sum("position_value")
    )

    # Calculate VaR
    var_threshold = portfolio_pnl.approxQuantile("portfolio_pnl", [1 - confidence_level], 0.01)[0]

    return var_threshold

```

---

## ğŸ“ **Datenbeschaffung und -integration**

### **Datenquellen im Banking-Kontext**

### **Interne Datenquellen**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Core Banking System                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ Transaktionsdaten (Payment Processing)           â”‚
â”‚ â€¢ Kundenstammdaten (CRM)                          â”‚
â”‚ â€¢ Kontoinformationen (Account Management)          â”‚
â”‚ â€¢ Kredithistorie (Risk Management)                 â”‚
â”‚ â€¢ Produktportfolio (Product Database)              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 Digital Channels                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ Mobile Banking Logs                              â”‚
â”‚ â€¢ Online Banking Clickstreams                      â”‚
â”‚ â€¢ ATM Transaction Logs                             â”‚
â”‚ â€¢ Call Center Interactions                         â”‚
â”‚ â€¢ Chat/Support Communications                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

```

### **Externe Datenquellen**

```python
# External Data Integration fÃ¼r Enhanced Analytics
external_data_sources = {
    "market_data": {
        "provider": "Bloomberg/Reuters",
        "frequency": "real_time",
        "data_types": ["stock_prices", "fx_rates", "interest_rates"],
        "volume": "1M records/day"
    },
    "credit_bureaus": {
        "provider": "Schufa/Experian",
        "frequency": "on_demand",
        "data_types": ["credit_scores", "payment_history"],
        "volume": "100K requests/day"
    },
    "economic_indicators": {
        "provider": "Government APIs",
        "frequency": "monthly",
        "data_types": ["inflation", "unemployment", "gdp"],
        "volume": "1K indicators"
    },
    "social_sentiment": {
        "provider": "Social Media APIs",
        "frequency": "hourly",
        "data_types": ["brand_mentions", "sentiment_scores"],
        "volume": "50K mentions/day"
    }
}

```

### **Datenextraktion Strategien**

### **1. Batch ETL fÃ¼r Historical Data**

```python
# Daily batch ETL fÃ¼r Transaktionsdaten
from pyspark.sql import SparkSession
from datetime import datetime, timedelta

def daily_transaction_etl(spark, processing_date):
    """
    Extract yesterday's transactions and load into data lake
    """

    # Extract from core banking database
    yesterday = processing_date - timedelta(days=1)

    transactions = spark.read \
        .format("jdbc") \
        .option("url", "jdbc:oracle:thin:@banking-db:1521:PROD") \
        .option("dbtable", f"""
            (SELECT * FROM transactions
             WHERE transaction_date = '{yesterday.strftime('%Y-%m-%d')}')
        """) \
        .option("user", "etl_user") \
        .option("password", "etl_password") \
        .load()

    # Transform: Add derived fields
    enriched_transactions = transactions \
        .withColumn("transaction_hour", hour("transaction_timestamp")) \
        .withColumn("is_weekend", dayofweek("transaction_date").isin([1, 7])) \
        .withColumn("amount_category",
                   when(col("amount") < 10, "micro")
                   .when(col("amount") < 100, "small")
                   .when(col("amount") < 1000, "medium")
                   .otherwise("large"))

    # Load to data lake (partitioned by date)
    enriched_transactions.write \
        .mode("overwrite") \
        .partitionBy("transaction_date") \
        .parquet(f"gs://banking-datalake/transactions/year={yesterday.year}/month={yesterday.month}")

    return f"Processed {enriched_transactions.count()} transactions for {yesterday}"

```

### **2. Real-time Streaming fÃ¼r Live Data**

```python
# Kafka-based real-time ingestion
from pyspark.sql.functions import *
from pyspark.sql.types import *

# Define transaction schema
transaction_schema = StructType([
    StructField("transaction_id", StringType(), True),
    StructField("customer_id", StringType(), True),
    StructField("amount", DecimalType(10,2), True),
    StructField("merchant", StringType(), True),
    StructField("timestamp", TimestampType(), True),
    StructField("channel", StringType(), True)
])

# Read streaming data from Kafka
streaming_transactions = spark \
    .readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", "banking-kafka:9092") \
    .option("subscribe", "live-transactions") \
    .load() \
    .select(from_json(col("value").cast("string"), transaction_schema).alias("data")) \
    .select("data.*")

# Real-time fraud scoring
fraud_scored_stream = streaming_transactions \
    .withColumn("fraud_score",
               when(col("amount") > 5000, 0.8)
               .when(col("channel") == "ATM", 0.3)
               .otherwise(0.1)) \
    .withColumn("risk_level",
               when(col("fraud_score") > 0.7, "HIGH")
               .when(col("fraud_score") > 0.3, "MEDIUM")
               .otherwise("LOW"))

# Write to multiple sinks
query = fraud_scored_stream.writeStream \
    .outputMode("append") \
    .foreachBatch(lambda df, epoch: [
        df.write.format("delta").mode("append").save("gs://fraud-detection/live-scores"),
        df.filter(col("risk_level") == "HIGH").write.format("kafka")
          .option("kafka.bootstrap.servers", "alert-kafka:9092")
          .option("topic", "fraud-alerts").save()
    ]) \
    .start()

```

### **3. API Integration fÃ¼r External Data**

```python
# External API data integration
import requests
from concurrent.futures import ThreadPoolExecutor
import pandas as pd

class ExternalDataIntegrator:
    def __init__(self, spark):
        self.spark = spark

    def fetch_market_data(self, symbols, date_range):
        """
        Fetch market data from external APIs
        """
        def fetch_symbol_data(symbol):
            response = requests.get(
                f"https://api.marketdata.com/v1/stocks/{symbol}/quotes",
                params={
                    "from": date_range["start"],
                    "to": date_range["end"],
                    "token": "market_api_token"
                }
            )
            return response.json()

        # Parallel API calls
        with ThreadPoolExecutor(max_workers=10) as executor:
            results = list(executor.map(fetch_symbol_data, symbols))

        # Convert to Spark DataFrame
        market_df = self.spark.createDataFrame(
            pd.DataFrame([item for sublist in results for item in sublist])
        )

        return market_df

    def enrich_transactions_with_market_data(self, transactions_df):
        """
        Enrich transactions with market context
        """
        # Get relevant market data
        market_data = self.fetch_market_data(
            symbols=["^GDAXI", "EURUSD=X"],
            date_range={"start": "2024-01-01", "end": "2024-12-31"}
        )

        # Join transactions with market data
        enriched = transactions_df.join(
            market_data.select("date", "market_volatility", "market_sentiment"),
            transactions_df.transaction_date == market_data.date,
            "left"
        )

        return enriched

```

### **Datenintegration Herausforderungen**

### **1. Schema Evolution Management**

```python
# Schema evolution handling mit Delta Lake
from delta.tables import DeltaTable

def handle_schema_evolution(new_data_df, target_table_path):
    """
    Gracefully handle schema changes in banking data
    """

    if DeltaTable.isDeltaTable(spark, target_table_path):
        # Table exists - check for schema evolution
        existing_table = DeltaTable.forPath(spark, target_table_path)

        # Schema comparison
        existing_schema = existing_table.toDF().schema
        new_schema = new_data_df.schema

        # Add new columns if they don't exist
        for field in new_schema.fields:
            if field.name not in [f.name for f in existing_schema.fields]:
                existing_table.toDF().withColumn(field.name, lit(None).cast(field.dataType))

        # Merge with schema evolution
        existing_table.alias("existing").merge(
            new_data_df.alias("new"),
            "existing.transaction_id = new.transaction_id"
        ).whenMatchedUpdateAll() \
         .whenNotMatchedInsertAll() \
         .execute()
    else:
        # First load - create table
        new_data_df.write.format("delta").save(target_table_path)

```

### **2. Data Quality Across Sources**

```python
# Multi-source data quality validation
class DataQualityValidator:

    def __init__(self, spark):
        self.spark = spark
        self.quality_rules = {
            "transactions": {
                "completeness": ["transaction_id", "customer_id", "amount"],
                "validity": {
                    "amount": "amount > 0 AND amount < 1000000",
                    "currency": "currency IN ('EUR', 'USD', 'GBP')",
                    "timestamp": "timestamp IS NOT NULL"
                },
                "uniqueness": ["transaction_id"]
            },
            "customers": {
                "completeness": ["customer_id", "email"],
                "validity": {
                    "email": "email RLIKE '^[A-Za-z0-9+_.-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}",
                    "birth_date": "birth_date < current_date() AND birth_date > '1900-01-01'"
                },
                "uniqueness": ["customer_id", "email"]
            }
        }

    def validate_dataset(self, df, dataset_type):
        """
        Comprehensive data quality validation
        """
        rules = self.quality_rules.get(dataset_type, {})
        quality_report = {}

        # Completeness checks
        for column in rules.get("completeness", []):
            null_count = df.filter(col(column).isNull()).count()
            total_count = df.count()
            quality_report[f"{column}_completeness"] = 1 - (null_count / total_count)

        # Validity checks
        for column, condition in rules.get("validity", {}).items():
            valid_count = df.filter(condition).count()
            total_count = df.count()
            quality_report[f"{column}_validity"] = valid_count / total_count

        # Uniqueness checks
        for column in rules.get("uniqueness", []):
            unique_count = df.select(column).distinct().count()
            total_count = df.count()
            quality_report[f"{column}_uniqueness"] = unique_count / total_count

        return quality_report

    def create_quality_dashboard(self, quality_reports):
        """
        Generate data quality monitoring dashboard
        """
        quality_df = self.spark.createDataFrame([
            (dataset, metric, score, datetime.now())
            for dataset, metrics in quality_reports.items()
            for metric, score in metrics.items()
        ], ["dataset", "metric", "quality_score", "timestamp"])

        # Save for monitoring dashboard
        quality_df.write.mode("append").saveAsTable("data_quality.monitoring")

        return quality_df

```

---

## ğŸ”§ **Hands-On Vorbereitung**

### **Environment Setup Checkliste**

### **Local Development Setup**

```bash
# 1. Python Environment
python -m venv bigdata_workshop
source bigdata_workshop/bin/activate  # Linux/Mac
# bigdata_workshop\Scripts\activate   # Windows

# 2. Core Packages Installation
pip install --upgrade pip
pip install pyspark==3.5.0
pip install pandas numpy matplotlib seaborn plotly
pip install jupyter notebook ipython
pip install requests beautifulsoup4 lxml

# 3. Java Installation Verification
java -version  # Should show Java 17+
echo $JAVA_HOME  # Should point to Java installation

# 4. Spark Installation Test
python -c "from pyspark.sql import SparkSession; print('Spark import successful')"

# 5. Memory Configuration
export SPARK_DRIVER_MEMORY=4g
export SPARK_EXECUTOR_MEMORY=2g

```

### **Data Download and Verification**

```python
# Data integrity verification script
import os
import hashlib
import requests
from pathlib import Path

def download_workshop_data():
    """
    Download and verify workshop datasets
    """

    datasets = {
        "banking_transactions_1M.csv": {
            "url": "https://workshop-data.example.com/transactions_1M.csv",
            "size_mb": 200,
            "checksum": "a1b2c3d4e5f6..."
        },
        "banking_transactions_5M.csv": {
            "url": "https://workshop-data.example.com/transactions_5M.csv",
            "size_mb": 1000,
            "checksum": "f6e5d4c3b2a1..."
        },
        "customer_data.csv": {
            "url": "https://workshop-data.example.com/customers.csv",
            "size_mb": 50,
            "checksum": "1a2b3c4d5e6f..."
        }
    }

    data_dir = Path("workshop_data")
    data_dir.mkdir(exist_ok=True)

    for filename, info in datasets.items():
        file_path = data_dir / filename

        if not file_path.exists():
            print(f"Downloading {filename}...")
            response = requests.get(info["url"], stream=True)

            with open(file_path, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    f.write(chunk)

        # Verify file integrity
        with open(file_path, 'rb') as f:
            file_hash = hashlib.md5(f.read()).hexdigest()

        if file_hash == info["checksum"]:
            print(f"âœ… {filename} verified successfully")
        else:
            print(f"âŒ {filename} checksum mismatch - please re-download")

    print("Data download complete!")

if __name__ == "__main__":
    download_workshop_data()

```

### **Banking Dataset Overview**

### **Synthetische DatensÃ¤tze (GDPR-konform)**

```python
# Dataset schemas and sample data
banking_schemas = {
    "transactions": {
        "schema": [
            ("transaction_id", "string"),
            ("customer_id", "string"),
            ("amount", "decimal(10,2)"),
            ("currency", "string"),
            ("merchant", "string"),
            ("merchant_category", "string"),
            ("transaction_type", "string"),
            ("channel", "string"),
            ("timestamp", "timestamp"),
            ("location_country", "string"),
            ("location_city", "string"),
            ("is_weekend", "boolean"),
            ("description", "string")
        ],
        "sample_data": {
            "transaction_id": "TXN_2024_000001",
            "customer_id": "CUST_12345",
            "amount": 45.67,
            "currency": "EUR",
            "merchant": "REWE Supermarket",
            "merchant_category": "Grocery",
            "transaction_type": "purchase",
            "channel": "card",
            "timestamp": "2024-08-09 14:23:15",
            "location_country": "DE",
            "location_city": "Frankfurt",
            "is_weekend": False,
            "description": "Weekly grocery shopping"
        },
        "volume": "5M records",
        "time_range": "2019-2024"
    },

    "customers": {
        "schema": [
            ("customer_id", "string"),
            ("age", "integer"),
            ("gender", "string"),
            ("income_bracket", "string"),
            ("city", "string"),
            ("country", "string"),
            ("account_type", "string"),
            ("registration_date", "date"),
            ("credit_score", "integer"),
            ("email", "string"),
            ("phone", "string"),
            ("preferred_channel", "string")
        ],
        "sample_data": {
            "customer_id": "CUST_12345",
            "age": 34,
            "gender": "F",
            "income_bracket": "50k-75k",
            "city": "Frankfurt",
            "country": "DE",
            "account_type": "premium",
            "registration_date": "2020-03-15",
            "credit_score": 750,
            "email": "customer@example.com",
            "phone": "+49-xxx-xxxxxxx",
            "preferred_channel": "mobile"
        },
        "volume": "100K records"
    }
}

```

Diese umfassende theoretische Grundlage bietet das notwendige Fundament fÃ¼r den praktischen Workshop-Tag 1 und kann direkt fÃ¼r PrÃ¤sentationen und Dokumentation verwendet werden.
