{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05e261dc",
   "metadata": {},
   "source": [
    "# Banking Data Analysis - Live Coding Workshop\n",
    "## Big Data Analytics im Banking | 13:00-15:40\n",
    "\n",
    "### ðŸŽ¯ **Workshop Agenda**\n",
    "- **13:00-13:45:** EinfÃ¼hrung in Datenanalyse + Banking Transaction Analysis\n",
    "- **13:55-14:40:** Spark Deep-Dive & GCP Setup\n",
    "- **14:50-15:40:** Datenbeschaffung und -integration\n",
    "\n",
    "### ðŸ›  **Was wir heute lernen:**\n",
    "1. **Datenanalyseprozess** in der Praxis\n",
    "2. **Data Mining** fÃ¼r Banking-Patterns\n",
    "3. **Spark Setup** und SQL-Queries\n",
    "4. **GCP/Databricks** Configuration\n",
    "5. **Web Scraping** fÃ¼r Financial Data\n",
    "6. **Multi-Source Integration**\n",
    "\n",
    "### ðŸ“‹ **Live Coding Approach**\n",
    "- **Instructor demonstrates** â†’ **Students modify/extend**\n",
    "- **Short code blocks** with thorough comments\n",
    "- **Interactive exercises** at each step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b73afc",
   "metadata": {},
   "source": [
    "## 1. Load Large Banking Transactions (PySpark) ðŸ¦\n",
    "**Goal:** Load a >1GB CSV efficiently using PySpark and prepare it for analysis\n",
    "\n",
    "Dataset: `transactions_data.csv` (set the path below)\n",
    "\n",
    "### ðŸŽ“ Live Coding Exercise:\n",
    "- **Instructor:** Sets up Spark and loads the dataset with an explicit schema or fast inference\n",
    "- **Students:** Add derived columns and validate data quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9f66ed9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/08/10 12:16:47 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/08/10 12:16:47 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "25/08/10 12:16:47 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Spark initialized for large dataset processing\n",
      "ðŸ”§ Spark version: 3.5.3\n"
     ]
    }
   ],
   "source": [
    "# Essential PySpark setup for large-scale banking data\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Path to the large CSV dataset (>1GB), relative to this notebook's working directory\n",
    "# Ensure the file 'transactions_data.csv' is in the same folder as this notebook\n",
    "dataset_path = \"transactions_data.csv\"\n",
    "\n",
    "# Create Spark session optimized for local analysis of large CSVs\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"Banking Transactions Analysis\")\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\")\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"200\")  # tune based on cores\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n",
    "    .config(\"spark.sql.session.timeZone\", \"Europe/Berlin\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "print(\"âœ… Spark initialized for large dataset processing\")\n",
    "print(f\"ðŸ”§ Spark version: {spark.version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f01ebe08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‹ Raw schema:\n",
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- date: timestamp (nullable = true)\n",
      " |-- client_id: integer (nullable = true)\n",
      " |-- card_id: integer (nullable = true)\n",
      " |-- amount: string (nullable = true)\n",
      " |-- use_chip: string (nullable = true)\n",
      " |-- merchant_id: integer (nullable = true)\n",
      " |-- merchant_city: string (nullable = true)\n",
      " |-- merchant_state: string (nullable = true)\n",
      " |-- zip: double (nullable = true)\n",
      " |-- mcc: integer (nullable = true)\n",
      " |-- errors: string (nullable = true)\n",
      "\n",
      "\n",
      "ðŸ”Ž Sample rows:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/10 12:17:22 WARN MemoryStore: Not enough space to cache rdd_17_120 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:22 WARN BlockManager: Persisting block rdd_17_120 to disk instead.\n",
      "25/08/10 12:17:22 WARN MemoryStore: Not enough space to cache rdd_17_120 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:22 WARN MemoryStore: Not enough space to cache rdd_17_122 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:22 WARN BlockManager: Persisting block rdd_17_122 to disk instead.\n",
      "25/08/10 12:17:22 WARN MemoryStore: Not enough space to cache rdd_17_120 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:22 WARN BlockManager: Persisting block rdd_17_120 to disk instead.\n",
      "25/08/10 12:17:22 WARN MemoryStore: Not enough space to cache rdd_17_120 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:22 WARN MemoryStore: Not enough space to cache rdd_17_122 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:22 WARN BlockManager: Persisting block rdd_17_122 to disk instead.\n",
      "25/08/10 12:17:22 WARN MemoryStore: Not enough space to cache rdd_17_122 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:22 WARN MemoryStore: Not enough space to cache rdd_17_123 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:22 WARN BlockManager: Persisting block rdd_17_123 to disk instead.\n",
      "25/08/10 12:17:22 WARN MemoryStore: Not enough space to cache rdd_17_125 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:22 WARN BlockManager: Persisting block rdd_17_125 to disk instead.\n",
      "25/08/10 12:17:22 WARN MemoryStore: Not enough space to cache rdd_17_125 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:22 WARN MemoryStore: Not enough space to cache rdd_17_123 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:22 WARN MemoryStore: Not enough space to cache rdd_17_124 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:22 WARN BlockManager: Persisting block rdd_17_124 to disk instead.\n",
      "25/08/10 12:17:22 WARN MemoryStore: Not enough space to cache rdd_17_124 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:22 WARN MemoryStore: Not enough space to cache rdd_17_122 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:22 WARN MemoryStore: Not enough space to cache rdd_17_123 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:22 WARN BlockManager: Persisting block rdd_17_123 to disk instead.\n",
      "25/08/10 12:17:22 WARN MemoryStore: Not enough space to cache rdd_17_125 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:22 WARN BlockManager: Persisting block rdd_17_125 to disk instead.\n",
      "25/08/10 12:17:22 WARN MemoryStore: Not enough space to cache rdd_17_125 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:22 WARN MemoryStore: Not enough space to cache rdd_17_123 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:22 WARN MemoryStore: Not enough space to cache rdd_17_124 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:22 WARN BlockManager: Persisting block rdd_17_124 to disk instead.\n",
      "25/08/10 12:17:22 WARN MemoryStore: Not enough space to cache rdd_17_124 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:22 WARN MemoryStore: Not enough space to cache rdd_17_127 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:22 WARN BlockManager: Persisting block rdd_17_127 to disk instead.\n",
      "25/08/10 12:17:22 WARN MemoryStore: Not enough space to cache rdd_17_126 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:22 WARN BlockManager: Persisting block rdd_17_126 to disk instead.\n",
      "25/08/10 12:17:22 WARN MemoryStore: Not enough space to cache rdd_17_127 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:22 WARN MemoryStore: Not enough space to cache rdd_17_126 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:22 WARN MemoryStore: Not enough space to cache rdd_17_128 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:22 WARN BlockManager: Persisting block rdd_17_128 to disk instead.\n",
      "25/08/10 12:17:22 WARN MemoryStore: Not enough space to cache rdd_17_127 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:22 WARN BlockManager: Persisting block rdd_17_127 to disk instead.\n",
      "25/08/10 12:17:22 WARN MemoryStore: Not enough space to cache rdd_17_126 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:22 WARN BlockManager: Persisting block rdd_17_126 to disk instead.\n",
      "25/08/10 12:17:22 WARN MemoryStore: Not enough space to cache rdd_17_127 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:22 WARN MemoryStore: Not enough space to cache rdd_17_126 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:22 WARN MemoryStore: Not enough space to cache rdd_17_128 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:22 WARN BlockManager: Persisting block rdd_17_128 to disk instead.\n",
      "25/08/10 12:17:22 WARN MemoryStore: Not enough space to cache rdd_17_128 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:22 WARN MemoryStore: Not enough space to cache rdd_17_130 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:22 WARN BlockManager: Persisting block rdd_17_130 to disk instead.\n",
      "25/08/10 12:17:22 WARN MemoryStore: Not enough space to cache rdd_17_129 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:22 WARN BlockManager: Persisting block rdd_17_129 to disk instead.\n",
      "25/08/10 12:17:22 WARN MemoryStore: Not enough space to cache rdd_17_130 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:22 WARN MemoryStore: Not enough space to cache rdd_17_129 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:22 WARN MemoryStore: Not enough space to cache rdd_17_131 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:22 WARN BlockManager: Persisting block rdd_17_131 to disk instead.\n",
      "25/08/10 12:17:22 WARN MemoryStore: Not enough space to cache rdd_17_132 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:22 WARN BlockManager: Persisting block rdd_17_132 to disk instead.\n",
      "25/08/10 12:17:22 WARN MemoryStore: Not enough space to cache rdd_17_131 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:22 WARN MemoryStore: Not enough space to cache rdd_17_132 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:22 WARN MemoryStore: Not enough space to cache rdd_17_128 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:22 WARN MemoryStore: Not enough space to cache rdd_17_130 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:22 WARN BlockManager: Persisting block rdd_17_130 to disk instead.\n",
      "25/08/10 12:17:22 WARN MemoryStore: Not enough space to cache rdd_17_129 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:22 WARN BlockManager: Persisting block rdd_17_129 to disk instead.\n",
      "25/08/10 12:17:22 WARN MemoryStore: Not enough space to cache rdd_17_130 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:22 WARN MemoryStore: Not enough space to cache rdd_17_129 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:22 WARN MemoryStore: Not enough space to cache rdd_17_131 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:22 WARN BlockManager: Persisting block rdd_17_131 to disk instead.\n",
      "25/08/10 12:17:22 WARN MemoryStore: Not enough space to cache rdd_17_132 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:22 WARN BlockManager: Persisting block rdd_17_132 to disk instead.\n",
      "25/08/10 12:17:22 WARN MemoryStore: Not enough space to cache rdd_17_131 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:22 WARN MemoryStore: Not enough space to cache rdd_17_132 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:22 WARN MemoryStore: Not enough space to cache rdd_17_135 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:22 WARN BlockManager: Persisting block rdd_17_135 to disk instead.\n",
      "25/08/10 12:17:22 WARN MemoryStore: Not enough space to cache rdd_17_135 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:23 WARN MemoryStore: Not enough space to cache rdd_17_133 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:23 WARN BlockManager: Persisting block rdd_17_133 to disk instead.\n",
      "25/08/10 12:17:23 WARN MemoryStore: Not enough space to cache rdd_17_133 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:23 WARN MemoryStore: Not enough space to cache rdd_17_134 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:23 WARN BlockManager: Persisting block rdd_17_134 to disk instead.\n",
      "25/08/10 12:17:23 WARN MemoryStore: Not enough space to cache rdd_17_134 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:23 WARN MemoryStore: Not enough space to cache rdd_17_136 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:23 WARN BlockManager: Persisting block rdd_17_136 to disk instead.\n",
      "25/08/10 12:17:23 WARN MemoryStore: Not enough space to cache rdd_17_136 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:22 WARN MemoryStore: Not enough space to cache rdd_17_135 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:22 WARN BlockManager: Persisting block rdd_17_135 to disk instead.\n",
      "25/08/10 12:17:22 WARN MemoryStore: Not enough space to cache rdd_17_135 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:23 WARN MemoryStore: Not enough space to cache rdd_17_133 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:23 WARN BlockManager: Persisting block rdd_17_133 to disk instead.\n",
      "25/08/10 12:17:23 WARN MemoryStore: Not enough space to cache rdd_17_133 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:23 WARN MemoryStore: Not enough space to cache rdd_17_134 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:23 WARN BlockManager: Persisting block rdd_17_134 to disk instead.\n",
      "25/08/10 12:17:23 WARN MemoryStore: Not enough space to cache rdd_17_134 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:23 WARN MemoryStore: Not enough space to cache rdd_17_136 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:23 WARN BlockManager: Persisting block rdd_17_136 to disk instead.\n",
      "25/08/10 12:17:23 WARN MemoryStore: Not enough space to cache rdd_17_136 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:23 WARN MemoryStore: Not enough space to cache rdd_17_137 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:23 WARN BlockManager: Persisting block rdd_17_137 to disk instead.\n",
      "25/08/10 12:17:23 WARN MemoryStore: Not enough space to cache rdd_17_138 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:23 WARN BlockManager: Persisting block rdd_17_138 to disk instead.\n",
      "25/08/10 12:17:23 WARN MemoryStore: Not enough space to cache rdd_17_137 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:23 WARN MemoryStore: Not enough space to cache rdd_17_138 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:23 WARN MemoryStore: Not enough space to cache rdd_17_139 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:23 WARN BlockManager: Persisting block rdd_17_139 to disk instead.\n",
      "25/08/10 12:17:23 WARN MemoryStore: Not enough space to cache rdd_17_139 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:23 WARN MemoryStore: Not enough space to cache rdd_17_140 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:23 WARN BlockManager: Persisting block rdd_17_140 to disk instead.\n",
      "25/08/10 12:17:23 WARN MemoryStore: Not enough space to cache rdd_17_140 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:23 WARN MemoryStore: Not enough space to cache rdd_17_141 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:23 WARN BlockManager: Persisting block rdd_17_141 to disk instead.\n",
      "25/08/10 12:17:23 WARN MemoryStore: Not enough space to cache rdd_17_142 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:23 WARN BlockManager: Persisting block rdd_17_142 to disk instead.\n",
      "25/08/10 12:17:23 WARN MemoryStore: Not enough space to cache rdd_17_141 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:23 WARN MemoryStore: Not enough space to cache rdd_17_142 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:23 WARN MemoryStore: Not enough space to cache rdd_17_137 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:23 WARN BlockManager: Persisting block rdd_17_137 to disk instead.\n",
      "25/08/10 12:17:23 WARN MemoryStore: Not enough space to cache rdd_17_138 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:23 WARN BlockManager: Persisting block rdd_17_138 to disk instead.\n",
      "25/08/10 12:17:23 WARN MemoryStore: Not enough space to cache rdd_17_137 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:23 WARN MemoryStore: Not enough space to cache rdd_17_138 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:23 WARN MemoryStore: Not enough space to cache rdd_17_139 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:23 WARN BlockManager: Persisting block rdd_17_139 to disk instead.\n",
      "25/08/10 12:17:23 WARN MemoryStore: Not enough space to cache rdd_17_139 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:23 WARN MemoryStore: Not enough space to cache rdd_17_140 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:23 WARN BlockManager: Persisting block rdd_17_140 to disk instead.\n",
      "25/08/10 12:17:23 WARN MemoryStore: Not enough space to cache rdd_17_140 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:23 WARN MemoryStore: Not enough space to cache rdd_17_141 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:23 WARN BlockManager: Persisting block rdd_17_141 to disk instead.\n",
      "25/08/10 12:17:23 WARN MemoryStore: Not enough space to cache rdd_17_142 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:23 WARN BlockManager: Persisting block rdd_17_142 to disk instead.\n",
      "25/08/10 12:17:23 WARN MemoryStore: Not enough space to cache rdd_17_141 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:23 WARN MemoryStore: Not enough space to cache rdd_17_142 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:23 WARN MemoryStore: Not enough space to cache rdd_17_143 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:23 WARN BlockManager: Persisting block rdd_17_143 to disk instead.\n",
      "25/08/10 12:17:23 WARN MemoryStore: Not enough space to cache rdd_17_143 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:23 WARN MemoryStore: Not enough space to cache rdd_17_144 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:23 WARN BlockManager: Persisting block rdd_17_144 to disk instead.\n",
      "25/08/10 12:17:23 WARN MemoryStore: Not enough space to cache rdd_17_144 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:23 WARN MemoryStore: Not enough space to cache rdd_17_143 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:23 WARN BlockManager: Persisting block rdd_17_143 to disk instead.\n",
      "25/08/10 12:17:23 WARN MemoryStore: Not enough space to cache rdd_17_143 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:23 WARN MemoryStore: Not enough space to cache rdd_17_144 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:23 WARN BlockManager: Persisting block rdd_17_144 to disk instead.\n",
      "25/08/10 12:17:23 WARN MemoryStore: Not enough space to cache rdd_17_144 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:23 WARN MemoryStore: Not enough space to cache rdd_17_147 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:23 WARN BlockManager: Persisting block rdd_17_147 to disk instead.\n",
      "25/08/10 12:17:23 WARN MemoryStore: Not enough space to cache rdd_17_148 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:23 WARN BlockManager: Persisting block rdd_17_148 to disk instead.\n",
      "25/08/10 12:17:23 WARN MemoryStore: Not enough space to cache rdd_17_146 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:23 WARN BlockManager: Persisting block rdd_17_146 to disk instead.\n",
      "25/08/10 12:17:23 WARN MemoryStore: Not enough space to cache rdd_17_145 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:23 WARN BlockManager: Persisting block rdd_17_145 to disk instead.\n",
      "25/08/10 12:17:23 WARN MemoryStore: Not enough space to cache rdd_17_146 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:23 WARN MemoryStore: Not enough space to cache rdd_17_148 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:23 WARN MemoryStore: Not enough space to cache rdd_17_145 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:23 WARN MemoryStore: Not enough space to cache rdd_17_147 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:23 WARN MemoryStore: Not enough space to cache rdd_17_150 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:23 WARN BlockManager: Persisting block rdd_17_150 to disk instead.\n",
      "25/08/10 12:17:24 WARN MemoryStore: Not enough space to cache rdd_17_150 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:23 WARN MemoryStore: Not enough space to cache rdd_17_147 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:23 WARN BlockManager: Persisting block rdd_17_147 to disk instead.\n",
      "25/08/10 12:17:23 WARN MemoryStore: Not enough space to cache rdd_17_148 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:23 WARN BlockManager: Persisting block rdd_17_148 to disk instead.\n",
      "25/08/10 12:17:23 WARN MemoryStore: Not enough space to cache rdd_17_146 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:23 WARN BlockManager: Persisting block rdd_17_146 to disk instead.\n",
      "25/08/10 12:17:23 WARN MemoryStore: Not enough space to cache rdd_17_145 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:23 WARN BlockManager: Persisting block rdd_17_145 to disk instead.\n",
      "25/08/10 12:17:23 WARN MemoryStore: Not enough space to cache rdd_17_146 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:23 WARN MemoryStore: Not enough space to cache rdd_17_148 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:23 WARN MemoryStore: Not enough space to cache rdd_17_145 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:23 WARN MemoryStore: Not enough space to cache rdd_17_147 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:23 WARN MemoryStore: Not enough space to cache rdd_17_150 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:23 WARN BlockManager: Persisting block rdd_17_150 to disk instead.\n",
      "25/08/10 12:17:24 WARN MemoryStore: Not enough space to cache rdd_17_150 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:24 WARN MemoryStore: Not enough space to cache rdd_17_149 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:24 WARN BlockManager: Persisting block rdd_17_149 to disk instead.\n",
      "25/08/10 12:17:24 WARN MemoryStore: Not enough space to cache rdd_17_151 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:24 WARN BlockManager: Persisting block rdd_17_151 to disk instead.\n",
      "25/08/10 12:17:24 WARN MemoryStore: Not enough space to cache rdd_17_149 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:24 WARN MemoryStore: Not enough space to cache rdd_17_151 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:24 WARN MemoryStore: Not enough space to cache rdd_17_152 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:24 WARN BlockManager: Persisting block rdd_17_152 to disk instead.\n",
      "25/08/10 12:17:24 WARN MemoryStore: Not enough space to cache rdd_17_152 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:24 WARN MemoryStore: Not enough space to cache rdd_17_149 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:24 WARN BlockManager: Persisting block rdd_17_149 to disk instead.\n",
      "25/08/10 12:17:24 WARN MemoryStore: Not enough space to cache rdd_17_151 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:24 WARN BlockManager: Persisting block rdd_17_151 to disk instead.\n",
      "25/08/10 12:17:24 WARN MemoryStore: Not enough space to cache rdd_17_149 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:24 WARN MemoryStore: Not enough space to cache rdd_17_151 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:24 WARN MemoryStore: Not enough space to cache rdd_17_152 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:24 WARN BlockManager: Persisting block rdd_17_152 to disk instead.\n",
      "25/08/10 12:17:24 WARN MemoryStore: Not enough space to cache rdd_17_152 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:24 WARN MemoryStore: Not enough space to cache rdd_17_156 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:24 WARN BlockManager: Persisting block rdd_17_156 to disk instead.\n",
      "25/08/10 12:17:24 WARN MemoryStore: Not enough space to cache rdd_17_156 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:24 WARN MemoryStore: Not enough space to cache rdd_17_154 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:24 WARN BlockManager: Persisting block rdd_17_154 to disk instead.\n",
      "25/08/10 12:17:24 WARN MemoryStore: Not enough space to cache rdd_17_155 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:24 WARN BlockManager: Persisting block rdd_17_155 to disk instead.\n",
      "25/08/10 12:17:24 WARN MemoryStore: Not enough space to cache rdd_17_154 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:24 WARN MemoryStore: Not enough space to cache rdd_17_155 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:24 WARN MemoryStore: Not enough space to cache rdd_17_153 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:24 WARN BlockManager: Persisting block rdd_17_153 to disk instead.\n",
      "25/08/10 12:17:24 WARN MemoryStore: Not enough space to cache rdd_17_153 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:24 WARN MemoryStore: Not enough space to cache rdd_17_156 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:24 WARN BlockManager: Persisting block rdd_17_156 to disk instead.\n",
      "25/08/10 12:17:24 WARN MemoryStore: Not enough space to cache rdd_17_156 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:24 WARN MemoryStore: Not enough space to cache rdd_17_154 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:24 WARN BlockManager: Persisting block rdd_17_154 to disk instead.\n",
      "25/08/10 12:17:24 WARN MemoryStore: Not enough space to cache rdd_17_155 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:24 WARN BlockManager: Persisting block rdd_17_155 to disk instead.\n",
      "25/08/10 12:17:24 WARN MemoryStore: Not enough space to cache rdd_17_154 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:24 WARN MemoryStore: Not enough space to cache rdd_17_155 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:24 WARN MemoryStore: Not enough space to cache rdd_17_153 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:24 WARN BlockManager: Persisting block rdd_17_153 to disk instead.\n",
      "25/08/10 12:17:24 WARN MemoryStore: Not enough space to cache rdd_17_153 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:24 WARN MemoryStore: Not enough space to cache rdd_17_157 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:24 WARN BlockManager: Persisting block rdd_17_157 to disk instead.\n",
      "25/08/10 12:17:24 WARN MemoryStore: Not enough space to cache rdd_17_158 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:24 WARN BlockManager: Persisting block rdd_17_158 to disk instead.\n",
      "25/08/10 12:17:24 WARN MemoryStore: Not enough space to cache rdd_17_157 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:24 WARN MemoryStore: Not enough space to cache rdd_17_158 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:24 WARN MemoryStore: Not enough space to cache rdd_17_159 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:24 WARN BlockManager: Persisting block rdd_17_159 to disk instead.\n",
      "25/08/10 12:17:24 WARN MemoryStore: Not enough space to cache rdd_17_159 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:24 WARN MemoryStore: Not enough space to cache rdd_17_157 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:24 WARN BlockManager: Persisting block rdd_17_157 to disk instead.\n",
      "25/08/10 12:17:24 WARN MemoryStore: Not enough space to cache rdd_17_158 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:24 WARN BlockManager: Persisting block rdd_17_158 to disk instead.\n",
      "25/08/10 12:17:24 WARN MemoryStore: Not enough space to cache rdd_17_157 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:24 WARN MemoryStore: Not enough space to cache rdd_17_158 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:24 WARN MemoryStore: Not enough space to cache rdd_17_159 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:24 WARN BlockManager: Persisting block rdd_17_159 to disk instead.\n",
      "25/08/10 12:17:24 WARN MemoryStore: Not enough space to cache rdd_17_159 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:24 WARN MemoryStore: Not enough space to cache rdd_17_160 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:24 WARN BlockManager: Persisting block rdd_17_160 to disk instead.\n",
      "25/08/10 12:17:24 WARN MemoryStore: Not enough space to cache rdd_17_160 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:24 WARN MemoryStore: Not enough space to cache rdd_17_160 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:24 WARN BlockManager: Persisting block rdd_17_160 to disk instead.\n",
      "25/08/10 12:17:24 WARN MemoryStore: Not enough space to cache rdd_17_160 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:24 WARN MemoryStore: Not enough space to cache rdd_17_163 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:24 WARN BlockManager: Persisting block rdd_17_163 to disk instead.\n",
      "25/08/10 12:17:25 WARN MemoryStore: Not enough space to cache rdd_17_162 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:25 WARN BlockManager: Persisting block rdd_17_162 to disk instead.\n",
      "25/08/10 12:17:25 WARN MemoryStore: Not enough space to cache rdd_17_163 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:25 WARN MemoryStore: Not enough space to cache rdd_17_161 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:25 WARN BlockManager: Persisting block rdd_17_161 to disk instead.\n",
      "25/08/10 12:17:25 WARN MemoryStore: Not enough space to cache rdd_17_162 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:25 WARN MemoryStore: Not enough space to cache rdd_17_161 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:24 WARN MemoryStore: Not enough space to cache rdd_17_163 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:24 WARN BlockManager: Persisting block rdd_17_163 to disk instead.\n",
      "25/08/10 12:17:25 WARN MemoryStore: Not enough space to cache rdd_17_162 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:25 WARN BlockManager: Persisting block rdd_17_162 to disk instead.\n",
      "25/08/10 12:17:25 WARN MemoryStore: Not enough space to cache rdd_17_163 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:25 WARN MemoryStore: Not enough space to cache rdd_17_161 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:25 WARN BlockManager: Persisting block rdd_17_161 to disk instead.\n",
      "25/08/10 12:17:25 WARN MemoryStore: Not enough space to cache rdd_17_162 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:25 WARN MemoryStore: Not enough space to cache rdd_17_161 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:25 WARN MemoryStore: Not enough space to cache rdd_17_164 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:25 WARN BlockManager: Persisting block rdd_17_164 to disk instead.\n",
      "25/08/10 12:17:25 WARN MemoryStore: Not enough space to cache rdd_17_166 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:25 WARN BlockManager: Persisting block rdd_17_166 to disk instead.\n",
      "25/08/10 12:17:25 WARN MemoryStore: Not enough space to cache rdd_17_165 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:25 WARN BlockManager: Persisting block rdd_17_165 to disk instead.\n",
      "25/08/10 12:17:25 WARN MemoryStore: Not enough space to cache rdd_17_166 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:25 WARN MemoryStore: Not enough space to cache rdd_17_165 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:25 WARN MemoryStore: Not enough space to cache rdd_17_164 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:25 WARN MemoryStore: Not enough space to cache rdd_17_167 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:25 WARN BlockManager: Persisting block rdd_17_167 to disk instead.\n",
      "25/08/10 12:17:25 WARN MemoryStore: Not enough space to cache rdd_17_167 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:25 WARN MemoryStore: Not enough space to cache rdd_17_168 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:25 WARN BlockManager: Persisting block rdd_17_168 to disk instead.\n",
      "25/08/10 12:17:25 WARN MemoryStore: Not enough space to cache rdd_17_168 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:25 WARN MemoryStore: Not enough space to cache rdd_17_164 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:25 WARN BlockManager: Persisting block rdd_17_164 to disk instead.\n",
      "25/08/10 12:17:25 WARN MemoryStore: Not enough space to cache rdd_17_166 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:25 WARN BlockManager: Persisting block rdd_17_166 to disk instead.\n",
      "25/08/10 12:17:25 WARN MemoryStore: Not enough space to cache rdd_17_165 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:25 WARN BlockManager: Persisting block rdd_17_165 to disk instead.\n",
      "25/08/10 12:17:25 WARN MemoryStore: Not enough space to cache rdd_17_166 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:25 WARN MemoryStore: Not enough space to cache rdd_17_165 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:25 WARN MemoryStore: Not enough space to cache rdd_17_164 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:25 WARN MemoryStore: Not enough space to cache rdd_17_167 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:25 WARN BlockManager: Persisting block rdd_17_167 to disk instead.\n",
      "25/08/10 12:17:25 WARN MemoryStore: Not enough space to cache rdd_17_167 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:25 WARN MemoryStore: Not enough space to cache rdd_17_168 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:25 WARN BlockManager: Persisting block rdd_17_168 to disk instead.\n",
      "25/08/10 12:17:25 WARN MemoryStore: Not enough space to cache rdd_17_168 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:25 WARN MemoryStore: Not enough space to cache rdd_17_171 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:25 WARN BlockManager: Persisting block rdd_17_171 to disk instead.\n",
      "25/08/10 12:17:25 WARN MemoryStore: Not enough space to cache rdd_17_171 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:25 WARN MemoryStore: Not enough space to cache rdd_17_170 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:25 WARN BlockManager: Persisting block rdd_17_170 to disk instead.\n",
      "25/08/10 12:17:25 WARN MemoryStore: Not enough space to cache rdd_17_169 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:25 WARN MemoryStore: Not enough space to cache rdd_17_170 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:25 WARN BlockManager: Persisting block rdd_17_169 to disk instead.\n",
      "25/08/10 12:17:25 WARN MemoryStore: Not enough space to cache rdd_17_169 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:25 WARN MemoryStore: Not enough space to cache rdd_17_173 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:25 WARN BlockManager: Persisting block rdd_17_173 to disk instead.\n",
      "25/08/10 12:17:25 WARN MemoryStore: Not enough space to cache rdd_17_173 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:25 WARN MemoryStore: Not enough space to cache rdd_17_172 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:25 WARN BlockManager: Persisting block rdd_17_172 to disk instead.\n",
      "25/08/10 12:17:25 WARN MemoryStore: Not enough space to cache rdd_17_174 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:25 WARN BlockManager: Persisting block rdd_17_174 to disk instead.\n",
      "25/08/10 12:17:25 WARN MemoryStore: Not enough space to cache rdd_17_174 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:25 WARN MemoryStore: Not enough space to cache rdd_17_172 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:25 WARN MemoryStore: Not enough space to cache rdd_17_171 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:25 WARN BlockManager: Persisting block rdd_17_171 to disk instead.\n",
      "25/08/10 12:17:25 WARN MemoryStore: Not enough space to cache rdd_17_171 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:25 WARN MemoryStore: Not enough space to cache rdd_17_170 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:25 WARN BlockManager: Persisting block rdd_17_170 to disk instead.\n",
      "25/08/10 12:17:25 WARN MemoryStore: Not enough space to cache rdd_17_169 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:25 WARN MemoryStore: Not enough space to cache rdd_17_170 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:25 WARN BlockManager: Persisting block rdd_17_169 to disk instead.\n",
      "25/08/10 12:17:25 WARN MemoryStore: Not enough space to cache rdd_17_169 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:25 WARN MemoryStore: Not enough space to cache rdd_17_173 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:25 WARN BlockManager: Persisting block rdd_17_173 to disk instead.\n",
      "25/08/10 12:17:25 WARN MemoryStore: Not enough space to cache rdd_17_173 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:25 WARN MemoryStore: Not enough space to cache rdd_17_172 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:25 WARN BlockManager: Persisting block rdd_17_172 to disk instead.\n",
      "25/08/10 12:17:25 WARN MemoryStore: Not enough space to cache rdd_17_174 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:25 WARN BlockManager: Persisting block rdd_17_174 to disk instead.\n",
      "25/08/10 12:17:25 WARN MemoryStore: Not enough space to cache rdd_17_174 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:25 WARN MemoryStore: Not enough space to cache rdd_17_172 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:25 WARN MemoryStore: Not enough space to cache rdd_17_176 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:25 WARN BlockManager: Persisting block rdd_17_176 to disk instead.\n",
      "25/08/10 12:17:25 WARN MemoryStore: Not enough space to cache rdd_17_175 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:25 WARN BlockManager: Persisting block rdd_17_175 to disk instead.\n",
      "25/08/10 12:17:25 WARN MemoryStore: Not enough space to cache rdd_17_176 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:25 WARN MemoryStore: Not enough space to cache rdd_17_175 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:25 WARN MemoryStore: Not enough space to cache rdd_17_176 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:25 WARN BlockManager: Persisting block rdd_17_176 to disk instead.\n",
      "25/08/10 12:17:25 WARN MemoryStore: Not enough space to cache rdd_17_175 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:25 WARN BlockManager: Persisting block rdd_17_175 to disk instead.\n",
      "25/08/10 12:17:25 WARN MemoryStore: Not enough space to cache rdd_17_176 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:25 WARN MemoryStore: Not enough space to cache rdd_17_175 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:25 WARN MemoryStore: Not enough space to cache rdd_17_178 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:25 WARN BlockManager: Persisting block rdd_17_178 to disk instead.\n",
      "25/08/10 12:17:26 WARN MemoryStore: Not enough space to cache rdd_17_178 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:26 WARN MemoryStore: Not enough space to cache rdd_17_179 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:26 WARN BlockManager: Persisting block rdd_17_179 to disk instead.\n",
      "25/08/10 12:17:26 WARN MemoryStore: Not enough space to cache rdd_17_179 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:26 WARN MemoryStore: Not enough space to cache rdd_17_177 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:26 WARN BlockManager: Persisting block rdd_17_177 to disk instead.\n",
      "25/08/10 12:17:25 WARN MemoryStore: Not enough space to cache rdd_17_178 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:25 WARN BlockManager: Persisting block rdd_17_178 to disk instead.\n",
      "25/08/10 12:17:26 WARN MemoryStore: Not enough space to cache rdd_17_178 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:26 WARN MemoryStore: Not enough space to cache rdd_17_179 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:26 WARN BlockManager: Persisting block rdd_17_179 to disk instead.\n",
      "25/08/10 12:17:26 WARN MemoryStore: Not enough space to cache rdd_17_179 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:26 WARN MemoryStore: Not enough space to cache rdd_17_177 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:26 WARN BlockManager: Persisting block rdd_17_177 to disk instead.\n",
      "25/08/10 12:17:26 WARN MemoryStore: Not enough space to cache rdd_17_177 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:26 WARN MemoryStore: Not enough space to cache rdd_17_182 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:26 WARN BlockManager: Persisting block rdd_17_182 to disk instead.\n",
      "25/08/10 12:17:26 WARN MemoryStore: Not enough space to cache rdd_17_180 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:26 WARN BlockManager: Persisting block rdd_17_180 to disk instead.\n",
      "25/08/10 12:17:26 WARN MemoryStore: Not enough space to cache rdd_17_182 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:26 WARN MemoryStore: Not enough space to cache rdd_17_181 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:26 WARN BlockManager: Persisting block rdd_17_181 to disk instead.\n",
      "25/08/10 12:17:26 WARN MemoryStore: Not enough space to cache rdd_17_184 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:26 WARN MemoryStore: Not enough space to cache rdd_17_181 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:26 WARN BlockManager: Persisting block rdd_17_184 to disk instead.\n",
      "25/08/10 12:17:26 WARN MemoryStore: Not enough space to cache rdd_17_177 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:26 WARN MemoryStore: Not enough space to cache rdd_17_182 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:26 WARN BlockManager: Persisting block rdd_17_182 to disk instead.\n",
      "25/08/10 12:17:26 WARN MemoryStore: Not enough space to cache rdd_17_180 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:26 WARN BlockManager: Persisting block rdd_17_180 to disk instead.\n",
      "25/08/10 12:17:26 WARN MemoryStore: Not enough space to cache rdd_17_182 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:26 WARN MemoryStore: Not enough space to cache rdd_17_181 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:26 WARN BlockManager: Persisting block rdd_17_181 to disk instead.\n",
      "25/08/10 12:17:26 WARN MemoryStore: Not enough space to cache rdd_17_184 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:26 WARN MemoryStore: Not enough space to cache rdd_17_181 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:26 WARN BlockManager: Persisting block rdd_17_184 to disk instead.\n",
      "25/08/10 12:17:26 WARN MemoryStore: Not enough space to cache rdd_17_180 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:26 WARN MemoryStore: Not enough space to cache rdd_17_184 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:26 WARN MemoryStore: Not enough space to cache rdd_17_183 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:26 WARN BlockManager: Persisting block rdd_17_183 to disk instead.\n",
      "25/08/10 12:17:26 WARN MemoryStore: Not enough space to cache rdd_17_183 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:26 WARN MemoryStore: Not enough space to cache rdd_17_180 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:26 WARN MemoryStore: Not enough space to cache rdd_17_184 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:26 WARN MemoryStore: Not enough space to cache rdd_17_183 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:26 WARN BlockManager: Persisting block rdd_17_183 to disk instead.\n",
      "25/08/10 12:17:26 WARN MemoryStore: Not enough space to cache rdd_17_183 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:26 WARN MemoryStore: Not enough space to cache rdd_17_185 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:26 WARN BlockManager: Persisting block rdd_17_185 to disk instead.\n",
      "25/08/10 12:17:26 WARN MemoryStore: Not enough space to cache rdd_17_185 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:26 WARN MemoryStore: Not enough space to cache rdd_17_186 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:26 WARN BlockManager: Persisting block rdd_17_186 to disk instead.\n",
      "25/08/10 12:17:26 WARN MemoryStore: Not enough space to cache rdd_17_186 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:26 WARN MemoryStore: Not enough space to cache rdd_17_187 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:26 WARN BlockManager: Persisting block rdd_17_187 to disk instead.\n",
      "25/08/10 12:17:26 WARN MemoryStore: Not enough space to cache rdd_17_187 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:26 WARN MemoryStore: Not enough space to cache rdd_17_191 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:26 WARN BlockManager: Persisting block rdd_17_191 to disk instead.\n",
      "25/08/10 12:17:26 WARN MemoryStore: Not enough space to cache rdd_17_185 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:26 WARN BlockManager: Persisting block rdd_17_185 to disk instead.\n",
      "25/08/10 12:17:26 WARN MemoryStore: Not enough space to cache rdd_17_185 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:26 WARN MemoryStore: Not enough space to cache rdd_17_186 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:26 WARN BlockManager: Persisting block rdd_17_186 to disk instead.\n",
      "25/08/10 12:17:26 WARN MemoryStore: Not enough space to cache rdd_17_186 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:26 WARN MemoryStore: Not enough space to cache rdd_17_187 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:26 WARN BlockManager: Persisting block rdd_17_187 to disk instead.\n",
      "25/08/10 12:17:26 WARN MemoryStore: Not enough space to cache rdd_17_187 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:26 WARN MemoryStore: Not enough space to cache rdd_17_191 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:26 WARN BlockManager: Persisting block rdd_17_191 to disk instead.\n",
      "25/08/10 12:17:26 WARN MemoryStore: Not enough space to cache rdd_17_190 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:26 WARN BlockManager: Persisting block rdd_17_190 to disk instead.\n",
      "25/08/10 12:17:26 WARN MemoryStore: Not enough space to cache rdd_17_191 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:26 WARN MemoryStore: Not enough space to cache rdd_17_190 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:26 WARN MemoryStore: Not enough space to cache rdd_17_188 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:26 WARN BlockManager: Persisting block rdd_17_188 to disk instead.\n",
      "25/08/10 12:17:26 WARN MemoryStore: Not enough space to cache rdd_17_189 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:26 WARN BlockManager: Persisting block rdd_17_189 to disk instead.\n",
      "25/08/10 12:17:26 WARN MemoryStore: Not enough space to cache rdd_17_188 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:27 WARN MemoryStore: Not enough space to cache rdd_17_189 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:27 WARN MemoryStore: Not enough space to cache rdd_17_192 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:27 WARN BlockManager: Persisting block rdd_17_192 to disk instead.\n",
      "25/08/10 12:17:26 WARN MemoryStore: Not enough space to cache rdd_17_190 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:26 WARN BlockManager: Persisting block rdd_17_190 to disk instead.\n",
      "25/08/10 12:17:26 WARN MemoryStore: Not enough space to cache rdd_17_191 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:26 WARN MemoryStore: Not enough space to cache rdd_17_190 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:26 WARN MemoryStore: Not enough space to cache rdd_17_188 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:26 WARN BlockManager: Persisting block rdd_17_188 to disk instead.\n",
      "25/08/10 12:17:26 WARN MemoryStore: Not enough space to cache rdd_17_189 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:26 WARN BlockManager: Persisting block rdd_17_189 to disk instead.\n",
      "25/08/10 12:17:26 WARN MemoryStore: Not enough space to cache rdd_17_188 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:27 WARN MemoryStore: Not enough space to cache rdd_17_189 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:27 WARN MemoryStore: Not enough space to cache rdd_17_192 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:27 WARN BlockManager: Persisting block rdd_17_192 to disk instead.\n",
      "25/08/10 12:17:27 WARN MemoryStore: Not enough space to cache rdd_17_192 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:27 WARN MemoryStore: Not enough space to cache rdd_17_195 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:27 WARN BlockManager: Persisting block rdd_17_195 to disk instead.\n",
      "25/08/10 12:17:27 WARN MemoryStore: Not enough space to cache rdd_17_193 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:27 WARN BlockManager: Persisting block rdd_17_193 to disk instead.\n",
      "25/08/10 12:17:27 WARN MemoryStore: Not enough space to cache rdd_17_194 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:27 WARN BlockManager: Persisting block rdd_17_194 to disk instead.\n",
      "25/08/10 12:17:27 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_17_193 in memory.\n",
      "25/08/10 12:17:27 WARN MemoryStore: Not enough space to cache rdd_17_193 in memory! (computed 384.0 B so far)\n",
      "25/08/10 12:17:27 WARN MemoryStore: Not enough space to cache rdd_17_194 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:27 WARN MemoryStore: Not enough space to cache rdd_17_196 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:27 WARN BlockManager: Persisting block rdd_17_196 to disk instead.\n",
      "25/08/10 12:17:27 WARN MemoryStore: Not enough space to cache rdd_17_192 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:27 WARN MemoryStore: Not enough space to cache rdd_17_195 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:27 WARN BlockManager: Persisting block rdd_17_195 to disk instead.\n",
      "25/08/10 12:17:27 WARN MemoryStore: Not enough space to cache rdd_17_193 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:27 WARN BlockManager: Persisting block rdd_17_193 to disk instead.\n",
      "25/08/10 12:17:27 WARN MemoryStore: Not enough space to cache rdd_17_194 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:27 WARN BlockManager: Persisting block rdd_17_194 to disk instead.\n",
      "25/08/10 12:17:27 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_17_193 in memory.\n",
      "25/08/10 12:17:27 WARN MemoryStore: Not enough space to cache rdd_17_193 in memory! (computed 384.0 B so far)\n",
      "25/08/10 12:17:27 WARN MemoryStore: Not enough space to cache rdd_17_194 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:27 WARN MemoryStore: Not enough space to cache rdd_17_196 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:27 WARN BlockManager: Persisting block rdd_17_196 to disk instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+---------+-------+------+-----------------+-----------+-------------+--------------+-------+----+------+-------------------+\n",
      "|id     |date               |client_id|card_id|amount|use_chip         |merchant_id|merchant_city|merchant_state|zip    |mcc |errors|transaction_date   |\n",
      "+-------+-------------------+---------+-------+------+-----------------+-----------+-------------+--------------+-------+----+------+-------------------+\n",
      "|9123100|2011-02-07 06:44:00|1664     |5147   |NULL  |Swipe Transaction|83480      |Ann Arbor    |MI            |48103.0|9402|NULL  |2011-02-07 06:44:00|\n",
      "|9048269|2011-01-20 10:18:00|1575     |2112   |NULL  |Swipe Transaction|61195      |Sarasota     |FL            |34232.0|5541|NULL  |2011-01-20 10:18:00|\n",
      "|8263643|2010-07-16 11:36:00|1857     |5089   |NULL  |Swipe Transaction|91128      |Morris Plains|NJ            |7950.0 |5411|NULL  |2010-07-16 11:36:00|\n",
      "|8791129|2010-11-20 10:52:00|96       |3695   |NULL  |Swipe Transaction|41782      |Yorba Linda  |CA            |92886.0|5912|NULL  |2010-11-20 10:52:00|\n",
      "|9040753|2011-01-18 13:52:00|940      |5117   |NULL  |Swipe Transaction|83271      |Miami        |FL            |33155.0|4214|NULL  |2011-01-18 13:52:00|\n",
      "+-------+-------------------+---------+-------+------+-----------------+-----------+-------------+--------------+-------+----+------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "âœ… Temp view 'banking_transactions' is ready for SQL queries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/10 12:17:27 WARN MemoryStore: Not enough space to cache rdd_17_198 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:27 WARN BlockManager: Persisting block rdd_17_198 to disk instead.\n",
      "25/08/10 12:17:27 WARN MemoryStore: Not enough space to cache rdd_17_199 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:27 WARN BlockManager: Persisting block rdd_17_199 to disk instead.\n",
      "25/08/10 12:17:27 WARN MemoryStore: Not enough space to cache rdd_17_196 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:27 WARN MemoryStore: Not enough space to cache rdd_17_197 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:27 WARN BlockManager: Persisting block rdd_17_197 to disk instead.\n",
      "25/08/10 12:17:27 WARN MemoryStore: Not enough space to cache rdd_17_198 in memory! (computed 3.5 MiB so far)\n",
      "25/08/10 12:17:27 WARN MemoryStore: Not enough space to cache rdd_17_197 in memory! (computed 3.5 MiB so far)\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# ðŸ“¦ LOAD LARGE DATASET - Banking Transactions CSV (>1GB)\n",
    "# This cell loads the dataset using PySpark and prepares standard columns\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Optional: Define an explicit schema for best performance (fill in when known)\n",
    "# Example (adjust to your dataset columns):\n",
    "# explicit_schema = StructType([\n",
    "#     StructField(\"transaction_id\", StringType(), True),\n",
    "#     StructField(\"customer_id\", StringType(), True),\n",
    "#     StructField(\"merchant\", StringType(), True),\n",
    "#     StructField(\"amount\", DoubleType(), True),\n",
    "#     StructField(\"currency\", StringType(), True),\n",
    "#     StructField(\"timestamp\", StringType(), True),\n",
    "#     # ... add other fields\n",
    "# ])\n",
    "explicit_schema = None  # set to the StructType above when ready\n",
    "\n",
    "read_builder = (\n",
    "    spark.read\n",
    "        .option(\"header\", True)\n",
    "        .option(\"inferSchema\", explicit_schema is None)\n",
    "        .option(\"multiLine\", False)\n",
    "        .option(\"mode\", \"PERMISSIVE\")\n",
    ")\n",
    "\n",
    "transactions_raw = (\n",
    "    read_builder.csv(dataset_path) if explicit_schema is None\n",
    "    else read_builder.schema(explicit_schema).csv(dataset_path)\n",
    ")\n",
    "\n",
    "print(\"ðŸ“‹ Raw schema:\")\n",
    "transactions_raw.printSchema()\n",
    "\n",
    "cols = set([c.lower() for c in transactions_raw.columns])\n",
    "\n",
    "# Identify and standardize key columns\n",
    "# 1) transaction_date (timestamp)\n",
    "candidate_date_cols = [c for c in [\"transaction_date\", \"timestamp\", \"event_time\", \"date\", \"datetime\"] if c in cols]\n",
    "if candidate_date_cols:\n",
    "    date_col = [c for c in transactions_raw.columns if c.lower() == candidate_date_cols[0]][0]\n",
    "    transactions_std = transactions_raw.withColumn(\n",
    "        \"transaction_date\",\n",
    "        F.to_timestamp(F.col(date_col))\n",
    "    )\n",
    "else:\n",
    "    transactions_std = transactions_raw  # proceed without date if missing\n",
    "\n",
    "# 2) amount (double)\n",
    "candidate_amount_cols = [c for c in [\"amount\", \"amt\", \"value\", \"transaction_amount\"] if c in cols]\n",
    "if candidate_amount_cols:\n",
    "    amount_src = [c for c in transactions_raw.columns if c.lower() == candidate_amount_cols[0]][0]\n",
    "    if amount_src != \"amount\":\n",
    "        transactions_std = transactions_std.withColumn(\"amount\", F.col(amount_src).cast(\"double\"))\n",
    "    else:\n",
    "        transactions_std = transactions_std.withColumn(\"amount\", F.col(\"amount\").cast(\"double\"))\n",
    "\n",
    "# 3) merchant (string)\n",
    "if \"merchant\" not in cols:\n",
    "    for alt in [\"merchant_name\", \"store\", \"vendor\"]:\n",
    "        if alt in cols:\n",
    "            alt_src = [c for c in transactions_raw.columns if c.lower() == alt][0]\n",
    "            transactions_std = transactions_std.withColumnRenamed(alt_src, \"merchant\")\n",
    "            break\n",
    "\n",
    "# 4) customer_id (string)\n",
    "if \"customer_id\" not in cols:\n",
    "    for alt in [\"customer\", \"customerid\", \"cust_id\", \"account_id\"]:\n",
    "        if alt in cols:\n",
    "            alt_src = [c for c in transactions_raw.columns if c.lower() == alt][0]\n",
    "            transactions_std = transactions_std.withColumnRenamed(alt_src, \"customer_id\")\n",
    "            break\n",
    "\n",
    "# Light-weight normalization/derivations (safe for large data)\n",
    "transactions_std = (\n",
    "    transactions_std\n",
    "    .withColumn(\"transaction_date\", F.col(\"transaction_date\"))  # ensure exists if created\n",
    ")\n",
    "\n",
    "# Repartition and persist for interactive analysis\n",
    "spark_banking_df = transactions_std.repartition(200).persist()\n",
    "\n",
    "print(\"\\nðŸ”Ž Sample rows:\")\n",
    "spark_banking_df.show(5, truncate=False)\n",
    "\n",
    "# Create a temp view for SQL queries\n",
    "spark_banking_df.createOrReplaceTempView(\"banking_transactions\")\n",
    "print(\"âœ… Temp view 'banking_transactions' is ready for SQL queries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64adc563",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¯ COMPLETE: Add useful derived columns at scale (Spark-only)\n",
      "============================================================\n",
      "ðŸ“… Adding time-based features...\n",
      "ðŸª Standardizing merchant names...\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `merchant` cannot be resolved. Did you mean one of the following? [`merchant_id`, `errors`, `amount`, `mcc`, `merchant_city`].;\n'Project [id#17, date#18, client_id#19, card_id#20, amount#56, use_chip#22, merchant_id#23, merchant_city#24, merchant_state#25, zip#26, mcc#27, errors#28, transaction_date#70, txn_date#474, txn_hour#489, weekday_short#505, is_weekend#522, upper(trim('merchant, None)) AS merchant_std#540]\n+- Project [id#17, date#18, client_id#19, card_id#20, amount#56, use_chip#22, merchant_id#23, merchant_city#24, merchant_state#25, zip#26, mcc#27, errors#28, transaction_date#70, txn_date#474, txn_hour#489, weekday_short#505, CASE WHEN dayofweek(cast(transaction_date#70 as date)) IN (1,7) THEN true ELSE false END AS is_weekend#522]\n   +- Project [id#17, date#18, client_id#19, card_id#20, amount#56, use_chip#22, merchant_id#23, merchant_city#24, merchant_state#25, zip#26, mcc#27, errors#28, transaction_date#70, txn_date#474, txn_hour#489, date_format(transaction_date#70, E, Some(Europe/Berlin)) AS weekday_short#505]\n      +- Project [id#17, date#18, client_id#19, card_id#20, amount#56, use_chip#22, merchant_id#23, merchant_city#24, merchant_state#25, zip#26, mcc#27, errors#28, transaction_date#70, txn_date#474, hour(transaction_date#70, Some(Europe/Berlin)) AS txn_hour#489]\n         +- Project [id#17, date#18, client_id#19, card_id#20, amount#56, use_chip#22, merchant_id#23, merchant_city#24, merchant_state#25, zip#26, mcc#27, errors#28, transaction_date#70, to_date(transaction_date#70, None, Some(Europe/Berlin), false) AS txn_date#474]\n            +- Repartition 200, true\n               +- Project [id#17, date#18, client_id#19, card_id#20, amount#56, use_chip#22, merchant_id#23, merchant_city#24, merchant_state#25, zip#26, mcc#27, errors#28, transaction_date#41 AS transaction_date#70]\n                  +- Project [id#17, date#18, client_id#19, card_id#20, cast(amount#21 as double) AS amount#56, use_chip#22, merchant_id#23, merchant_city#24, merchant_state#25, zip#26, mcc#27, errors#28, transaction_date#41]\n                     +- Project [id#17, date#18, client_id#19, card_id#20, amount#21, use_chip#22, merchant_id#23, merchant_city#24, merchant_state#25, zip#26, mcc#27, errors#28, to_timestamp(date#18, None, TimestampType, Some(Europe/Berlin), false) AS transaction_date#41]\n                        +- Relation [id#17,date#18,client_id#19,card_id#20,amount#21,use_chip#22,merchant_id#23,merchant_city#24,merchant_state#25,zip#26,mcc#27,errors#28] csv\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Solution 2: Clean/standardize merchant values\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mðŸª Standardizing merchant names...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 19\u001b[0m spark_banking_df \u001b[38;5;241m=\u001b[39m \u001b[43mspark_banking_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwithColumn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmerchant_std\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mupper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrim\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmerchant\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \\\n\u001b[1;32m     20\u001b[0m                                    \u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmerchant_clean\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[1;32m     21\u001b[0m                                               when(col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmerchant\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39misNull() \u001b[38;5;241m|\u001b[39m (col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmerchant\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUNKNOWN\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     22\u001b[0m                                               \u001b[38;5;241m.\u001b[39motherwise(col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmerchant_std\u001b[39m\u001b[38;5;124m\"\u001b[39m)))\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Solution 3: Amount quality flags\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mðŸ’° Adding amount quality flags...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/Coding/big-data-analytics/.venv/lib/python3.12/site-packages/pyspark/sql/dataframe.py:5176\u001b[0m, in \u001b[0;36mDataFrame.withColumn\u001b[0;34m(self, colName, col)\u001b[0m\n\u001b[1;32m   5171\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(col, Column):\n\u001b[1;32m   5172\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m   5173\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_COLUMN\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   5174\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcol\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(col)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[1;32m   5175\u001b[0m     )\n\u001b[0;32m-> 5176\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwithColumn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolName\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcol\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jc\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparkSession)\n",
      "File \u001b[0;32m~/Documents/Coding/big-data-analytics/.venv/lib/python3.12/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/Documents/Coding/big-data-analytics/.venv/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `merchant` cannot be resolved. Did you mean one of the following? [`merchant_id`, `errors`, `amount`, `mcc`, `merchant_city`].;\n'Project [id#17, date#18, client_id#19, card_id#20, amount#56, use_chip#22, merchant_id#23, merchant_city#24, merchant_state#25, zip#26, mcc#27, errors#28, transaction_date#70, txn_date#474, txn_hour#489, weekday_short#505, is_weekend#522, upper(trim('merchant, None)) AS merchant_std#540]\n+- Project [id#17, date#18, client_id#19, card_id#20, amount#56, use_chip#22, merchant_id#23, merchant_city#24, merchant_state#25, zip#26, mcc#27, errors#28, transaction_date#70, txn_date#474, txn_hour#489, weekday_short#505, CASE WHEN dayofweek(cast(transaction_date#70 as date)) IN (1,7) THEN true ELSE false END AS is_weekend#522]\n   +- Project [id#17, date#18, client_id#19, card_id#20, amount#56, use_chip#22, merchant_id#23, merchant_city#24, merchant_state#25, zip#26, mcc#27, errors#28, transaction_date#70, txn_date#474, txn_hour#489, date_format(transaction_date#70, E, Some(Europe/Berlin)) AS weekday_short#505]\n      +- Project [id#17, date#18, client_id#19, card_id#20, amount#56, use_chip#22, merchant_id#23, merchant_city#24, merchant_state#25, zip#26, mcc#27, errors#28, transaction_date#70, txn_date#474, hour(transaction_date#70, Some(Europe/Berlin)) AS txn_hour#489]\n         +- Project [id#17, date#18, client_id#19, card_id#20, amount#56, use_chip#22, merchant_id#23, merchant_city#24, merchant_state#25, zip#26, mcc#27, errors#28, transaction_date#70, to_date(transaction_date#70, None, Some(Europe/Berlin), false) AS txn_date#474]\n            +- Repartition 200, true\n               +- Project [id#17, date#18, client_id#19, card_id#20, amount#56, use_chip#22, merchant_id#23, merchant_city#24, merchant_state#25, zip#26, mcc#27, errors#28, transaction_date#41 AS transaction_date#70]\n                  +- Project [id#17, date#18, client_id#19, card_id#20, cast(amount#21 as double) AS amount#56, use_chip#22, merchant_id#23, merchant_city#24, merchant_state#25, zip#26, mcc#27, errors#28, transaction_date#41]\n                     +- Project [id#17, date#18, client_id#19, card_id#20, amount#21, use_chip#22, merchant_id#23, merchant_city#24, merchant_state#25, zip#26, mcc#27, errors#28, to_timestamp(date#18, None, TimestampType, Some(Europe/Berlin), false) AS transaction_date#41]\n                        +- Relation [id#17,date#18,client_id#19,card_id#20,amount#21,use_chip#22,merchant_id#23,merchant_city#24,merchant_state#25,zip#26,mcc#27,errors#28] csv\n"
     ]
    }
   ],
   "source": [
    "# âœ… COMPLETE SOLUTION: Derive features in Spark (big dataset)\n",
    "# Goal: Enrich the loaded DataFrame without collecting to the driver\n",
    "print(\"ðŸŽ¯ COMPLETE: Add useful derived columns at scale (Spark-only)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Solution 1: Create time features\n",
    "print(\"ðŸ“… Adding time-based features...\")\n",
    "spark_banking_df = spark_banking_df.withColumn(\"txn_date\", to_date(col(\"transaction_date\"))) \\\n",
    "                                   .withColumn(\"txn_hour\", hour(col(\"transaction_date\"))) \\\n",
    "                                   .withColumn(\"weekday_short\", date_format(col(\"transaction_date\"), \"E\")) \\\n",
    "                                   .withColumn(\"is_weekend\", \n",
    "                                              when(dayofweek(col(\"transaction_date\")).isin([1, 7]), True)\n",
    "                                              .otherwise(False))\n",
    "\n",
    "# Solution 2: Clean/standardize merchant values\n",
    "print(\"ðŸª Standardizing merchant names...\")\n",
    "\n",
    "# Create a merchant column by combining merchant_id and merchant_city\n",
    "spark_banking_df = spark_banking_df.withColumn(\n",
    "    \"merchant\",\n",
    "    concat_ws(\"_\", col(\"merchant_id\").cast(\"string\"), col(\"merchant_city\"))\n",
    ")\n",
    "\n",
    "spark_banking_df = spark_banking_df.withColumn(\"merchant_std\", upper(trim(col(\"merchant\")))) \\\n",
    "                                   .withColumn(\"merchant_clean\", \n",
    "                                              when(col(\"merchant\").isNull() | (col(\"merchant\") == \"\"), \"UNKNOWN\")\n",
    "                                              .otherwise(col(\"merchant_std\")))\n",
    "\n",
    "# Solution 3: Amount quality flags\n",
    "print(\"ðŸ’° Adding amount quality flags...\")\n",
    "spark_banking_df = spark_banking_df.withColumn(\"is_amount_null\", col(\"amount\").isNull()) \\\n",
    "                                   .withColumn(\"is_amount_negative\", col(\"amount\") < 0) \\\n",
    "                                   .withColumn(\"amount_abs\", abs(col(\"amount\"))) \\\n",
    "                                   .withColumn(\"amount_category\",\n",
    "                                              when(col(\"amount\") < 10, \"Micro\")\n",
    "                                              .when(col(\"amount\") < 100, \"Small\")\n",
    "                                              .when(col(\"amount\") < 1000, \"Medium\")\n",
    "                                              .otherwise(\"Large\"))\n",
    "\n",
    "# Solution 4: Recreate/refresh temp view after enrichment\n",
    "spark_banking_df.createOrReplaceTempView(\"banking_transactions\")\n",
    "print(\"âœ… Temp view refreshed with new features\")\n",
    "\n",
    "# Preview enriched data\n",
    "print(\"\\nðŸ“Š Sample of enriched data:\")\n",
    "spark_banking_df.select(\"customer_id\", \"merchant_clean\", \"amount\", \"amount_category\", \n",
    "                        \"txn_date\", \"weekday_short\", \"is_weekend\", \"txn_hour\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9609b4d",
   "metadata": {},
   "source": [
    "## 2. Basic Data Exploration with Spark ðŸ¼âž¡ï¸ðŸ”¥\n",
    "**Goal:** Explore the 1GB+ dataset with Spark (no pandas copies)\n",
    "\n",
    "### ðŸŽ“ Live Coding Exercise:\n",
    "- **Instructor:** Demonstrates Spark actions and SQL\n",
    "- **Students:** Build aggregations and quality checks at scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f757ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ§‘â€ðŸ« INSTRUCTOR: Basic Spark exploration (precoded)\n",
    "def explore_banking_data_spark(df):\n",
    "    \"\"\"\n",
    "    Scalable data exploration using Spark\n",
    "    - Schema, counts, ranges, basic distributions\n",
    "    - No driver-side collect() on large datasets\n",
    "    \"\"\"\n",
    "    from pyspark.sql import functions as F\n",
    "    \n",
    "    print(\"ðŸ“Š BANKING DATA OVERVIEW (Spark)\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    print(f\"Total rows: {df.count():,}\")\n",
    "    df.printSchema()\n",
    "    \n",
    "    # Columns we expect (best effort)\n",
    "    available_cols = set([c.lower() for c in df.columns])\n",
    "    \n",
    "    if \"transaction_date\" in available_cols:\n",
    "        print(\"\\nðŸ“… Date range:\")\n",
    "        df.select(F.min(\"transaction_date\").alias(\"min_date\"), F.max(\"transaction_date\").alias(\"max_date\")).show()\n",
    "        \n",
    "        print(\"\\nðŸ“† Transactions by weekday:\")\n",
    "        df.withColumn(\"weekday\", F.date_format(F.col(\"transaction_date\"), \"E\")).groupBy(\"weekday\").count().orderBy(\"weekday\").show()\n",
    "    \n",
    "    if \"customer_id\" in available_cols:\n",
    "        print(\"\\nðŸ‘¥ Unique customers:\")\n",
    "        df.select(F.countDistinct(\"customer_id\").alias(\"unique_customers\")).show()\n",
    "    \n",
    "    if \"amount\" in available_cols:\n",
    "        print(\"\\nðŸ’° Amount stats:\")\n",
    "        df.select(\n",
    "            F.count(\"amount\").alias(\"n\"),\n",
    "            F.mean(\"amount\").alias(\"avg\"),\n",
    "            F.expr(\"percentile_approx(amount, array(0.25,0.5,0.75), 10000)\").alias(\"quantiles\"),\n",
    "            F.min(\"amount\").alias(\"min\"),\n",
    "            F.max(\"amount\").alias(\"max\")\n",
    "        ).show(truncate=False)\n",
    "    \n",
    "    if \"merchant\" in available_cols:\n",
    "        print(\"\\nðŸª Top merchants:\")\n",
    "        df.groupBy(\"merchant\").count().orderBy(F.desc(\"count\")).show(10, truncate=False)\n",
    "\n",
    "# Run the exploration\n",
    "explore_banking_data_spark(spark_banking_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38f42ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… COMPLETE SOLUTION: Custom data queries (Spark)\n",
    "print(\"ðŸŽ¯ COMPLETE: Find interesting patterns at scale!\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Solution 1: Find customers with highest spending (Spark)\n",
    "print(\"ðŸ’¸ TOP SPENDERS:\")\n",
    "top_spenders = spark_banking_df.groupBy('customer_id') \\\n",
    "                              .agg(sum('amount').alias('total_spent'),\n",
    "                                   count('*').alias('transaction_count'),\n",
    "                                   avg('amount').alias('avg_transaction')) \\\n",
    "                              .orderBy(desc('total_spent'))\n",
    "top_spenders.show(10)\n",
    "\n",
    "print(\"\\nðŸ’³ SPENDING BY MERCHANT CATEGORY:\")\n",
    "# Solution 2: Create merchant categories and analyze spending\n",
    "categorized_spending = spark_banking_df.withColumn('category', \n",
    "    when(col('merchant_clean').isin('REWE', 'EDEKA', 'ALDI', 'LIDL'), 'Food')\n",
    "    .when(col('merchant_clean').isin('DEUTSCHE BAHN', 'BVG', 'UBER'), 'Transport')\n",
    "    .when(col('merchant_clean').isin('AMAZON.DE', 'MEDIAMARKT', 'H&M'), 'Shopping')\n",
    "    .when(col('merchant_clean').isin('SHELL', 'ARAL', 'ESSO'), 'Fuel')\n",
    "    .when(col('merchant_clean').isin('SPARKASSE ATM', 'COMMERZBANK ATM'), 'Banking')\n",
    "    .when(col('merchant_clean').isin('MCDONALD\\'S', 'BURGER KING', 'STARBUCKS'), 'Dining')\n",
    "    .otherwise('Other')\n",
    ") \\\n",
    ".groupBy('category') \\\n",
    ".agg(sum('amount').alias('total_spending'),\n",
    "     count('*').alias('transaction_count'),\n",
    "     avg('amount').alias('avg_amount')) \\\n",
    ".orderBy(desc('total_spending'))\n",
    "\n",
    "categorized_spending.show()\n",
    "\n",
    "print(\"\\nðŸ“ˆ DAILY SPENDING TRENDS:\")\n",
    "# Solution 3: Show daily total spending\n",
    "daily_trends = spark_banking_df.groupBy(to_date('transaction_date').alias('date')) \\\n",
    "                               .agg(sum('amount').alias('total_daily_spending'),\n",
    "                                    count('*').alias('daily_transactions'),\n",
    "                                    avg('amount').alias('avg_daily_transaction')) \\\n",
    "                               .orderBy('date')\n",
    "daily_trends.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29205fbb",
   "metadata": {},
   "source": [
    "## 3. Spark Session Recap ðŸš€\n",
    "Spark is already initialized. Weâ€™ll keep this short and move to SQL analytics.\n",
    "\n",
    "- Session tuned for local development and large CSVs\n",
    "- Temp view `banking_transactions` is ready\n",
    "- Proceed to analytics at scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649b9020",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Optional) Spark utilities\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "print(\"â„¹ï¸ Spark utilities available. Session already created above.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1792d1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… COMPLETE SOLUTION: Spark DataFrame operations\n",
    "print(\"ðŸŽ¯ COMPLETE: Practice Spark DataFrame operations!\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Solution 1: Basic Spark DataFrame exploration\n",
    "print(\"ðŸ“Š BASIC SPARK OPERATIONS:\")\n",
    "print(f\"Total rows: {spark_banking_df.count():,}\")\n",
    "\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "spark_banking_df.show(5, truncate=False)\n",
    "\n",
    "print(\"\\nDataFrame description:\")\n",
    "spark_banking_df.describe().show()\n",
    "\n",
    "print(\"\\nðŸ“ˆ SPARK OPERATIONS:\")\n",
    "# Solution 2: Spark operations (updated without pandas reference)\n",
    "print(\"Counting unique customers:\")\n",
    "unique_customers = spark_banking_df.select(\"customer_id\").distinct().count()\n",
    "print(f\"Spark: {unique_customers:,} unique customers\")\n",
    "\n",
    "print(f\"\\nUnique merchants: {spark_banking_df.select('merchant_clean').distinct().count()}\")\n",
    "print(f\"Date range: {spark_banking_df.select(min('transaction_date'), max('transaction_date')).collect()[0]}\")\n",
    "\n",
    "# Solution 3: Temporary view already created above, but let's verify\n",
    "print(\"\\nðŸ—„ï¸  TEMPORARY VIEW STATUS:\")\n",
    "print(\"âœ… 'banking_transactions' view already created with enriched data\")\n",
    "\n",
    "# Test the view with additional queries\n",
    "print(\"âœ… Testing the view with SQL:\")\n",
    "spark.sql(\"SELECT COUNT(*) as total_transactions FROM banking_transactions\").show()\n",
    "\n",
    "print(\"\\nSample SQL query - Weekend vs Weekday analysis:\")\n",
    "spark.sql(\"\"\"\n",
    "SELECT \n",
    "    is_weekend,\n",
    "    COUNT(*) as transaction_count,\n",
    "    SUM(amount) as total_amount,\n",
    "    AVG(amount) as avg_amount\n",
    "FROM banking_transactions \n",
    "GROUP BY is_weekend\n",
    "ORDER BY is_weekend\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1092b80a",
   "metadata": {},
   "source": [
    "## 4. Advanced Spark SQL Analytics ðŸ”\n",
    "**Goal:** Complex banking analytics using SQL on big data\n",
    "\n",
    "### ðŸ¦ Real Banking Use Cases:\n",
    "- **Fraud Detection:** Unusual spending patterns\n",
    "- **Customer Segmentation:** Spending behavior analysis\n",
    "- **Risk Assessment:** Transaction pattern analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1292fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ§‘â€ðŸ« INSTRUCTOR: Advanced SQL analytics (precoded)\n",
    "def run_banking_analytics():\n",
    "    print(\"ðŸ” ADVANCED BANKING ANALYTICS (Spark SQL)\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # 1. Customer spending ranking with window functions\n",
    "    print(\"ðŸ‘‘ TOP CUSTOMERS BY SPENDING:\")\n",
    "    query1 = \"\"\"\n",
    "    SELECT \n",
    "        customer_id,\n",
    "        SUM(amount) as total_spent,\n",
    "        COUNT(*) as transaction_count,\n",
    "        AVG(amount) as avg_transaction,\n",
    "        RANK() OVER (ORDER BY SUM(amount) DESC) as spending_rank\n",
    "    FROM banking_transactions \n",
    "    GROUP BY customer_id \n",
    "    ORDER BY total_spent DESC \n",
    "    LIMIT 10\n",
    "    \"\"\"\n",
    "    spark.sql(query1).show(truncate=False)\n",
    "\n",
    "    # 2. Merchant performance analysis\n",
    "    print(\"\\nðŸª MERCHANT REVENUE ANALYSIS:\")\n",
    "    query2 = \"\"\"\n",
    "    SELECT \n",
    "        merchant,\n",
    "        COUNT(*) as transactions,\n",
    "        SUM(amount) as total_revenue,\n",
    "        AVG(amount) as avg_transaction,\n",
    "        STDDEV_POP(amount) as amount_volatility,\n",
    "        MIN(amount) as min_amount,\n",
    "        MAX(amount) as max_amount\n",
    "    FROM banking_transactions \n",
    "    GROUP BY merchant \n",
    "    HAVING COUNT(*) >= 100\n",
    "    ORDER BY total_revenue DESC\n",
    "    \"\"\"\n",
    "    spark.sql(query2).show(truncate=False)\n",
    "\n",
    "    # 3. Time-based patterns (fraud detection)\n",
    "    print(\"\\nâ° HOURLY TRANSACTION PATTERNS:\")\n",
    "    query3 = \"\"\"\n",
    "    SELECT \n",
    "        HOUR(transaction_date) as hour,\n",
    "        COUNT(*) as transactions,\n",
    "        SUM(amount) as total_amount,\n",
    "        AVG(amount) as avg_amount,\n",
    "        CASE \n",
    "            WHEN HOUR(transaction_date) BETWEEN 9 AND 17 THEN 'Business Hours'\n",
    "            WHEN HOUR(transaction_date) BETWEEN 18 AND 22 THEN 'Evening'\n",
    "            ELSE 'Off Hours'\n",
    "        END as time_category\n",
    "    FROM banking_transactions \n",
    "    GROUP BY HOUR(transaction_date)\n",
    "    ORDER BY hour\n",
    "    \"\"\"\n",
    "    spark.sql(query3).show()\n",
    "\n",
    "    print(\"âœ… Advanced analytics complete!\")\n",
    "\n",
    "# Run the analytics\n",
    "run_banking_analytics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d106fcbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… COMPLETE SOLUTION: Complex SQL queries for fraud detection\n",
    "print(\"ðŸŽ¯ COMPLETE: Build fraud detection queries!\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Solution 1: Fraud Detection - Unusual spending patterns\n",
    "print(\"ðŸš¨ POTENTIAL FRAUD DETECTION:\")\n",
    "print(\"Find customers with transactions > 3 standard deviations from their average\")\n",
    "\n",
    "fraud_query = \"\"\"\n",
    "WITH customer_stats AS (\n",
    "    SELECT \n",
    "        customer_id,\n",
    "        transaction_date,\n",
    "        amount,\n",
    "        AVG(amount) OVER (PARTITION BY customer_id) as avg_amount,\n",
    "        STDDEV_POP(amount) OVER (PARTITION BY customer_id) as stddev_amount\n",
    "    FROM banking_transactions\n",
    "),\n",
    "potential_fraud AS (\n",
    "    SELECT \n",
    "        customer_id,\n",
    "        transaction_date,\n",
    "        amount,\n",
    "        avg_amount,\n",
    "        stddev_amount,\n",
    "        ABS(amount - avg_amount) as deviation,\n",
    "        CASE \n",
    "            WHEN stddev_amount > 0 AND ABS(amount - avg_amount) > 3 * stddev_amount \n",
    "            THEN 'HIGH_RISK'\n",
    "            WHEN stddev_amount > 0 AND ABS(amount - avg_amount) > 2 * stddev_amount \n",
    "            THEN 'MEDIUM_RISK'\n",
    "            ELSE 'NORMAL'\n",
    "        END as risk_level\n",
    "    FROM customer_stats\n",
    ")\n",
    "SELECT \n",
    "    risk_level,\n",
    "    COUNT(*) as transaction_count,\n",
    "    AVG(amount) as avg_suspicious_amount,\n",
    "    MIN(amount) as min_amount,\n",
    "    MAX(amount) as max_amount\n",
    "FROM potential_fraud \n",
    "WHERE risk_level != 'NORMAL'\n",
    "GROUP BY risk_level\n",
    "ORDER BY transaction_count DESC\n",
    "\"\"\"\n",
    "\n",
    "print(\"Fraud detection results:\")\n",
    "spark.sql(fraud_query).show()\n",
    "\n",
    "print(\"\\nðŸ’³ CUSTOMER BEHAVIOR SEGMENTATION:\")\n",
    "# Solution 2: Segment customers by spending behavior\n",
    "segmentation_query = \"\"\"\n",
    "WITH customer_behavior AS (\n",
    "    SELECT \n",
    "        customer_id,\n",
    "        SUM(amount) as total_spending,\n",
    "        COUNT(*) as transaction_frequency,\n",
    "        AVG(amount) as avg_transaction_size,\n",
    "        STDDEV_POP(amount) as spending_volatility\n",
    "    FROM banking_transactions \n",
    "    GROUP BY customer_id\n",
    "),\n",
    "customer_segments AS (\n",
    "    SELECT \n",
    "        customer_id,\n",
    "        total_spending,\n",
    "        transaction_frequency,\n",
    "        avg_transaction_size,\n",
    "        spending_volatility,\n",
    "        CASE \n",
    "            WHEN total_spending > 10000 THEN 'High Spender'\n",
    "            WHEN total_spending > 2000 THEN 'Medium Spender'\n",
    "            ELSE 'Low Spender'\n",
    "        END as spending_segment,\n",
    "        CASE \n",
    "            WHEN transaction_frequency > 50 THEN 'Frequent'\n",
    "            WHEN transaction_frequency > 15 THEN 'Regular'\n",
    "            ELSE 'Occasional'\n",
    "        END as frequency_segment,\n",
    "        CASE \n",
    "            WHEN avg_transaction_size > 500 THEN 'Large Transactions'\n",
    "            WHEN avg_transaction_size > 100 THEN 'Medium Transactions'\n",
    "            ELSE 'Small Transactions'\n",
    "        END as size_segment\n",
    "    FROM customer_behavior\n",
    ")\n",
    "SELECT \n",
    "    spending_segment,\n",
    "    frequency_segment,\n",
    "    size_segment,\n",
    "    COUNT(*) as customer_count,\n",
    "    AVG(total_spending) as avg_total_spending,\n",
    "    AVG(transaction_frequency) as avg_frequency\n",
    "FROM customer_segments\n",
    "GROUP BY spending_segment, frequency_segment, size_segment\n",
    "ORDER BY customer_count DESC\n",
    "\"\"\"\n",
    "\n",
    "print(\"Customer segmentation results:\")\n",
    "spark.sql(segmentation_query).show(20, truncate=False)\n",
    "\n",
    "print(\"\\nðŸ“Š WEEKEND vs WEEKDAY SPENDING:\")\n",
    "# Solution 3: Compare spending patterns\n",
    "weekend_query = \"\"\"\n",
    "SELECT \n",
    "    is_weekend,\n",
    "    weekday_short,\n",
    "    COUNT(*) as transaction_count,\n",
    "    SUM(amount) as total_spending,\n",
    "    AVG(amount) as avg_transaction,\n",
    "    MIN(amount) as min_transaction,\n",
    "    MAX(amount) as max_transaction,\n",
    "    PERCENTILE_APPROX(amount, 0.5) as median_transaction\n",
    "FROM banking_transactions \n",
    "GROUP BY is_weekend, weekday_short\n",
    "ORDER BY is_weekend, weekday_short\n",
    "\"\"\"\n",
    "\n",
    "print(\"Weekend vs Weekday spending patterns:\")\n",
    "spark.sql(weekend_query).show()\n",
    "\n",
    "# Additional analysis: Hourly patterns\n",
    "print(\"\\nâ° HOURLY SPENDING PATTERNS:\")\n",
    "hourly_query = \"\"\"\n",
    "SELECT \n",
    "    txn_hour,\n",
    "    COUNT(*) as transactions,\n",
    "    SUM(amount) as total_amount,\n",
    "    AVG(amount) as avg_amount,\n",
    "    CASE \n",
    "        WHEN txn_hour BETWEEN 9 AND 17 THEN 'Business Hours'\n",
    "        WHEN txn_hour BETWEEN 18 AND 22 THEN 'Evening'\n",
    "        WHEN txn_hour BETWEEN 23 AND 6 THEN 'Night'\n",
    "        ELSE 'Early Morning'\n",
    "    END as time_category\n",
    "FROM banking_transactions \n",
    "GROUP BY txn_hour\n",
    "ORDER BY txn_hour\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(hourly_query).show(24)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6c301f",
   "metadata": {},
   "source": [
    "## 5. GCP Databricks Setup â˜ï¸\n",
    "**Goal:** Deploy our banking analysis to Google Cloud Platform\n",
    "\n",
    "### ðŸŒŸ Why GCP + Databricks?\n",
    "- **Scalability:** Handle millions of banking transactions\n",
    "- **Security:** Enterprise-grade data protection\n",
    "- **Compliance:** Meet banking regulatory requirements\n",
    "- **Integration:** Connect to various data sources\n",
    "\n",
    "### ðŸ“‹ Pre-requisites:\n",
    "- GCP Account with billing enabled\n",
    "- Databricks workspace access\n",
    "- Service account with proper permissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3b21eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ§‘â€ðŸ« INSTRUCTOR: GCP Databricks configuration (precoded)\n",
    "import os\n",
    "import json\n",
    "from google.cloud import storage\n",
    "\n",
    "def setup_gcp_connection():\n",
    "    \"\"\"\n",
    "    Configure GCP connection for banking data upload\n",
    "    \n",
    "    This demonstrates:\n",
    "    - Service account authentication\n",
    "    - Cloud Storage bucket creation\n",
    "    - Data upload preparation\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"â˜ï¸  GCP DATABRICKS SETUP\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Configuration for GCP\n",
    "    gcp_config = {\n",
    "        \"project_id\": \"your-banking-project-id\",  # Change this\n",
    "        \"bucket_name\": \"banking-data-analytics\",   # Change this\n",
    "        \"dataset_location\": \"europe-west3\",       # Frankfurt region\n",
    "        \"service_account_path\": \"/path/to/service-account.json\"\n",
    "    }\n",
    "    \n",
    "    print(\"ðŸ“‹ GCP Configuration:\")\n",
    "    for key, value in gcp_config.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    \n",
    "    # Sample Databricks cluster configuration\n",
    "    databricks_config = {\n",
    "        \"cluster_name\": \"banking-analytics-cluster\",\n",
    "        \"spark_version\": \"11.3.x-scala2.12\",\n",
    "        \"node_type_id\": \"n1-standard-4\",\n",
    "        \"num_workers\": 2,\n",
    "        \"autotermination_minutes\": 60,\n",
    "        \"spark_conf\": {\n",
    "            \"spark.sql.adaptive.enabled\": \"true\",\n",
    "            \"spark.sql.adaptive.coalescePartitions.enabled\": \"true\",\n",
    "            \"spark.serializer\": \"org.apache.spark.serializer.KryoSerializer\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    print(\"\\nðŸš€ Databricks Cluster Config:\")\n",
    "    print(json.dumps(databricks_config, indent=2))\n",
    "    \n",
    "    return gcp_config, databricks_config\n",
    "\n",
    "def prepare_data_for_upload(df, output_path=\"banking_data.parquet\"):\n",
    "    \"\"\"\n",
    "    Prepare banking data for GCP upload\n",
    "    \n",
    "    Best practices:\n",
    "    - Use Parquet format for efficiency\n",
    "    - Partition by date for query performance\n",
    "    - Add metadata for governance\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\nðŸ“¦ PREPARING DATA FOR UPLOAD\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    # Convert to Spark DataFrame if pandas\n",
    "    if hasattr(df, 'to_pandas'):\n",
    "        print(\"âœ… Already Spark DataFrame\")\n",
    "        spark_df = df\n",
    "    else:\n",
    "        print(\"ðŸ”„ Converting pandas to Spark\")\n",
    "        spark_df = spark.createDataFrame(df)\n",
    "    \n",
    "    # Add metadata columns\n",
    "    spark_df_enhanced = spark_df.withColumn(\"upload_date\", current_date()) \\\n",
    "                               .withColumn(\"data_source\", lit(\"synthetic_banking\")) \\\n",
    "                               .withColumn(\"data_version\", lit(\"v1.0\"))\n",
    "    \n",
    "    # Write to local parquet (simulate GCS upload)\n",
    "    print(f\"ðŸ’¾ Writing to {output_path}...\")\n",
    "    spark_df_enhanced.coalesce(1).write.mode(\"overwrite\").parquet(output_path)\n",
    "    \n",
    "    print(f\"âœ… Data prepared: {spark_df_enhanced.count():,} records\")\n",
    "    return spark_df_enhanced\n",
    "\n",
    "# Run the setup\n",
    "gcp_config, databricks_config = setup_gcp_connection()\n",
    "enhanced_data = prepare_data_for_upload(spark_banking_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f6ea5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… COMPLETE SOLUTION: Customize GCP deployment\n",
    "print(\"ðŸŽ¯ COMPLETE: Customize the deployment configuration!\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Solution 1: Update configuration for your environment\n",
    "print(\"âš™ï¸ CUSTOMIZE YOUR CONFIGURATION:\")\n",
    "my_gcp_config = {\n",
    "    \"project_id\": \"banking-analytics-demo-2025\",  # Example project ID\n",
    "    \"bucket_name\": \"banking-data-workshop-eu\",   # Example bucket name\n",
    "    \"region\": \"europe-west3\",                    # Frankfurt region for GDPR compliance\n",
    "    \"dataset_location\": \"EU\",                    # European Union for data residency\n",
    "    \"service_account_email\": \"banking-analytics@banking-analytics-demo-2025.iam.gserviceaccount.com\",\n",
    "    \"vpc_network\": \"banking-vpc\",                # Custom VPC for security\n",
    "    \"subnet\": \"banking-subnet-eu-west3\"          # Specific subnet\n",
    "}\n",
    "\n",
    "print(\"ðŸ“ Your GCP Config:\")\n",
    "import json\n",
    "print(json.dumps(my_gcp_config, indent=2))\n",
    "\n",
    "# Solution 2: Create a deployment checklist\n",
    "print(\"\\nâœ… DEPLOYMENT CHECKLIST:\")\n",
    "deployment_checklist = [\n",
    "    \"GCP project created and billing enabled\",\n",
    "    \"Service account created with necessary permissions\",\n",
    "    \"Cloud Storage bucket created in EU region\",\n",
    "    \"Databricks workspace provisioned\",\n",
    "    \"VPC and firewall rules configured\",\n",
    "    \"BigQuery dataset created for analytics\",\n",
    "    \"Cloud IAM roles assigned properly\",\n",
    "    \"Data governance policies defined\",\n",
    "    \"Monitoring and alerting set up\",\n",
    "    \"Backup and disaster recovery plan ready\",\n",
    "    \"Security scan completed\",\n",
    "    \"Compliance review (GDPR, PCI DSS) passed\"\n",
    "]\n",
    "\n",
    "for i, item in enumerate(deployment_checklist, 1):\n",
    "    print(f\"{i:2d}. â˜ {item}\")\n",
    "\n",
    "# Solution 3: Estimate costs for your banking analytics\n",
    "print(\"\\nðŸ’° COST ESTIMATION:\")\n",
    "# Calculate estimated costs based on realistic banking scenarios\n",
    "\n",
    "# Data assumptions\n",
    "data_size_gb = 50  # 50GB of transaction data\n",
    "queries_per_day = 100  # 100 analytical queries daily\n",
    "cluster_hours_per_day = 8  # Cluster running 8 hours per day\n",
    "storage_retention_months = 12  # 1 year data retention\n",
    "\n",
    "# Cost estimates (EUR, approximate 2025 pricing)\n",
    "costs = {\n",
    "    \"compute_daily\": cluster_hours_per_day * 2.5,  # â‚¬2.50/hour for cluster\n",
    "    \"storage_monthly\": data_size_gb * 0.02,        # â‚¬0.02/GB/month\n",
    "    \"bigquery_monthly\": queries_per_day * 30 * 0.005,  # â‚¬0.005 per query\n",
    "    \"network_monthly\": 10,  # â‚¬10/month for network egress\n",
    "    \"monitoring_monthly\": 25  # â‚¬25/month for monitoring\n",
    "}\n",
    "\n",
    "monthly_compute = costs[\"compute_daily\"] * 30\n",
    "monthly_total = (monthly_compute + costs[\"storage_monthly\"] + \n",
    "                costs[\"bigquery_monthly\"] + costs[\"network_monthly\"] + \n",
    "                costs[\"monitoring_monthly\"])\n",
    "\n",
    "print(f\"ðŸ“Š MONTHLY COST BREAKDOWN:\")\n",
    "print(f\"  Compute (Databricks): â‚¬{monthly_compute:,.2f}\")\n",
    "print(f\"  Storage (Cloud Storage): â‚¬{costs['storage_monthly']:,.2f}\")\n",
    "print(f\"  Analytics (BigQuery): â‚¬{costs['bigquery_monthly']:,.2f}\")\n",
    "print(f\"  Network: â‚¬{costs['network_monthly']:,.2f}\")\n",
    "print(f\"  Monitoring: â‚¬{costs['monitoring_monthly']:,.2f}\")\n",
    "print(f\"  TOTAL MONTHLY: â‚¬{monthly_total:,.2f}\")\n",
    "print(f\"  TOTAL YEARLY: â‚¬{monthly_total * 12:,.2f}\")\n",
    "\n",
    "print(\"\\nðŸ’¡ COST OPTIMIZATION TIPS:\")\n",
    "optimization_tips = [\n",
    "    \"Use preemptible VMs for non-critical workloads (60-70% savings)\",\n",
    "    \"Schedule cluster auto-shutdown during non-business hours\",\n",
    "    \"Implement data lifecycle policies (move old data to Coldline storage)\",\n",
    "    \"Use BigQuery slots for predictable query costs\",\n",
    "    \"Optimize Spark jobs to reduce compute time\",\n",
    "    \"Use committed use discounts for sustained workloads\"\n",
    "]\n",
    "\n",
    "for tip in optimization_tips:\n",
    "    print(f\"  â€¢ {tip}\")\n",
    "\n",
    "print(\"\\nðŸŽ“ NEXT STEPS:\")\n",
    "next_steps = [\n",
    "    \"Set up your GCP account with billing alerts\",\n",
    "    \"Create Databricks workspace with auto-scaling enabled\",\n",
    "    \"Configure service account with minimal required permissions\",\n",
    "    \"Upload sample data and test data pipeline\",\n",
    "    \"Test connection from Databricks to BigQuery\",\n",
    "    \"Set up monitoring dashboards and alerts\",\n",
    "    \"Create backup and restore procedures\",\n",
    "    \"Document the architecture for team knowledge sharing\"\n",
    "]\n",
    "\n",
    "for i, step in enumerate(next_steps, 1):\n",
    "    print(f\"{i}. {step}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6ae0d8",
   "metadata": {},
   "source": [
    "## 6. Web Scraping for Financial Data ðŸ•·ï¸\n",
    "**Goal:** Integrate external financial data sources\n",
    "\n",
    "### ðŸ’¡ Real-World Banking Use Cases:\n",
    "- **Exchange Rates:** Currency conversion for international transactions  \n",
    "- **Stock Prices:** Portfolio valuation and risk assessment\n",
    "- **Economic Indicators:** Market analysis and forecasting\n",
    "- **Regulatory Updates:** Compliance monitoring\n",
    "\n",
    "### âš–ï¸ Ethical Considerations:\n",
    "- Always check `robots.txt` and terms of service\n",
    "- Respect rate limits and server resources\n",
    "- Use APIs when available instead of scraping\n",
    "- Consider data privacy and compliance requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc24aa81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ§‘â€ðŸ« INSTRUCTOR: Web scraping setup (precoded)\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_exchange_rates():\n",
    "    \"\"\"\n",
    "    Simulate scraping EUR/USD exchange rates\n",
    "    \n",
    "    In production, use:\n",
    "    - Official APIs (ECB, Federal Reserve, etc.)\n",
    "    - Financial data providers (Alpha Vantage, Yahoo Finance API)\n",
    "    - Respect rate limits and terms of service\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"ðŸ’± SIMULATING EXCHANGE RATE SCRAPING\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Simulate exchange rate data (in production, scrape from real source)\n",
    "    print(\"âš ï¸  NOTE: This is simulated data for demo purposes\")\n",
    "    print(\"ðŸ”— Real sources: European Central Bank API, Yahoo Finance, etc.\")\n",
    "    \n",
    "    # Generate realistic EUR/USD rates for past 30 days\n",
    "    base_rate = 1.10\n",
    "    dates = pd.date_range(start=datetime.now() - timedelta(days=30), \n",
    "                         end=datetime.now(), freq='D')\n",
    "    \n",
    "    exchange_rates = []\n",
    "    for i, date in enumerate(dates):\n",
    "        # Simulate rate fluctuation\n",
    "        rate = base_rate + (i * 0.001) + (0.02 * (i % 7 - 3) / 7)\n",
    "        exchange_rates.append({\n",
    "            'date': date.strftime('%Y-%m-%d'),\n",
    "            'currency_pair': 'EUR/USD',\n",
    "            'rate': round(rate, 4),\n",
    "            'source': 'simulated_ecb'\n",
    "        })\n",
    "    \n",
    "    rates_df = pd.DataFrame(exchange_rates)\n",
    "    \n",
    "    print(f\"ðŸ“Š Retrieved {len(rates_df)} exchange rates\")\n",
    "    print(\"\\nðŸ“ˆ Sample rates:\")\n",
    "    print(rates_df.tail())\n",
    "    \n",
    "    return rates_df\n",
    "\n",
    "def scrape_economic_indicators():\n",
    "    \"\"\"\n",
    "    Simulate scraping economic indicators relevant to banking\n",
    "    \n",
    "    Real indicators to track:\n",
    "    - Interest rates (ECB, Federal Reserve)\n",
    "    - Inflation rates\n",
    "    - GDP growth\n",
    "    - Unemployment rates\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\nðŸ“Š SIMULATING ECONOMIC INDICATORS SCRAPING\")\n",
    "    print(\"=\" * 45)\n",
    "    \n",
    "    # Simulated economic data\n",
    "    indicators = [\n",
    "        {'indicator': 'ECB_Interest_Rate', 'value': 4.25, 'date': '2024-01-15'},\n",
    "        {'indicator': 'EUR_Inflation_Rate', 'value': 2.8, 'date': '2024-01-15'},\n",
    "        {'indicator': 'DE_Unemployment_Rate', 'value': 5.9, 'date': '2024-01-15'},\n",
    "        {'indicator': 'EUR_GDP_Growth', 'value': 1.2, 'date': '2024-01-15'},\n",
    "    ]\n",
    "    \n",
    "    indicators_df = pd.DataFrame(indicators)\n",
    "    \n",
    "    print(\"ðŸ›ï¸ Key Economic Indicators:\")\n",
    "    print(indicators_df)\n",
    "    \n",
    "    return indicators_df\n",
    "\n",
    "# Execute the scraping functions\n",
    "exchange_rates_df = scrape_exchange_rates()\n",
    "economic_indicators_df = scrape_economic_indicators()\n",
    "\n",
    "print(\"\\nâœ… External data sources ready for integration!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf21d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… COMPLETE SOLUTION: Build custom financial data scrapers\n",
    "print(\"ðŸŽ¯ COMPLETE: Create custom financial data scrapers!\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "import random\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Solution 1: Create a scraper for stock prices (simulated)\n",
    "def scrape_stock_prices(symbols=['DAX', 'BMW', 'SAP', 'ADIDAS', 'SIEMENS']):\n",
    "    \"\"\"\n",
    "    Simulate scraping German stock prices\n",
    "    \n",
    "    Complete implementation that would scrape stock prices\n",
    "    for major German companies relevant to banking portfolios\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"ðŸ“ˆ STOCK PRICE SCRAPER:\")\n",
    "    \n",
    "    stock_data = []\n",
    "    \n",
    "    for symbol in symbols:\n",
    "        # Simulate realistic stock data\n",
    "        base_price = {\n",
    "            'DAX': 15800, 'BMW': 95, 'SAP': 180, \n",
    "            'ADIDAS': 220, 'SIEMENS': 155\n",
    "        }.get(symbol, 100)\n",
    "        \n",
    "        # Add some realistic volatility\n",
    "        price_change = random.uniform(-5, 5)  # +/- 5%\n",
    "        current_price = base_price * (1 + price_change/100)\n",
    "        \n",
    "        volume = random.randint(100000, 2000000)\n",
    "        \n",
    "        stock_info = {\n",
    "            'symbol': symbol,\n",
    "            'price': round(current_price, 2),\n",
    "            'change': round(price_change, 2),\n",
    "            'change_pct': f\"{price_change:+.2f}%\",\n",
    "            'volume': volume,\n",
    "            'timestamp': datetime.now(),\n",
    "            'market': 'XETRA',  # German stock exchange\n",
    "            'currency': 'EUR'\n",
    "        }\n",
    "        \n",
    "        stock_data.append(stock_info)\n",
    "        \n",
    "        # Simulate API rate limiting\n",
    "        time.sleep(0.1)\n",
    "    \n",
    "    df = pd.DataFrame(stock_data)\n",
    "    print(f\"âœ… Scraped {len(df)} stock prices\")\n",
    "    print(df.to_string(index=False))\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Test the function\n",
    "stock_prices_df = scrape_stock_prices()\n",
    "\n",
    "# Solution 2: Implement rate limiting and error handling\n",
    "def safe_scraper(url, delay=1, retries=3, timeout=10):\n",
    "    \"\"\"\n",
    "    Create a robust scraper with proper error handling\n",
    "    \n",
    "    Complete implementation with:\n",
    "    - Rate limiting (time delays)\n",
    "    - Retry logic for failed requests\n",
    "    - User-agent rotation\n",
    "    - Timeout handling\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"ðŸ›¡ï¸ IMPLEMENTING SAFE SCRAPING:\")\n",
    "    \n",
    "    user_agents = [\n",
    "        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "        'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "    ]\n",
    "    \n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            print(f\"Attempt {attempt + 1}/{retries} for {url}\")\n",
    "            \n",
    "            # Random user agent rotation\n",
    "            headers = {\n",
    "                'User-Agent': random.choice(user_agents),\n",
    "                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
    "                'Accept-Language': 'en-US,en;q=0.5',\n",
    "                'Accept-Encoding': 'gzip, deflate',\n",
    "                'Connection': 'keep-alive',\n",
    "            }\n",
    "            \n",
    "            # Make request with timeout\n",
    "            response = requests.get(url, headers=headers, timeout=timeout)\n",
    "            response.raise_for_status()  # Raise exception for bad status codes\n",
    "            \n",
    "            print(f\"âœ… Success: {response.status_code}\")\n",
    "            return response\n",
    "            \n",
    "        except requests.exceptions.Timeout:\n",
    "            print(f\"â° Timeout on attempt {attempt + 1}\")\n",
    "        except requests.exceptions.ConnectionError:\n",
    "            print(f\"ðŸ”Œ Connection error on attempt {attempt + 1}\")\n",
    "        except requests.exceptions.HTTPError as e:\n",
    "            print(f\"ðŸš« HTTP error on attempt {attempt + 1}: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Unexpected error on attempt {attempt + 1}: {e}\")\n",
    "        \n",
    "        if attempt < retries - 1:\n",
    "            wait_time = delay * (2 ** attempt)  # Exponential backoff\n",
    "            print(f\"â³ Waiting {wait_time} seconds before retry...\")\n",
    "            time.sleep(wait_time)\n",
    "    \n",
    "    print(f\"ðŸ’¥ Failed to scrape {url} after {retries} attempts\")\n",
    "    return None\n",
    "\n",
    "# Solution 3: Create a data validation function\n",
    "def validate_financial_data(df, data_type='exchange_rates'):\n",
    "    \"\"\"\n",
    "    Validate scraped financial data\n",
    "    \n",
    "    Complete implementation checking for:\n",
    "    - Missing values\n",
    "    - Unrealistic values (e.g., negative exchange rates)\n",
    "    - Date format consistency\n",
    "    - Duplicate entries\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"âœ… VALIDATING {data_type.upper()} DATA:\")\n",
    "    \n",
    "    validation_results = {\n",
    "        'total_records': len(df),\n",
    "        'issues_found': [],\n",
    "        'is_valid': True\n",
    "    }\n",
    "    \n",
    "    if data_type == 'exchange_rates':\n",
    "        # Check for missing values\n",
    "        if df['rate'].isnull().any():\n",
    "            missing_count = df['rate'].isnull().sum()\n",
    "            validation_results['issues_found'].append(f\"Missing rates: {missing_count}\")\n",
    "            validation_results['is_valid'] = False\n",
    "        \n",
    "        # Check for unrealistic values\n",
    "        if (df['rate'] <= 0).any():\n",
    "            negative_count = (df['rate'] <= 0).sum()\n",
    "            validation_results['issues_found'].append(f\"Non-positive rates: {negative_count}\")\n",
    "            validation_results['is_valid'] = False\n",
    "        \n",
    "        # Check for extreme values (rates should be reasonable)\n",
    "        if (df['rate'] > 10).any() or (df['rate'] < 0.1).any():\n",
    "            extreme_count = ((df['rate'] > 10) | (df['rate'] < 0.1)).sum()\n",
    "            validation_results['issues_found'].append(f\"Extreme rates: {extreme_count}\")\n",
    "    \n",
    "    elif data_type == 'stock_prices':\n",
    "        # Check for missing prices\n",
    "        if df['price'].isnull().any():\n",
    "            missing_count = df['price'].isnull().sum()\n",
    "            validation_results['issues_found'].append(f\"Missing prices: {missing_count}\")\n",
    "            validation_results['is_valid'] = False\n",
    "        \n",
    "        # Check for negative prices\n",
    "        if (df['price'] < 0).any():\n",
    "            negative_count = (df['price'] < 0).sum()\n",
    "            validation_results['issues_found'].append(f\"Negative prices: {negative_count}\")\n",
    "            validation_results['is_valid'] = False\n",
    "    \n",
    "    # Check for duplicates\n",
    "    if df.duplicated().any():\n",
    "        duplicate_count = df.duplicated().sum()\n",
    "        validation_results['issues_found'].append(f\"Duplicate records: {duplicate_count}\")\n",
    "    \n",
    "    # Check date consistency (if date column exists)\n",
    "    if 'date' in df.columns:\n",
    "        try:\n",
    "            pd.to_datetime(df['date'])\n",
    "        except:\n",
    "            validation_results['issues_found'].append(\"Invalid date formats found\")\n",
    "            validation_results['is_valid'] = False\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"Total records: {validation_results['total_records']}\")\n",
    "    if validation_results['issues_found']:\n",
    "        print(\"Issues found:\")\n",
    "        for issue in validation_results['issues_found']:\n",
    "            print(f\"  âš ï¸  {issue}\")\n",
    "    else:\n",
    "        print(\"âœ… No issues found\")\n",
    "    \n",
    "    print(f\"Data validation: {'PASSED' if validation_results['is_valid'] else 'FAILED'}\")\n",
    "    \n",
    "    return validation_results['is_valid']\n",
    "\n",
    "# Test validation\n",
    "validate_financial_data(stock_prices_df, 'stock_prices')\n",
    "validate_financial_data(exchange_rates_df, 'exchange_rates')\n",
    "\n",
    "print(\"\\nðŸŽ“ BONUS CHALLENGE ANSWERS:\")\n",
    "print(\"Research and list 3 official financial APIs that would be better than scraping:\")\n",
    "financial_apis = [\n",
    "    \"1. European Central Bank (ECB) API - Official exchange rates and monetary policy data\",\n",
    "    \"2. Alpha Vantage API - Real-time and historical stock market data with free tier\",\n",
    "    \"3. Yahoo Finance API - Comprehensive financial data including stocks, forex, and commodities\"\n",
    "]\n",
    "\n",
    "for api in financial_apis:\n",
    "    print(api)\n",
    "\n",
    "print(\"\\nðŸ”— Additional recommended APIs:\")\n",
    "additional_apis = [\n",
    "    \"â€¢ Quandl (now part of Nasdaq) - Economic and financial data\",\n",
    "    \"â€¢ IEX Cloud - US stock market data with generous free tier\",\n",
    "    \"â€¢ Frankfurter API - European Central Bank exchange rates (free)\",\n",
    "    \"â€¢ Federal Reserve Economic Data (FRED) - US economic indicators\",\n",
    "    \"â€¢ Financial Modeling Prep - Financial statements and market data\"\n",
    "]\n",
    "\n",
    "for api in additional_apis:\n",
    "    print(api)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc056ce1",
   "metadata": {},
   "source": [
    "## 7. Multi-Source Data Integration ðŸ”—\n",
    "**Goal:** Combine banking transactions with external financial data\n",
    "\n",
    "### ðŸ¦ Enterprise Banking Reality:\n",
    "- **Internal Data:** Transactions, customer profiles, account balances\n",
    "- **External Data:** Market data, economic indicators, regulatory feeds  \n",
    "- **Real-time Streams:** Payment networks, fraud detection systems\n",
    "- **Historical Archives:** Years of transaction history for analysis\n",
    "\n",
    "### ðŸŽ¯ Integration Challenges:\n",
    "- **Schema Variations:** Different data formats and structures\n",
    "- **Data Quality:** Missing values, duplicates, inconsistencies\n",
    "- **Time Synchronization:** Aligning data from different time zones\n",
    "- **Scale:** Processing billions of records efficiently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd481a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ§‘â€ðŸ« INSTRUCTOR: Multi-source data integration (precoded)\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "def integrate_financial_data():\n",
    "    \"\"\"\n",
    "    Demonstrate enterprise-level data integration\n",
    "    \n",
    "    This function shows:\n",
    "    - Converting external data to Spark DataFrames\n",
    "    - Schema alignment and data type conversions  \n",
    "    - Time-based joins for financial analysis\n",
    "    - Data quality checks and validation\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"ðŸ”— MULTI-SOURCE DATA INTEGRATION\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # 1. Convert external data to Spark DataFrames\n",
    "    print(\"ðŸ“Š Converting external data to Spark...\")\n",
    "    \n",
    "    # Exchange rates to Spark DataFrame\n",
    "    exchange_rates_spark = spark.createDataFrame(exchange_rates_df) \\\n",
    "        .withColumn(\"rate_date\", to_date(col(\"date\"), \"yyyy-MM-dd\")) \\\n",
    "        .withColumn(\"rate\", col(\"rate\").cast(\"double\"))\n",
    "    \n",
    "    # Economic indicators to Spark DataFrame  \n",
    "    indicators_spark = spark.createDataFrame(economic_indicators_df) \\\n",
    "        .withColumn(\"indicator_date\", to_date(col(\"date\"), \"yyyy-MM-dd\")) \\\n",
    "        .withColumn(\"value\", col(\"value\").cast(\"double\"))\n",
    "    \n",
    "    # 2. Prepare banking data for joins\n",
    "    print(\"ðŸ¦ Preparing banking transactions...\")\n",
    "    \n",
    "    banking_with_date = spark_banking_df \\\n",
    "        .withColumn(\"transaction_date_only\", \n",
    "                   to_date(col(\"transaction_date\"))) \\\n",
    "        .withColumn(\"month_year\", \n",
    "                   date_format(col(\"transaction_date\"), \"yyyy-MM\"))\n",
    "    \n",
    "    # 3. Join banking data with exchange rates (for international analysis)\n",
    "    print(\"ðŸ’± Integrating exchange rate data...\")\n",
    "    \n",
    "    banking_with_rates = banking_with_date.join(\n",
    "        exchange_rates_spark.select(\"rate_date\", \"rate\", \"currency_pair\"),\n",
    "        banking_with_date.transaction_date_only == exchange_rates_spark.rate_date,\n",
    "        \"left\"\n",
    "    ).withColumn(\"amount_usd\", \n",
    "                col(\"amount\") * col(\"rate\")) \\\n",
    "     .drop(\"rate_date\")\n",
    "    \n",
    "    # 4. Add economic context\n",
    "    print(\"ðŸ“ˆ Adding economic indicators...\")\n",
    "    \n",
    "    # Get monthly economic data (simplified join)\n",
    "    monthly_indicators = indicators_spark \\\n",
    "        .withColumn(\"month_year\", date_format(col(\"indicator_date\"), \"yyyy-MM\")) \\\n",
    "        .groupBy(\"month_year\") \\\n",
    "        .agg(\n",
    "            avg(when(col(\"indicator\") == \"ECB_Interest_Rate\", col(\"value\"))).alias(\"interest_rate\"),\n",
    "            avg(when(col(\"indicator\") == \"EUR_Inflation_Rate\", col(\"value\"))).alias(\"inflation_rate\")\n",
    "        )\n",
    "    \n",
    "    # Final integrated dataset\n",
    "    integrated_banking_data = banking_with_rates.join(\n",
    "        monthly_indicators,\n",
    "        \"month_year\",\n",
    "        \"left\"\n",
    "    )\n",
    "    \n",
    "    # 5. Create summary view\n",
    "    print(\"\\nðŸ“‹ INTEGRATED DATA SUMMARY:\")\n",
    "    integrated_banking_data.select(\n",
    "        \"customer_id\", \"merchant\", \"amount\", \"amount_usd\", \n",
    "        \"rate\", \"interest_rate\", \"inflation_rate\", \"transaction_date\"\n",
    "    ).show(5, truncate=False)\n",
    "    \n",
    "    print(f\"\\nâœ… Integration complete: {integrated_banking_data.count():,} enriched transactions\")\n",
    "    \n",
    "    return integrated_banking_data\n",
    "\n",
    "# Execute integration\n",
    "integrated_data = integrate_financial_data()\n",
    "\n",
    "# Create temporary view for final analysis\n",
    "integrated_data.createOrReplaceTempView(\"integrated_banking_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24d8bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… COMPLETE SOLUTION: Comprehensive Banking Analytics Dashboard\n",
    "print(\"ðŸŽ¯ COMPLETE SOLUTION: Complete banking analytics dashboard!\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Solution 1: Economic Impact Analysis\n",
    "print(\"ðŸ“Š ECONOMIC IMPACT ANALYSIS:\")\n",
    "economic_analysis_query = \"\"\"\n",
    "SELECT \n",
    "    -- Economic period classification\n",
    "    CASE \n",
    "        WHEN interest_rate >= 2.0 THEN 'High Interest Period'\n",
    "        WHEN interest_rate BETWEEN 0.5 AND 2.0 THEN 'Medium Interest Period' \n",
    "        ELSE 'Low Interest Period'\n",
    "    END as interest_period,\n",
    "    \n",
    "    CASE\n",
    "        WHEN inflation_rate >= 3.0 THEN 'High Inflation Period'\n",
    "        WHEN inflation_rate BETWEEN 1.0 AND 3.0 THEN 'Normal Inflation Period'\n",
    "        ELSE 'Low Inflation Period' \n",
    "    END as inflation_period,\n",
    "    \n",
    "    -- Spending analysis\n",
    "    COUNT(*) as transaction_count,\n",
    "    ROUND(AVG(amount), 2) as avg_transaction_eur,\n",
    "    ROUND(AVG(amount_usd), 2) as avg_transaction_usd,\n",
    "    ROUND(SUM(amount), 2) as total_spending_eur,\n",
    "    ROUND(SUM(amount_usd), 2) as total_spending_usd,\n",
    "    \n",
    "    -- Currency impact\n",
    "    COUNT(CASE WHEN rate IS NOT NULL THEN 1 END) as international_transactions,\n",
    "    ROUND(AVG(rate), 4) as avg_exchange_rate,\n",
    "    \n",
    "    -- Most affected merchant categories\n",
    "    merchant_category,\n",
    "    COUNT(*) as category_transactions\n",
    "    \n",
    "FROM integrated_banking_data\n",
    "WHERE interest_rate IS NOT NULL AND inflation_rate IS NOT NULL\n",
    "GROUP BY \n",
    "    CASE \n",
    "        WHEN interest_rate >= 2.0 THEN 'High Interest Period'\n",
    "        WHEN interest_rate BETWEEN 0.5 AND 2.0 THEN 'Medium Interest Period' \n",
    "        ELSE 'Low Interest Period'\n",
    "    END,\n",
    "    CASE\n",
    "        WHEN inflation_rate >= 3.0 THEN 'High Inflation Period'\n",
    "        WHEN inflation_rate BETWEEN 1.0 AND 3.0 THEN 'Normal Inflation Period'\n",
    "        ELSE 'Low Inflation Period' \n",
    "    END,\n",
    "    merchant_category\n",
    "ORDER BY total_spending_eur DESC\n",
    "\"\"\"\n",
    "\n",
    "print(\"âœ… ECONOMIC IMPACT ANALYSIS RESULTS:\")\n",
    "economic_results = spark.sql(economic_analysis_query)\n",
    "economic_results.show(20)\n",
    "\n",
    "# Solution 2: Advanced Customer Segmentation\n",
    "print(\"\\nðŸ‘¥ ADVANCED CUSTOMER SEGMENTATION:\")\n",
    "segmentation_query = \"\"\"\n",
    "WITH customer_profiles AS (\n",
    "    SELECT \n",
    "        customer_id,\n",
    "        COUNT(*) as total_transactions,\n",
    "        ROUND(AVG(amount), 2) as avg_transaction,\n",
    "        ROUND(STDDEV(amount), 2) as spending_volatility,\n",
    "        ROUND(SUM(amount), 2) as total_spending,\n",
    "        \n",
    "        -- Currency usage patterns\n",
    "        COUNT(CASE WHEN rate IS NOT NULL THEN 1 END) as international_txns,\n",
    "        ROUND(COUNT(CASE WHEN rate IS NOT NULL THEN 1 END) * 100.0 / COUNT(*), 1) as intl_percentage,\n",
    "        \n",
    "        -- Economic period behavior\n",
    "        AVG(CASE WHEN interest_rate >= 2.0 THEN amount ELSE 0 END) as high_interest_spending,\n",
    "        AVG(CASE WHEN inflation_rate >= 3.0 THEN amount ELSE 0 END) as high_inflation_spending,\n",
    "        \n",
    "        -- Time patterns\n",
    "        COUNT(CASE WHEN hour BETWEEN 9 AND 17 THEN 1 END) as business_hours_txns,\n",
    "        COUNT(CASE WHEN hour NOT BETWEEN 9 AND 17 THEN 1 END) as off_hours_txns,\n",
    "        \n",
    "        -- Merchant diversity\n",
    "        COUNT(DISTINCT merchant_category) as merchant_categories,\n",
    "        \n",
    "        -- Risk indicators\n",
    "        COUNT(CASE WHEN amount > 1000 THEN 1 END) as high_value_txns\n",
    "        \n",
    "    FROM integrated_banking_data \n",
    "    GROUP BY customer_id\n",
    "),\n",
    "customer_segments AS (\n",
    "    SELECT *,\n",
    "        CASE \n",
    "            WHEN total_spending >= 5000 AND intl_percentage >= 20 THEN 'Premium International'\n",
    "            WHEN total_spending >= 3000 AND spending_volatility >= 200 THEN 'High-Volume Variable'\n",
    "            WHEN intl_percentage >= 30 THEN 'International Focused' \n",
    "            WHEN off_hours_txns > business_hours_txns THEN 'Off-Hours Active'\n",
    "            WHEN merchant_categories >= 5 THEN 'Diverse Spender'\n",
    "            WHEN total_spending <= 1000 THEN 'Conservative Spender'\n",
    "            ELSE 'Standard Customer'\n",
    "        END as customer_segment,\n",
    "        \n",
    "        CASE\n",
    "            WHEN high_value_txns >= 3 OR spending_volatility >= 500 THEN 'High Risk'\n",
    "            WHEN intl_percentage >= 50 OR off_hours_txns >= 10 THEN 'Medium Risk'\n",
    "            ELSE 'Low Risk'\n",
    "        END as risk_profile\n",
    "        \n",
    "    FROM customer_profiles\n",
    ")\n",
    "SELECT \n",
    "    customer_segment,\n",
    "    risk_profile,\n",
    "    COUNT(*) as customer_count,\n",
    "    ROUND(AVG(total_spending), 2) as avg_total_spending,\n",
    "    ROUND(AVG(avg_transaction), 2) as avg_transaction_size,\n",
    "    ROUND(AVG(intl_percentage), 1) as avg_intl_percentage,\n",
    "    ROUND(AVG(merchant_categories), 1) as avg_merchant_diversity\n",
    "FROM customer_segments\n",
    "GROUP BY customer_segment, risk_profile\n",
    "ORDER BY customer_count DESC\n",
    "\"\"\"\n",
    "\n",
    "print(\"âœ… ADVANCED CUSTOMER SEGMENTATION RESULTS:\")\n",
    "segmentation_results = spark.sql(segmentation_query)\n",
    "segmentation_results.show()\n",
    "\n",
    "# Solution 3: Predictive Risk Indicators\n",
    "print(\"\\nðŸš¨ RISK AND FRAUD INDICATORS:\")\n",
    "risk_analysis_query = \"\"\"\n",
    "WITH risk_indicators AS (\n",
    "    SELECT \n",
    "        customer_id,\n",
    "        transaction_id,\n",
    "        amount,\n",
    "        amount_usd,\n",
    "        merchant_category,\n",
    "        hour,\n",
    "        interest_rate,\n",
    "        inflation_rate,\n",
    "        rate as exchange_rate,\n",
    "        \n",
    "        -- Risk flags\n",
    "        CASE WHEN amount > 2000 THEN 1 ELSE 0 END as high_value_flag,\n",
    "        CASE WHEN hour BETWEEN 22 AND 6 THEN 1 ELSE 0 END as unusual_time_flag,\n",
    "        CASE WHEN rate IS NOT NULL AND ABS(amount_usd/amount - rate) > rate * 0.1 THEN 1 ELSE 0 END as currency_anomaly_flag,\n",
    "        \n",
    "        -- Economic volatility periods\n",
    "        CASE WHEN interest_rate > 3.0 OR inflation_rate > 4.0 THEN 1 ELSE 0 END as economic_volatility_flag,\n",
    "        \n",
    "        -- Customer historical context (using window functions)\n",
    "        AVG(amount) OVER (PARTITION BY customer_id) as customer_avg_amount,\n",
    "        STDDEV(amount) OVER (PARTITION BY customer_id) as customer_stddev_amount\n",
    "        \n",
    "    FROM integrated_banking_data\n",
    "    WHERE amount IS NOT NULL\n",
    "),\n",
    "risk_scores AS (\n",
    "    SELECT *,\n",
    "        -- Deviation from customer norm\n",
    "        CASE WHEN ABS(amount - customer_avg_amount) > 2 * customer_stddev_amount THEN 1 ELSE 0 END as amount_deviation_flag,\n",
    "        \n",
    "        -- Composite risk score\n",
    "        (high_value_flag + unusual_time_flag + currency_anomaly_flag + \n",
    "         economic_volatility_flag) as composite_risk_score\n",
    "         \n",
    "    FROM risk_indicators\n",
    ")\n",
    "SELECT \n",
    "    -- Risk level distribution\n",
    "    CASE \n",
    "        WHEN composite_risk_score >= 3 THEN 'Critical Risk'\n",
    "        WHEN composite_risk_score = 2 THEN 'High Risk' \n",
    "        WHEN composite_risk_score = 1 THEN 'Medium Risk'\n",
    "        ELSE 'Low Risk'\n",
    "    END as risk_level,\n",
    "    \n",
    "    COUNT(*) as transaction_count,\n",
    "    COUNT(DISTINCT customer_id) as affected_customers,\n",
    "    ROUND(AVG(amount), 2) as avg_amount,\n",
    "    ROUND(SUM(amount), 2) as total_amount,\n",
    "    \n",
    "    -- Risk breakdown\n",
    "    SUM(high_value_flag) as high_value_transactions,\n",
    "    SUM(unusual_time_flag) as unusual_time_transactions, \n",
    "    SUM(currency_anomaly_flag) as currency_anomaly_transactions,\n",
    "    SUM(economic_volatility_flag) as economic_volatility_transactions,\n",
    "    SUM(amount_deviation_flag) as customer_deviation_transactions\n",
    "    \n",
    "FROM risk_scores\n",
    "GROUP BY \n",
    "    CASE \n",
    "        WHEN composite_risk_score >= 3 THEN 'Critical Risk'\n",
    "        WHEN composite_risk_score = 2 THEN 'High Risk' \n",
    "        WHEN composite_risk_score = 1 THEN 'Medium Risk'\n",
    "        ELSE 'Low Risk'\n",
    "    END\n",
    "ORDER BY \n",
    "    CASE \n",
    "        WHEN risk_level = 'Critical Risk' THEN 1\n",
    "        WHEN risk_level = 'High Risk' THEN 2 \n",
    "        WHEN risk_level = 'Medium Risk' THEN 3\n",
    "        ELSE 4\n",
    "    END\n",
    "\"\"\"\n",
    "\n",
    "print(\"âœ… RISK AND FRAUD ANALYSIS RESULTS:\")\n",
    "risk_results = spark.sql(risk_analysis_query)\n",
    "risk_results.show()\n",
    "\n",
    "# Solution 4: Executive Summary Dashboard\n",
    "print(\"\\nðŸ“ˆ EXECUTIVE SUMMARY DASHBOARD:\")\n",
    "print(\"Creating comprehensive dashboard for bank executives...\")\n",
    "\n",
    "# Calculate dashboard metrics\n",
    "dashboard_query = \"\"\"\n",
    "SELECT \n",
    "    COUNT(*) as total_transactions,\n",
    "    COUNT(DISTINCT customer_id) as total_customers,\n",
    "    ROUND(SUM(amount), 2) as total_volume_eur,\n",
    "    ROUND(SUM(COALESCE(amount_usd, amount)), 2) as total_volume_usd,\n",
    "    COUNT(CASE WHEN rate IS NOT NULL THEN 1 END) as international_transactions,\n",
    "    ROUND(AVG(interest_rate), 2) as avg_interest_rate,\n",
    "    ROUND(AVG(inflation_rate), 2) as avg_inflation_rate\n",
    "FROM integrated_banking_data\n",
    "\"\"\"\n",
    "\n",
    "dashboard_base = spark.sql(dashboard_query).collect()[0]\n",
    "\n",
    "# High-risk customers count\n",
    "high_risk_query = \"\"\"\n",
    "WITH customer_risk AS (\n",
    "    SELECT \n",
    "        customer_id,\n",
    "        COUNT(*) as txn_count,\n",
    "        AVG(amount) as avg_amount,\n",
    "        STDDEV(amount) as stddev_amount,\n",
    "        COUNT(CASE WHEN hour BETWEEN 22 AND 6 THEN 1 END) as unusual_time_count,\n",
    "        COUNT(CASE WHEN amount > 2000 THEN 1 END) as high_value_count\n",
    "    FROM integrated_banking_data\n",
    "    GROUP BY customer_id\n",
    ")\n",
    "SELECT COUNT(*) as high_risk_customers\n",
    "FROM customer_risk \n",
    "WHERE unusual_time_count >= 2 OR high_value_count >= 2 OR stddev_amount > 500\n",
    "\"\"\"\n",
    "\n",
    "high_risk_count = spark.sql(high_risk_query).collect()[0]['high_risk_customers']\n",
    "\n",
    "# Economic exposure calculation\n",
    "economic_exposure_query = \"\"\"\n",
    "SELECT \n",
    "    ROUND(SUM(CASE WHEN interest_rate > 2.0 OR inflation_rate > 3.0 THEN amount ELSE 0 END), 2) as economic_exposure_eur\n",
    "FROM integrated_banking_data\n",
    "WHERE interest_rate IS NOT NULL AND inflation_rate IS NOT NULL\n",
    "\"\"\"\n",
    "\n",
    "economic_exposure = spark.sql(economic_exposure_query).collect()[0]['economic_exposure_eur'] or 0\n",
    "\n",
    "# Compile dashboard metrics\n",
    "dashboard_metrics = {\n",
    "    \"total_transactions\": dashboard_base['total_transactions'],\n",
    "    \"total_customers\": dashboard_base['total_customers'],\n",
    "    \"total_volume_eur\": dashboard_base['total_volume_eur'],\n",
    "    \"total_volume_usd\": dashboard_base['total_volume_usd'], \n",
    "    \"international_transactions\": dashboard_base['international_transactions'],\n",
    "    \"high_risk_customers\": high_risk_count,\n",
    "    \"economic_exposure_eur\": economic_exposure,\n",
    "    \"avg_interest_rate\": dashboard_base['avg_interest_rate'] or 0,\n",
    "    \"avg_inflation_rate\": dashboard_base['avg_inflation_rate'] or 0,\n",
    "}\n",
    "\n",
    "print(\"ðŸ¦ EXECUTIVE DASHBOARD SUMMARY:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"ðŸ“Š Portfolio Overview:\")\n",
    "print(f\"  â€¢ Total Transactions: {dashboard_metrics['total_transactions']:,}\")\n",
    "print(f\"  â€¢ Total Customers: {dashboard_metrics['total_customers']:,}\")\n",
    "print(f\"  â€¢ Total Volume (EUR): â‚¬{dashboard_metrics['total_volume_eur']:,.2f}\")\n",
    "print(f\"  â€¢ Total Volume (USD): ${dashboard_metrics['total_volume_usd']:,.2f}\")\n",
    "\n",
    "print(f\"\\nðŸŒ International Exposure:\")\n",
    "print(f\"  â€¢ International Transactions: {dashboard_metrics['international_transactions']:,}\")\n",
    "intl_percentage = (dashboard_metrics['international_transactions'] / dashboard_metrics['total_transactions']) * 100 if dashboard_metrics['total_transactions'] > 0 else 0\n",
    "print(f\"  â€¢ International Percentage: {intl_percentage:.1f}%\")\n",
    "\n",
    "print(f\"\\nðŸ“ˆ Economic Context:\")\n",
    "print(f\"  â€¢ Average Interest Rate: {dashboard_metrics['avg_interest_rate']:.2f}%\")\n",
    "print(f\"  â€¢ Average Inflation Rate: {dashboard_metrics['avg_inflation_rate']:.2f}%\")\n",
    "print(f\"  â€¢ Economic Risk Exposure: â‚¬{dashboard_metrics['economic_exposure_eur']:,.2f}\")\n",
    "\n",
    "print(f\"\\nðŸš¨ Risk Management:\")\n",
    "print(f\"  â€¢ High-Risk Customers: {dashboard_metrics['high_risk_customers']:,}\")\n",
    "risk_percentage = (dashboard_metrics['high_risk_customers'] / dashboard_metrics['total_customers']) * 100 if dashboard_metrics['total_customers'] > 0 else 0\n",
    "print(f\"  â€¢ Risk Customer Percentage: {risk_percentage:.1f}%\")\n",
    "\n",
    "print(f\"\\nâœ… Regulatory Compliance:\")\n",
    "print(f\"  â€¢ AML Alerts Generated: {dashboard_metrics['high_risk_customers']:,}\")\n",
    "print(f\"  â€¢ Data Quality Score: 98.5%\")  # Simulated\n",
    "print(f\"  â€¢ Reporting Completeness: 100%\")  # Simulated\n",
    "\n",
    "print(\"\\nðŸŽ‰ CONGRATULATIONS!\")\n",
    "print(\"You've completed a full big data banking analytics pipeline:\")\n",
    "print(\"âœ… Data Generation & Quality Assessment\")  \n",
    "print(\"âœ… Scalable Processing with Spark\")\n",
    "print(\"âœ… Advanced SQL Analytics\")\n",
    "print(\"âœ… Cloud Deployment Preparation\")\n",
    "print(\"âœ… External Data Integration\")\n",
    "print(\"âœ… Multi-source Analytics\")\n",
    "print(\"âœ… Executive Dashboard Creation\")\n",
    "print(\"âœ… Risk Management & Compliance\")\n",
    "\n",
    "print(\"\\nðŸš€ NEXT STEPS FOR PRODUCTION:\")\n",
    "print(\"1. Implement real-time streaming with Kafka\")\n",
    "print(\"2. Add machine learning for fraud detection\")  \n",
    "print(\"3. Create automated reporting pipelines\")\n",
    "print(\"4. Implement data governance and lineage\")\n",
    "print(\"5. Add regulatory compliance monitoring\")\n",
    "print(\"6. Deploy to cloud infrastructure (GCP/AWS/Azure)\")\n",
    "print(\"7. Add API endpoints for business applications\")\n",
    "print(\"8. Implement data lakehouse architecture\")\n",
    "\n",
    "print(\"\\nðŸ“š WORKSHOP SUMMARY:\")\n",
    "print(\"You have successfully:\")\n",
    "print(\"â€¢ Built a scalable big data pipeline using PySpark\")\n",
    "print(\"â€¢ Performed advanced analytics on >1GB banking dataset\")\n",
    "print(\"â€¢ Integrated multiple external data sources\")\n",
    "print(\"â€¢ Created enterprise-level risk management dashboards\")\n",
    "print(\"â€¢ Prepared for cloud deployment and production scaling\")\n",
    "print(\"â€¢ Demonstrated production-ready data engineering skills\")\n",
    "\n",
    "print(\"\\nðŸŽ“ You're now ready to work with enterprise big data systems!\")\n",
    "print(\"Thank you for participating in this intensive banking analytics workshop!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
