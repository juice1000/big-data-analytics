{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05e261dc",
   "metadata": {},
   "source": [
    "# Banking Data Analysis - Live Coding Workshop\n",
    "## Big Data Analytics im Banking | 13:00-15:40\n",
    "\n",
    "### üéØ **Workshop Agenda**\n",
    "- **13:00-13:45:** Einf√ºhrung in Datenanalyse + Banking Transaction Analysis\n",
    "- **13:55-14:40:** Spark Deep-Dive & GCP Setup\n",
    "- **14:50-15:40:** Datenbeschaffung und -integration\n",
    "\n",
    "### üõ† **Was wir heute lernen:**\n",
    "1. **Datenanalyseprozess** in der Praxis\n",
    "2. **Data Mining** f√ºr Banking-Patterns\n",
    "3. **Spark Setup** und SQL-Queries\n",
    "4. **GCP/Databricks** Configuration\n",
    "5. **Web Scraping** f√ºr Financial Data\n",
    "6. **Multi-Source Integration**\n",
    "\n",
    "### üìã **Live Coding Approach**\n",
    "- **Instructor demonstrates** ‚Üí **Students modify/extend**\n",
    "- **Short code blocks** with thorough comments\n",
    "- **Interactive exercises** at each step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b73afc",
   "metadata": {},
   "source": [
    "## 1. Load Large Banking Transactions (PySpark) üè¶\n",
    "**Goal:** Load a >1GB CSV efficiently using PySpark and prepare it for analysis\n",
    "\n",
    "Dataset: `transactions_data.csv` (set the path below)\n",
    "\n",
    "### üéì Live Coding Exercise:\n",
    "- **Instructor:** Sets up Spark and loads the dataset with an explicit schema or fast inference\n",
    "- **Students:** Add derived columns and validate data quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f66ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple PySpark setup for banking data\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Dataset path\n",
    "dataset_path = \"transactions_data.csv\"\n",
    "\n",
    "# Create basic Spark session\n",
    "spark = SparkSession.builder.appName(\"Banking Analysis\").getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "print(\"‚úÖ Spark ready!\")\n",
    "print(f\"Version: {spark.version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01ebe08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare banking data\n",
    "print(\"üì¶ Loading banking dataset...\")\n",
    "\n",
    "# Simple data loading\n",
    "df = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(dataset_path)\n",
    "\n",
    "print(\"üìã Schema:\")\n",
    "df.printSchema()\n",
    "\n",
    "# Basic column mapping\n",
    "df = df.withColumnRenamed(\"client_id\", \"customer_id\") \\\n",
    "       .withColumnRenamed(\"id\", \"transaction_id\") \\\n",
    "       .withColumn(\"transaction_date\", to_timestamp(col(\"date\")))\n",
    "\n",
    "print(f\"üìä Loaded {df.count():,} transactions\")\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64adc563",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add basic features\n",
    "print(\"üîß Adding basic features...\")\n",
    "\n",
    "# First, ensure amount is properly numeric - convert to amount_usd as double\n",
    "df = df.withColumn(\"amount_usd\", regexp_replace(col(\"amount\"), \"[$]\", \"\").cast(\"double\"))\n",
    "\n",
    "print(f\"‚úÖ Amount data types - USD: {dict(df.dtypes)['amount_usd']}\")\n",
    "\n",
    "# Add simple time features\n",
    "df = df.withColumn(\"hour\", hour(col(\"transaction_date\"))) \\\n",
    "       .withColumn(\"is_weekend\", dayofweek(col(\"transaction_date\")).isin([1, 7]))\n",
    "\n",
    "# Add merchant category (simplified)\n",
    "df = df.withColumn(\"merchant_category\",\n",
    "                   when(col(\"mcc\").isin(5411, 5441), \"Grocery\")\n",
    "                   .when(col(\"mcc\").isin(5812, 5813), \"Restaurant\") \n",
    "                   .when(col(\"mcc\").isin(5541, 5542), \"Gas Station\")\n",
    "                   .otherwise(\"Other\"))\n",
    "\n",
    "# Add zip_region column\n",
    "df = df.withColumn(\"zip_region\",\n",
    "                   when(col(\"zip\").substr(1,1).isin([\"0\", \"1\", \"2\"]), \"Northeast\")\n",
    "                   .when(col(\"zip\").substr(1,1).isin([\"3\", \"4\", \"5\"]), \"Southeast\") \n",
    "                   .when(col(\"zip\").substr(1,1).isin([\"6\", \"7\"]), \"Central\")\n",
    "                   .when(col(\"zip\").substr(1,1).isin([\"8\", \"9\"]), \"West\")\n",
    "                   .otherwise(\"Unknown\"))\n",
    "\n",
    "# Create temp view for SQL\n",
    "df.createOrReplaceTempView(\"transactions\")\n",
    "print(\"‚úÖ Features added and temp view created!\")\n",
    "\n",
    "# Show sample with numeric amounts\n",
    "df.select(\"customer_id\", \"amount_usd\", \"merchant_category\", \"hour\", \"is_weekend\", \"zip_region\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9609b4d",
   "metadata": {},
   "source": [
    "## 2. Basic Data Exploration with Spark üêº‚û°Ô∏èüî•\n",
    "**Goal:** Explore the 1GB+ dataset with Spark (no pandas copies)\n",
    "\n",
    "### üéì Live Coding Exercise:\n",
    "- **Instructor:** Demonstrates Spark actions and SQL\n",
    "- **Students:** Build aggregations and quality checks at scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f757ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üßë‚Äçüè´ INSTRUCTOR: Basic Spark exploration (LIVE CODING)\n",
    "# TODO: Live code the data exploration function\n",
    "print(\"üìä BANKING DATA OVERVIEW\")\n",
    "print(\"üéØ LIVE CODING: We'll build a comprehensive exploration function together\")\n",
    "\n",
    "# Students will learn:\n",
    "# - Basic DataFrame operations (count, printSchema)\n",
    "# - Date range analysis with min/max\n",
    "# - Groupby operations for weekday analysis\n",
    "# - Distinct counts for unique customers\n",
    "# - Statistical functions and percentiles\n",
    "# - Top N analysis with orderBy\n",
    "\n",
    "print(\"‚úÖ Ready for live coding session!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38f42ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üßë‚Äçüéì STUDENT EXERCISE: Basic data exploration\n",
    "print(\"üìä EXERCISE: Complete the basic dataset analysis\")\n",
    "\n",
    "# TODO: Students complete these basic operations\n",
    "print(\"‚úèÔ∏è YOUR TASKS:\")\n",
    "print(\"1. Show total transaction count\")\n",
    "print(\"2. Show unique customer count\") \n",
    "print(\"3. Create spending analysis by merchant category\")\n",
    "print(\"4. Order results by total spending\")\n",
    "\n",
    "print(\"\\nüéØ EXERCISE OBJECTIVES:\")\n",
    "print(\"‚Ä¢ Practice DataFrame operations\")\n",
    "print(\"‚Ä¢ Learn aggregation functions (sum, count)\")\n",
    "print(\"‚Ä¢ Use groupBy and orderBy\")\n",
    "print(\"‚Ä¢ Work with column aliases\")\n",
    "\n",
    "# Hint: Use df.count(), df.select().distinct().count()\n",
    "# Hint: Use df.groupBy().agg(sum(), count()).orderBy()\n",
    "\n",
    "print(\"‚úÖ Ready for your solution!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29205fbb",
   "metadata": {},
   "source": [
    "## 3. Spark Session Recap üöÄ\n",
    "Spark is already initialized. We‚Äôll keep this short and move to SQL analytics.\n",
    "\n",
    "- Session tuned for local development and large CSVs\n",
    "- Temp view `banking_transactions` is ready\n",
    "- Proceed to analytics at scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649b9020",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Optional) Spark utilities\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "print(\"‚ÑπÔ∏è Spark utilities available. Session already created above.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1792d1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Spark DataFrame operations\n",
    "print(\"üîß LIVE CODING: Basic Spark operations\")\n",
    "\n",
    "# Show basic dataset statistics\n",
    "print(\"üí∞ Amount statistics:\")\n",
    "df.select(\"amount_usd\").describe().show()\n",
    "\n",
    "print(\"‚úèÔ∏è EXERCISE: Weekend vs Weekday Analysis\")\n",
    "print(\"TODO: Write SQL query to compare weekend vs weekday spending\")\n",
    "print(\"Hint: Use is_weekend column, COUNT(*), SUM(amount_usd)\")\n",
    "print(\"Hint: GROUP BY is_weekend\")\n",
    "\n",
    "# Template:\n",
    "# spark.sql(\"\"\"\n",
    "# SELECT \n",
    "#     is_weekend,\n",
    "#     -- Add your aggregations here\n",
    "# FROM transactions \n",
    "# -- Add your GROUP BY here\n",
    "# \"\"\").show()\n",
    "\n",
    "print(\"‚úÖ Ready for your SQL solution!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1092b80a",
   "metadata": {},
   "source": [
    "## 4. Advanced Spark SQL Analytics üîç\n",
    "**Goal:** Complex banking analytics using SQL on big data\n",
    "\n",
    "### üè¶ Real Banking Use Cases:\n",
    "- **Fraud Detection:** Unusual spending patterns\n",
    "- **Customer Segmentation:** Spending behavior analysis\n",
    "- **Risk Assessment:** Transaction pattern analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1292fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üßë‚Äçüè´ INSTRUCTOR: Banking analytics (LIVE CODING)\n",
    "print(\"üîç LIVE CODING: Banking Analytics Session\")\n",
    "\n",
    "print(\"üéØ We'll build together:\")\n",
    "print(\"1. Top customers by spending\")\n",
    "print(\"2. Top merchants by transaction volume\")\n",
    "print(\"3. Revenue analysis by merchant\")\n",
    "\n",
    "print(\"üí° Students will learn:\")\n",
    "print(\"‚Ä¢ Complex SQL with aggregate functions\")\n",
    "print(\"‚Ä¢ Window functions and ranking\")\n",
    "print(\"‚Ä¢ Business KPI calculations\")\n",
    "print(\"‚Ä¢ ORDER BY with LIMIT for top-N queries\")\n",
    "\n",
    "# TODO: Live code the customer analysis\n",
    "print(\"\\nüëë COMING UP: Top customers analysis\")\n",
    "\n",
    "# TODO: Live code the merchant analysis  \n",
    "print(\"üè™ COMING UP: Merchant performance analysis\")\n",
    "\n",
    "print(\"‚úÖ Ready for live banking analytics session!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d106fcbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üßë‚Äçüéì STUDENT EXERCISE: Fraud Detection\n",
    "print(\"üö® EXERCISE: Build a Basic Fraud Detection System\")\n",
    "\n",
    "print(\"‚úèÔ∏è YOUR CHALLENGES:\")\n",
    "print(\"1. Find transactions above $200 (potential high-value fraud)\")\n",
    "print(\"2. ADVANCED: Find customers with unusual spending patterns\")\n",
    "\n",
    "print(\"\\nüéØ BASIC EXERCISE (Start here):\")\n",
    "print(\"Write SQL to find high-amount transactions\")\n",
    "print(\"‚Ä¢ SELECT customer_id, amount_usd, merchant_id\") \n",
    "print(\"‚Ä¢ WHERE amount_usd > 200\")\n",
    "print(\"‚Ä¢ ORDER BY amount_usd DESC\")\n",
    "print(\"‚Ä¢ LIMIT 5\")\n",
    "\n",
    "# TODO: Write your SQL query here\n",
    "basic_fraud_query = \"\"\"\n",
    "-- Write your high-amount transaction query here\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\nüéØ ADVANCED EXERCISE (Optional):\")\n",
    "print(\"Statistical fraud detection using window functions\")\n",
    "print(\"‚Ä¢ Calculate customer average and standard deviation\")  \n",
    "print(\"‚Ä¢ Find transactions > 3 standard deviations from average\")\n",
    "print(\"‚Ä¢ Classify as HIGH_RISK, MEDIUM_RISK, NORMAL\")\n",
    "\n",
    "# TODO: Advanced students can attempt window functions\n",
    "advanced_fraud_query = \"\"\"\n",
    "-- Advanced: Use window functions for statistical analysis\n",
    "-- WITH customer_stats AS (...)\n",
    "\"\"\"\n",
    "\n",
    "print(\"‚úÖ Ready to detect fraud!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6c301f",
   "metadata": {},
   "source": [
    "## 5. Simple Cloud Deployment üå•Ô∏è\n",
    "**Goal:** Basic overview of deploying to cloud\n",
    "\n",
    "### üåü Why Cloud?\n",
    "- **Scale:** Handle big datasets\n",
    "- **Storage:** Secure data storage  \n",
    "- **Compute:** More processing power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3b21eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üßë‚Äçüè´ INSTRUCTOR: Simple cloud overview (demo only)\n",
    "print(\"‚òÅÔ∏è CLOUD DEPLOYMENT BASICS\")\n",
    "print(\"Key concepts:\")\n",
    "print(\"1. Upload data to cloud storage\")\n",
    "print(\"2. Create compute cluster\") \n",
    "print(\"3. Run Spark jobs\")\n",
    "print(\"4. Store results\")\n",
    "\n",
    "print(\"\\n‚úÖ Cloud overview complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f6ea5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ COMPLETE SOLUTION: Customize GCP deployment\n",
    "print(\"üéØ COMPLETE: Customize the deployment configuration!\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Solution 1: Update configuration for your environment\n",
    "print(\"‚öôÔ∏è CUSTOMIZE YOUR CONFIGURATION:\")\n",
    "my_gcp_config = {\n",
    "    \"project_id\": \"banking-analytics-demo-2025\",  # Example project ID\n",
    "    \"bucket_name\": \"banking-data-workshop-eu\",   # Example bucket name\n",
    "    \"region\": \"europe-west3\",                    # Frankfurt region for GDPR compliance\n",
    "    \"dataset_location\": \"EU\",                    # European Union for data residency\n",
    "    \"service_account_email\": \"banking-analytics@banking-analytics-demo-2025.iam.gserviceaccount.com\",\n",
    "    \"vpc_network\": \"banking-vpc\",                # Custom VPC for security\n",
    "    \"subnet\": \"banking-subnet-eu-west3\"          # Specific subnet\n",
    "}\n",
    "\n",
    "print(\"üìù Your GCP Config:\")\n",
    "import json\n",
    "print(json.dumps(my_gcp_config, indent=2))\n",
    "\n",
    "# Solution 2: Create a deployment checklist\n",
    "print(\"\\n‚úÖ DEPLOYMENT CHECKLIST:\")\n",
    "deployment_checklist = [\n",
    "    \"GCP project created and billing enabled\",\n",
    "    \"Service account created with necessary permissions\",\n",
    "    \"Cloud Storage bucket created in EU region\",\n",
    "    \"Databricks workspace provisioned\"\n",
    "]\n",
    "print(\"\\n‚úÖ Cloud overview complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc056ce1",
   "metadata": {},
   "source": [
    "## 6. Simple Data Integration üîó\n",
    "**Goal:** Combine banking data with external sources\n",
    "\n",
    "### üí° Real Banking Examples:\n",
    "- **Exchange Rates:** Convert international transactions\n",
    "- **Interest Rates:** Economic impact on spending\n",
    "- **Merchant Data:** Enhanced merchant information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd481a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üßë‚Äçüè´ INSTRUCTOR: API Integration (LIVE CODING)\n",
    "print(\"üîó LIVE CODING: External Data Integration\")\n",
    "\n",
    "print(\"üéØ We'll demonstrate:\")\n",
    "print(\"1. Making API calls to financial services\")\n",
    "print(\"2. Handling JSON responses\")\n",
    "print(\"3. Error handling and fallbacks\")\n",
    "print(\"4. Integrating live data into Spark DataFrames\")\n",
    "\n",
    "# Students will learn:\n",
    "import requests\n",
    "\n",
    "print(\"üìä COMING UP: Live exchange rate API integration\")\n",
    "print(\"üí° Key concepts:\")\n",
    "print(\"‚Ä¢ REST API calls with requests library\")\n",
    "print(\"‚Ä¢ JSON data parsing\")\n",
    "print(\"‚Ä¢ Try/except error handling\")\n",
    "print(\"‚Ä¢ DataFrame transformations with live data\")\n",
    "\n",
    "# TODO: Live code the API integration\n",
    "api_url = \"https://api.exchangeratesapi.io/v1/latest?access_key=24da234d4ded987472b5ece3b4981c9b&format=1\"\n",
    "\n",
    "print(\"‚úÖ Ready for live API integration session!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24d8bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üßë‚Äçüéì FINAL EXERCISE: Integration Analysis\n",
    "print(\"üìä FINAL CHALLENGE: Live Exchange Rate Analysis\")\n",
    "\n",
    "print(\"üéØ CAPSTONE EXERCISE:\")\n",
    "print(\"Combine everything you've learned to analyze currency impact!\")\n",
    "\n",
    "print(\"\\n‚úèÔ∏è YOUR MISSION:\")\n",
    "print(\"1. Use the live exchange rate data we just integrated\")\n",
    "print(\"2. Compare USD vs EUR spending by merchant category\") \n",
    "print(\"3. Calculate currency impact percentages\")\n",
    "print(\"4. Generate business insights\")\n",
    "\n",
    "print(\"\\nüí° REQUIRED QUERIES:\")\n",
    "print(\"Query 1: Basic comparison\")\n",
    "print(\"‚Ä¢ SELECT merchant_category, SUM(amount_usd), SUM(amount_eur)\")\n",
    "print(\"‚Ä¢ Calculate totals and averages for both currencies\")\n",
    "print(\"‚Ä¢ GROUP BY merchant_category\")\n",
    "\n",
    "print(\"\\nQuery 2: Advanced analysis\")  \n",
    "print(\"‚Ä¢ Calculate (amount_usd - amount_eur) differences\")\n",
    "print(\"‚Ä¢ Compute percentage impact: (usd-eur)/eur * 100\")\n",
    "print(\"‚Ä¢ Order by currency impact\")\n",
    "\n",
    "print(\"\\nüéØ LEARNING OBJECTIVES:\")\n",
    "print(\"‚Ä¢ Integration of live external data\")\n",
    "print(\"‚Ä¢ Multi-currency financial analysis\") \n",
    "print(\"‚Ä¢ Business insight generation\")\n",
    "print(\"‚Ä¢ Advanced SQL calculations\")\n",
    "\n",
    "# TODO: Students write their analysis queries\n",
    "live_rate_analysis = \"\"\"\n",
    "-- Write your currency comparison query here\n",
    "-- Include: merchant_category, totals, averages, differences\n",
    "\"\"\"\n",
    "\n",
    "currency_impact_query = \"\"\"\n",
    "-- Write your advanced impact analysis here\n",
    "-- Calculate percentage impacts and ranking\n",
    "\"\"\"\n",
    "\n",
    "print(\"‚úÖ Ready for your final analysis challenge!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f38621",
   "metadata": {},
   "source": [
    "## üéØ Workshop Summary\n",
    "\n",
    "You've learned the essentials of big data analytics:\n",
    "\n",
    "1. **PySpark Basics:** Loading and processing data\n",
    "2. **SQL Analysis:** Simple aggregations and insights\n",
    "3. **Banking Analytics:** Basic fraud detection\n",
    "4. **Cloud Concepts:** Deployment overview"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
