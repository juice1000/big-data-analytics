{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05e261dc",
   "metadata": {},
   "source": [
    "# Banking Data Analysis - Live Coding Workshop\n",
    "## Big Data Analytics im Banking | 13:00-15:40\n",
    "\n",
    "### 🎯 **Workshop Agenda**\n",
    "- **13:00-13:45:** Einführung in Datenanalyse + Banking Transaction Analysis\n",
    "- **13:55-14:40:** Spark Deep-Dive & GCP Setup\n",
    "- **14:50-15:40:** Datenbeschaffung und -integration\n",
    "\n",
    "### 🛠 **Was wir heute lernen:**\n",
    "1. **Datenanalyseprozess** in der Praxis\n",
    "2. **Data Mining** für Banking-Patterns\n",
    "3. **Spark Setup** und SQL-Queries\n",
    "4. **GCP/Databricks** Configuration\n",
    "5. **Web Scraping** für Financial Data\n",
    "6. **Multi-Source Integration**\n",
    "\n",
    "### 📋 **Live Coding Approach**\n",
    "- **Instructor demonstrates** → **Students modify/extend**\n",
    "- **Short code blocks** with thorough comments\n",
    "- **Interactive exercises** at each step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b73afc",
   "metadata": {},
   "source": [
    "## 1. Load Large Banking Transactions (PySpark) 🏦\n",
    "**Goal:** Load a >1GB CSV efficiently using PySpark and prepare it for analysis\n",
    "\n",
    "Dataset: `transactions_data.csv` (set the path below)\n",
    "\n",
    "### 🎓 Live Coding Exercise:\n",
    "- **Instructor:** Sets up Spark and loads the dataset with an explicit schema or fast inference\n",
    "- **Students:** Add derived columns and validate data quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9f66ed9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/11 14:29:19 WARN Utils: Your hostname, Maclook-Bro.local resolves to a loopback address: 127.0.0.1; using 192.168.222.131 instead (on interface en0)\n",
      "25/08/11 14:29:19 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/08/11 14:29:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/08/11 14:29:20 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Spark ready!\n",
      "Version: 3.5.3\n"
     ]
    }
   ],
   "source": [
    "# Simple PySpark setup for banking data\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Dataset path\n",
    "dataset_path = \"../data/transactions_data.csv\"\n",
    "\n",
    "if 'spark' in locals():\n",
    "    print(\"Stopping existing Spark session...\")\n",
    "    spark.stop()  # Stop any existing Spark session\n",
    "\n",
    "\n",
    "# Create basic Spark session\n",
    "# We set the driver memory to 4GB for better performance\n",
    "# We use local[*] to utilize all available cores on our machine\n",
    "spark = SparkSession.builder.appName(\"Banking Analysis\").config(\"spark.driver.memory\", \"4g\").master(\"local[*]\").getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"WARN\") # Set log level to WARN to reduce noise\n",
    "\n",
    "print(\"✅ Spark ready!\")\n",
    "print(f\"Version: {spark.version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f01ebe08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📦 Loading banking dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:====================================================>     (9 + 1) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📋 Schema:\n",
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- date: timestamp (nullable = true)\n",
      " |-- client_id: integer (nullable = true)\n",
      " |-- card_id: integer (nullable = true)\n",
      " |-- amount: string (nullable = true)\n",
      " |-- use_chip: string (nullable = true)\n",
      " |-- merchant_id: integer (nullable = true)\n",
      " |-- merchant_city: string (nullable = true)\n",
      " |-- merchant_state: string (nullable = true)\n",
      " |-- zip: double (nullable = true)\n",
      " |-- mcc: integer (nullable = true)\n",
      " |-- errors: string (nullable = true)\n",
      "\n",
      "type: <class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Load and prepare banking data\n",
    "print(\"📦 Loading banking dataset...\")\n",
    "\n",
    "# Simple data loading\n",
    "df = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(dataset_path)\n",
    "\n",
    "print(\"📋 Schema:\")\n",
    "df.printSchema()\n",
    "\n",
    "\n",
    "print(\"type:\", type(df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a11149d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:====================================================>     (9 + 1) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Loaded 13,305,915 transactions\n",
      "+--------------+-------------------+-----------+-------+-------+-----------------+-----------+-------------+--------------+-------+----+------+-------------------+\n",
      "|transaction_id|               date|customer_id|card_id| amount|         use_chip|merchant_id|merchant_city|merchant_state|    zip| mcc|errors|   transaction_date|\n",
      "+--------------+-------------------+-----------+-------+-------+-----------------+-----------+-------------+--------------+-------+----+------+-------------------+\n",
      "|       7475327|2010-01-01 00:01:00|       1556|   2972|$-77.00|Swipe Transaction|      59935|       Beulah|            ND|58523.0|5499|  NULL|2010-01-01 00:01:00|\n",
      "|       7475328|2010-01-01 00:02:00|        561|   4575| $14.57|Swipe Transaction|      67570|   Bettendorf|            IA|52722.0|5311|  NULL|2010-01-01 00:02:00|\n",
      "|       7475329|2010-01-01 00:02:00|       1129|    102| $80.00|Swipe Transaction|      27092|        Vista|            CA|92084.0|4829|  NULL|2010-01-01 00:02:00|\n",
      "|       7475331|2010-01-01 00:05:00|        430|   2860|$200.00|Swipe Transaction|      27092|  Crown Point|            IN|46307.0|4829|  NULL|2010-01-01 00:05:00|\n",
      "|       7475332|2010-01-01 00:06:00|        848|   3915| $46.41|Swipe Transaction|      13051|      Harwood|            MD|20776.0|5813|  NULL|2010-01-01 00:06:00|\n",
      "+--------------+-------------------+-----------+-------+-------+-----------------+-----------+-------------+--------------+-------+----+------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Basic column mapping\n",
    "df = (\n",
    "    df.withColumnRenamed(\"client_id\", \"customer_id\")\n",
    "    .withColumnRenamed(\"id\", \"transaction_id\")\n",
    "    .withColumn(\"transaction_date\", to_timestamp(col(\"date\")))\n",
    ")\n",
    "\n",
    "\n",
    "print(f\"📊 Loaded {df.count():,} transactions\")\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64adc563",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Adding basic features...\n",
      "✅ Amount data types - USD: double\n",
      "✅ Features added and temp view created!\n",
      "+-----------+----------+-----------------+----+----------+----------+\n",
      "|customer_id|amount_usd|merchant_category|hour|is_weekend|zip_region|\n",
      "+-----------+----------+-----------------+----+----------+----------+\n",
      "|       1556|     -77.0|            Other|   0|     false| Southeast|\n",
      "|        561|     14.57|            Other|   0|     false| Southeast|\n",
      "|       1129|      80.0|            Other|   0|     false|      West|\n",
      "|        430|     200.0|            Other|   0|     false| Southeast|\n",
      "|        848|     46.41|       Restaurant|   0|     false| Northeast|\n",
      "|       1807|      4.81|            Other|   0|     false| Northeast|\n",
      "|       1556|      77.0|            Other|   0|     false| Southeast|\n",
      "|       1684|     26.46|            Other|   0|     false|   Unknown|\n",
      "|        335|    261.58|            Other|   0|     false|   Unknown|\n",
      "|        351|     10.74|       Restaurant|   0|     false| Northeast|\n",
      "|        554|      3.51|            Other|   0|     false|   Central|\n",
      "|        605|      2.58|          Grocery|   0|     false| Northeast|\n",
      "|       1556|     39.63|            Other|   0|     false| Southeast|\n",
      "|       1797|     43.33|            Other|   0|     false|      West|\n",
      "|        114|     49.42|      Gas Station|   0|     false|      West|\n",
      "|       1634|      1.09|            Other|   0|     false|   Central|\n",
      "|        646|     73.79|            Other|   0|     false| Northeast|\n",
      "|       1129|     100.0|            Other|   0|     false|      West|\n",
      "|        394|     26.04|            Other|   0|     false|   Unknown|\n",
      "|        114|     -64.0|      Gas Station|   0|     false|      West|\n",
      "+-----------+----------+-----------------+----+----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add basic features\n",
    "print(\"🔧 Adding basic features...\")\n",
    "\n",
    "# First, ensure amount is properly numeric - convert to amount_usd as double\n",
    "df = df.withColumn(\"amount_usd\", regexp_replace(col(\"amount\"), \"[$]\", \"\").cast(\"double\"))\n",
    "\n",
    "print(f\"✅ Amount data types - USD: {dict(df.dtypes)['amount_usd']}\")\n",
    "\n",
    "# Add simple time features\n",
    "df = df.withColumn(\"hour\", hour(col(\"transaction_date\"))) \\\n",
    "       .withColumn(\"is_weekend\", dayofweek(col(\"transaction_date\")).isin([1, 7]))\n",
    "\n",
    "# Add merchant category (simplified)\n",
    "df = df.withColumn(\"merchant_category\",\n",
    "                   when(col(\"mcc\").isin(5411, 5441), \"Grocery\")\n",
    "                   .when(col(\"mcc\").isin(5812, 5813), \"Restaurant\") \n",
    "                   .when(col(\"mcc\").isin(5541, 5542), \"Gas Station\")\n",
    "                   .otherwise(\"Other\"))\n",
    "\n",
    "# Add zip_region column\n",
    "df = df.withColumn(\"zip_region\",\n",
    "                   when(col(\"zip\").substr(1,1).isin([\"0\", \"1\", \"2\"]), \"Northeast\")\n",
    "                   .when(col(\"zip\").substr(1,1).isin([\"3\", \"4\", \"5\"]), \"Southeast\") \n",
    "                   .when(col(\"zip\").substr(1,1).isin([\"6\", \"7\"]), \"Central\")\n",
    "                   .when(col(\"zip\").substr(1,1).isin([\"8\", \"9\"]), \"West\")\n",
    "                   .otherwise(\"Unknown\"))\n",
    "\n",
    "\n",
    "\n",
    "# Show sample with numeric amounts\n",
    "df.select(\"customer_id\", \"amount_usd\", \"merchant_category\", \"hour\", \"is_weekend\", \"zip_region\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9609b4d",
   "metadata": {},
   "source": [
    "## 2. Basic Data Exploration with Spark 🐼➡️🔥\n",
    "**Goal:** Explore the 1GB+ dataset with Spark (no pandas copies)\n",
    "\n",
    "### 🎓 Live Coding Exercise:\n",
    "- **Instructor:** Demonstrates Spark actions and SQL\n",
    "- **Students:** Build aggregations and quality checks at scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d2f757ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 BANKING DATA OVERVIEW\n",
      "🎯 LIVE CODING: We'll build a comprehensive exploration function together\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+\n",
      "|           min_date|           max_date|\n",
      "+-------------------+-------------------+\n",
      "|2010-01-01 00:01:00|2019-10-31 23:59:00|\n",
      "+-------------------+-------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+\n",
      "|weekday|  count|\n",
      "+-------+-------+\n",
      "|    Fri|1895372|\n",
      "|    Mon|1896914|\n",
      "|    Sat|1902370|\n",
      "|    Sun|1899044|\n",
      "|    Thu|1918666|\n",
      "|    Tue|1897678|\n",
      "|    Wed|1895871|\n",
      "+-------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 25:===================================================>     (9 + 1) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------------+--------------------+------+------+\n",
      "|n       |avg              |quantiles           |min   |max   |\n",
      "+--------+-----------------+--------------------+------+------+\n",
      "|13305915|42.97603902324682|[8.93, 28.99, 63.71]|-500.0|6820.2|\n",
      "+--------+-----------------+--------------------+------+------+\n",
      "\n",
      "✅ Ready for live coding session!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 🧑‍🏫 INSTRUCTOR: Basic Spark exploration (LIVE CODING)\n",
    "# TODO: Live code the data exploration function\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "print(\"📊 BANKING DATA OVERVIEW\")\n",
    "print(\"🎯 LIVE CODING: We'll build a comprehensive exploration function together\")\n",
    "\n",
    "# - Date range analysis with min/max\n",
    "df.select(\n",
    "    min(\"transaction_date\").alias(\"min_date\"),\n",
    "    max(\"transaction_date\").alias(\"max_date\")\n",
    ").show()\n",
    "# - Groupby operations for weekday analysis\n",
    "df.withColumn(\"weekday\", date_format(col(\"transaction_date\"), \"E\")).groupBy(\"weekday\").count().orderBy(\"weekday\").show()\n",
    "# - Distinct counts for unique customers\n",
    "df.select(F.countDistinct(\"customer_id\").alias(\"unique_customers\"))\n",
    "# - Statistical functions and percentiles\n",
    "# - Top N analysis with orderBy\n",
    "df.select(\n",
    "    F.count(\"amount_usd\").alias(\"n\"),\n",
    "    F.mean(\"amount_usd\").alias(\"avg\"),\n",
    "    F.expr(\"percentile_approx(amount_usd, array(0.25,0.5,0.75), 10000)\").alias(\"quantiles\"),\n",
    "    F.min(\"amount_usd\").alias(\"min\"),\n",
    "    F.max(\"amount_usd\").alias(\"max\"),\n",
    ").show(truncate=False)\n",
    "\n",
    "\n",
    "\n",
    "print(\"✅ Ready for live coding session!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38f42ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🧑‍🎓 STUDENT EXERCISE: Basic data exploration\n",
    "print(\"📊 EXERCISE: Complete the basic dataset analysis\")\n",
    "\n",
    "# TODO: Students complete these basic operations\n",
    "print(\"✏️ YOUR TASKS:\")\n",
    "print(\"1. Show total transaction count\")\n",
    "print(\"2. Show unique customer count\") \n",
    "print(\"3. Create spending analysis by merchant category\")\n",
    "print(\"4. Order results by total spending\")\n",
    "\n",
    "print(\"\\n🎯 EXERCISE OBJECTIVES:\")\n",
    "print(\"• Practice DataFrame operations\")\n",
    "print(\"• Learn aggregation functions (sum, count)\")\n",
    "print(\"• Use groupBy and orderBy\")\n",
    "print(\"• Work with column aliases\")\n",
    "\n",
    "# Hint: Use df.count(), df.select().distinct().count()\n",
    "# Hint: Use df.groupBy().agg(sum(), count()).orderBy()\n",
    "\n",
    "print(\"✅ Ready for your solution!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29205fbb",
   "metadata": {},
   "source": [
    "## 3. Spark Session Recap 🚀\n",
    "Spark is already initialized. We’ll keep this short and move to SQL analytics.\n",
    "\n",
    "- Session tuned for local development and large CSVs\n",
    "- Temp view `banking_transactions` is ready\n",
    "- Proceed to analytics at scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649b9020",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Optional) Spark utilities\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "print(\"ℹ️ Spark utilities available. Session already created above.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1792d1eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 LIVE CODING: Basic Spark operations\n",
      "💰 Amount statistics:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+\n",
      "|summary|       amount_usd|\n",
      "+-------+-----------------+\n",
      "|  count|         13305915|\n",
      "|   mean|42.97603902324682|\n",
      "| stddev|81.65574765375871|\n",
      "|    min|           -500.0|\n",
      "|    max|           6820.2|\n",
      "+-------+-----------------+\n",
      "\n",
      "✅ Features added and temp view created!\n",
      "✏️ EXERCISE: Weekend vs Weekday Analysis\n",
      "TODO: Write SQL query to compare weekend vs weekday spending\n",
      "Hint: Use is_weekend column, COUNT(*), SUM(amount_usd)\n",
      "Hint: GROUP BY is_weekend\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 34:===================================================>     (9 + 1) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------+------------------+--------------------+---------------+\n",
      "|is_weekend|transaction_count|      avg_spending|      total_spending|median_spending|\n",
      "+----------+-----------------+------------------+--------------------+---------------+\n",
      "|      true|          3801414|43.101191701298404|1.6384547354999956E8|          29.06|\n",
      "|     false|          9504501| 42.92598304003574| 4.079900487300027E8|          28.96|\n",
      "+----------+-----------------+------------------+--------------------+---------------+\n",
      "\n",
      "✅ Ready for your SQL solution!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Simple Spark DataFrame operations\n",
    "print(\"🔧 LIVE CODING: Basic Spark operations\")\n",
    "\n",
    "# Show basic dataset statistics\n",
    "print(\"💰 Amount statistics:\")\n",
    "df.select(\"amount_usd\").describe().show()\n",
    "\n",
    "# Create temp view for SQL\n",
    "df.createOrReplaceTempView(\"transactions\")\n",
    "print(\"✅ Features added and temp view created!\")\n",
    "\n",
    "print(\"✏️ EXERCISE: Weekend vs Weekday Analysis\")\n",
    "print(\"TODO: Write SQL query to compare weekend vs weekday spending\")\n",
    "print(\"Hint: Use is_weekend column, COUNT(*), SUM(amount_usd)\")\n",
    "print(\"Hint: GROUP BY is_weekend\")\n",
    "\n",
    "# Template:\n",
    "spark.sql(\n",
    "    \"\"\"\n",
    "SELECT \n",
    "    is_weekend,\n",
    "    COUNT(*) AS transaction_count,\n",
    "    AVG(amount_usd) AS avg_spending,\n",
    "    SUM(amount_usd) AS total_spending,\n",
    "    MEDIAN(amount_usd) AS median_spending \n",
    "FROM transactions \n",
    "GROUP BY is_weekend\n",
    "\"\"\"\n",
    ").show()\n",
    "\n",
    "print(\"✅ Ready for your SQL solution!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1092b80a",
   "metadata": {},
   "source": [
    "## 4. Advanced Spark SQL Analytics 🔍\n",
    "**Goal:** Complex banking analytics using SQL on big data\n",
    "\n",
    "### 🏦 Real Banking Use Cases:\n",
    "- **Fraud Detection:** Unusual spending patterns\n",
    "- **Customer Segmentation:** Spending behavior analysis\n",
    "- **Risk Assessment:** Transaction pattern analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1292fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🧑‍🏫 INSTRUCTOR: Banking analytics (LIVE CODING)\n",
    "print(\"🔍 LIVE CODING: Banking Analytics Session\")\n",
    "\n",
    "print(\"🎯 We'll build together:\")\n",
    "print(\"1. Top customers by spending\")\n",
    "print(\"2. Top merchants by transaction volume\")\n",
    "print(\"3. Revenue analysis by merchant\")\n",
    "\n",
    "print(\"• Complex SQL with aggregate functions\")\n",
    "print(\"• Window functions and ranking\")\n",
    "print(\"• Business KPI calculations\")\n",
    "print(\"• ORDER BY with LIMIT for top-N queries\")\n",
    "\n",
    "# TODO: Live code the customer analysis\n",
    "print(\"\\n👑 COMING UP: Top customers analysis\")\n",
    "\n",
    "# TODO: Live code the merchant analysis  \n",
    "print(\"🏪 COMING UP: Merchant performance analysis\")\n",
    "\n",
    "print(\"✅ Ready for live banking analytics session!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d106fcbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚨 BASIC FRAUD DETECTION\n",
      "💰 Transactions above $200:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+-----------+\n",
      "|customer_id|amount_usd|merchant_id|\n",
      "+-----------+----------+-----------+\n",
      "|        708|    6820.2|      34524|\n",
      "|       1081|   6613.44|       9026|\n",
      "|       1259|   5913.37|      85983|\n",
      "|       1487|   5813.78|       9026|\n",
      "|        278|   5696.78|       7202|\n",
      "+-----------+----------+-----------+\n",
      "\n",
      "✅ Basic fraud detection complete!\n",
      "🚨 POTENTIAL FRAUD DETECTION:\n",
      "Find customers with transactions > 3 standard deviations from their average\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 47:>                                                         (0 + 8) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------------+--------------------+------------------+\n",
      "| risk_level|transaction_count|        total_amount|        avg_amount|\n",
      "+-----------+-----------------+--------------------+------------------+\n",
      "|  HIGH_RISK|           209928| 5.057713439000009E7|240.92610032963725|\n",
      "|MEDIUM_RISK|           249851|2.8490776459999997E7| 114.0310683567406|\n",
      "|     NORMAL|         12846136|4.9276761142999566E8|  38.3592086702177|\n",
      "+-----------+-----------------+--------------------+------------------+\n",
      "\n",
      "✅ Basic fraud detection complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 🧑‍🎓 STUDENT EXERCISE: Fraud Detection\n",
    "print(\"🚨 BASIC FRAUD DETECTION\")\n",
    "\n",
    "# High amount transactions (potential fraud) using numeric amounts\n",
    "print(\"💰 Transactions above $200:\")\n",
    "spark.sql(\n",
    "    \"\"\"\n",
    "SELECT customer_id, \n",
    "       ROUND(amount_usd, 2) as amount_usd,\n",
    "       merchant_id\n",
    "FROM transactions \n",
    "WHERE amount_usd > 200\n",
    "ORDER BY amount_usd DESC\n",
    "LIMIT 5\n",
    "\"\"\"\n",
    ").show()\n",
    "\n",
    "print(\"✅ Basic fraud detection complete!\")\n",
    "print(\"🚨 POTENTIAL FRAUD DETECTION:\")\n",
    "print(\"Find customers with transactions > 3 standard deviations from their average\")\n",
    "\n",
    "fraud_query = \"\"\"\n",
    "WITH customer_stats AS (\n",
    "    SELECT \n",
    "        customer_id,\n",
    "        transaction_date,\n",
    "        amount_usd,\n",
    "        AVG(amount_usd) OVER (PARTITION BY customer_id) as avg_amount,\n",
    "        ABS(STDDEV_POP(amount_usd) OVER (PARTITION BY customer_id)) as stddev_amount\n",
    "    FROM transactions\n",
    "),\n",
    "potential_fraud AS (\n",
    "    SELECT \n",
    "        customer_id,\n",
    "        transaction_date,\n",
    "        amount_usd,\n",
    "        avg_amount,\n",
    "        stddev_amount,\n",
    "        ABS(amount_usd - avg_amount) as deviation,\n",
    "        CASE \n",
    "            WHEN ABS(amount_usd - avg_amount) > 3 * stddev_amount \n",
    "            THEN 'HIGH_RISK'\n",
    "            WHEN ABS(amount_usd - avg_amount) > 2 * stddev_amount \n",
    "            THEN 'MEDIUM_RISK'\n",
    "            ELSE 'NORMAL'\n",
    "        END as risk_level\n",
    "    FROM customer_stats\n",
    ")\n",
    "SELECT \n",
    "    risk_level, \n",
    "    COUNT(*) as transaction_count,\n",
    "    SUM(amount_usd) as total_amount,\n",
    "    AVG(amount_usd) as avg_amount\n",
    "FROM potential_fraud\n",
    "GROUP BY risk_level\n",
    "ORDER BY risk_level\n",
    "\"\"\"\n",
    "spark.sql(fraud_query).show()\n",
    "print(\"✅ Basic fraud detection complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6c301f",
   "metadata": {},
   "source": [
    "## 5. Simple Cloud Deployment 🌥️\n",
    "**Goal:** Basic overview of deploying to cloud\n",
    "\n",
    "### 🌟 Why Cloud?\n",
    "- **Scale:** Handle big datasets\n",
    "- **Storage:** Secure data storage  \n",
    "- **Compute:** More processing power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3b21eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🧑‍🏫 INSTRUCTOR: Simple cloud overview (demo only)\n",
    "print(\"☁️ CLOUD DEPLOYMENT BASICS\")\n",
    "print(\"Key concepts:\")\n",
    "print(\"1. Upload data to cloud storage\")\n",
    "print(\"2. Create compute cluster\") \n",
    "print(\"3. Run Spark jobs\")\n",
    "print(\"4. Store results\")\n",
    "\n",
    "print(\"\\n✅ Cloud overview complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f6ea5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ COMPLETE SOLUTION: Customize GCP deployment\n",
    "print(\"🎯 COMPLETE: Customize the deployment configuration!\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Solution 1: Update configuration for your environment\n",
    "print(\"⚙️ CUSTOMIZE YOUR CONFIGURATION:\")\n",
    "my_gcp_config = {\n",
    "    \"project_id\": \"banking-analytics-demo-2025\",  # Example project ID\n",
    "    \"bucket_name\": \"banking-data-workshop-eu\",   # Example bucket name\n",
    "    \"region\": \"europe-west3\",                    # Frankfurt region for GDPR compliance\n",
    "    \"dataset_location\": \"EU\",                    # European Union for data residency\n",
    "    \"service_account_email\": \"banking-analytics@banking-analytics-demo-2025.iam.gserviceaccount.com\",\n",
    "    \"vpc_network\": \"banking-vpc\",                # Custom VPC for security\n",
    "    \"subnet\": \"banking-subnet-eu-west3\"          # Specific subnet\n",
    "}\n",
    "\n",
    "print(\"📝 Your GCP Config:\")\n",
    "import json\n",
    "print(json.dumps(my_gcp_config, indent=2))\n",
    "\n",
    "# Solution 2: Create a deployment checklist\n",
    "print(\"\\n✅ DEPLOYMENT CHECKLIST:\")\n",
    "deployment_checklist = [\n",
    "    \"GCP project created and billing enabled\",\n",
    "    \"Service account created with necessary permissions\",\n",
    "    \"Cloud Storage bucket created in EU region\",\n",
    "    \"Databricks workspace provisioned\"\n",
    "]\n",
    "print(\"\\n✅ Cloud overview complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc056ce1",
   "metadata": {},
   "source": [
    "## 6. Simple Data Integration 🔗\n",
    "**Goal:** Combine banking data with external sources\n",
    "\n",
    "### 💡 Real Banking Examples:\n",
    "- **Exchange Rates:** Convert international transactions\n",
    "- **Interest Rates:** Economic impact on spending\n",
    "- **Merchant Data:** Enhanced merchant information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd481a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🧑‍🏫 INSTRUCTOR: API Integration (LIVE CODING)\n",
    "print(\"🔗 LIVE CODING: External Data Integration\")\n",
    "\n",
    "print(\"🎯 We'll demonstrate:\")\n",
    "print(\"1. Making API calls to financial services\")\n",
    "print(\"2. Handling JSON responses\")\n",
    "print(\"3. Error handling and fallbacks\")\n",
    "print(\"4. Integrating live data into Spark DataFrames\")\n",
    "\n",
    "# Students will learn:\n",
    "import requests\n",
    "\n",
    "print(\"📊 COMING UP: Live exchange rate API integration\")\n",
    "print(\"💡 Key concepts:\")\n",
    "print(\"• REST API calls with requests library\")\n",
    "print(\"• JSON data parsing\")\n",
    "print(\"• Try/except error handling\")\n",
    "print(\"• DataFrame transformations with live data\")\n",
    "\n",
    "# TODO: Live code the API integration\n",
    "api_url = \"https://api.exchangeratesapi.io/v1/latest?access_key=24da234d4ded987472b5ece3b4981c9b&format=1\"\n",
    "\n",
    "print(\"✅ Ready for live API integration session!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24d8bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🧑‍🎓 FINAL EXERCISE: Integration Analysis\n",
    "print(\"📊 FINAL CHALLENGE: Live Exchange Rate Analysis\")\n",
    "\n",
    "print(\"🎯 CAPSTONE EXERCISE:\")\n",
    "print(\"Combine everything you've learned to analyze currency impact!\")\n",
    "\n",
    "print(\"\\n✏️ YOUR MISSION:\")\n",
    "print(\"1. Use the live exchange rate data we just integrated\")\n",
    "print(\"2. Compare USD vs EUR spending by merchant category\") \n",
    "print(\"3. Calculate currency impact percentages\")\n",
    "print(\"4. Generate business insights\")\n",
    "\n",
    "print(\"\\n💡 REQUIRED QUERIES:\")\n",
    "print(\"Query 1: Basic comparison\")\n",
    "print(\"• SELECT merchant_category, SUM(amount_usd), SUM(amount_eur)\")\n",
    "print(\"• Calculate totals and averages for both currencies\")\n",
    "print(\"• GROUP BY merchant_category\")\n",
    "\n",
    "print(\"\\nQuery 2: Advanced analysis\")  \n",
    "print(\"• Calculate (amount_usd - amount_eur) differences\")\n",
    "print(\"• Compute percentage impact: (usd-eur)/eur * 100\")\n",
    "print(\"• Order by currency impact\")\n",
    "\n",
    "print(\"\\n🎯 LEARNING OBJECTIVES:\")\n",
    "print(\"• Integration of live external data\")\n",
    "print(\"• Multi-currency financial analysis\") \n",
    "print(\"• Business insight generation\")\n",
    "print(\"• Advanced SQL calculations\")\n",
    "\n",
    "# TODO: Students write their analysis queries\n",
    "live_rate_analysis = \"\"\"\n",
    "-- Write your currency comparison query here\n",
    "-- Include: merchant_category, totals, averages, differences\n",
    "\"\"\"\n",
    "\n",
    "currency_impact_query = \"\"\"\n",
    "-- Write your advanced impact analysis here\n",
    "-- Calculate percentage impacts and ranking\n",
    "\"\"\"\n",
    "\n",
    "print(\"✅ Ready for your final analysis challenge!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f38621",
   "metadata": {},
   "source": [
    "## 🎯 Workshop Summary\n",
    "\n",
    "You've learned the essentials of big data analytics:\n",
    "\n",
    "1. **PySpark Basics:** Loading and processing data\n",
    "2. **SQL Analysis:** Simple aggregations and insights\n",
    "3. **Banking Analytics:** Basic fraud detection\n",
    "4. **Cloud Concepts:** Deployment overview"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
