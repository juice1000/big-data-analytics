{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05e261dc",
   "metadata": {},
   "source": [
    "# Banking Data Analysis - Live Coding Workshop\n",
    "## Big Data Analytics im Banking | 13:00-15:40\n",
    "\n",
    "### üéØ **Workshop Agenda**\n",
    "- **13:00-13:45:** Einf√ºhrung in Datenanalyse + Banking Transaction Analysis\n",
    "- **13:55-14:40:** Spark Deep-Dive & GCP Setup\n",
    "- **14:50-15:40:** Datenbeschaffung und -integration\n",
    "\n",
    "### üõ† **Was wir heute lernen:**\n",
    "1. **Datenanalyseprozess** in der Praxis\n",
    "2. **Data Mining** f√ºr Banking-Patterns\n",
    "3. **Spark Setup** und SQL-Queries\n",
    "4. **GCP/Databricks** Configuration\n",
    "5. **Web Scraping** f√ºr Financial Data\n",
    "6. **Multi-Source Integration**\n",
    "\n",
    "### üìã **Live Coding Approach**\n",
    "- **Instructor demonstrates** ‚Üí **Students modify/extend**\n",
    "- **Short code blocks** with thorough comments\n",
    "- **Interactive exercises** at each step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b73afc",
   "metadata": {},
   "source": [
    "## 1. Load Large Banking Transactions (PySpark) üè¶\n",
    "**Goal:** Load a >1GB CSV efficiently using PySpark and prepare it for analysis\n",
    "\n",
    "Dataset: `transactions_data.csv` (set the path below)\n",
    "\n",
    "### üéì Live Coding Exercise:\n",
    "- **Instructor:** Sets up Spark and loads the dataset with an explicit schema or fast inference\n",
    "- **Students:** Add derived columns and validate data quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9f66ed9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/11 14:29:19 WARN Utils: Your hostname, Maclook-Bro.local resolves to a loopback address: 127.0.0.1; using 192.168.222.131 instead (on interface en0)\n",
      "25/08/11 14:29:19 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/08/11 14:29:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/08/11 14:29:20 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Spark ready!\n",
      "Version: 3.5.3\n"
     ]
    }
   ],
   "source": [
    "# Simple PySpark setup for banking data\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Dataset path\n",
    "dataset_path = \"../data/transactions_data.csv\"\n",
    "\n",
    "if 'spark' in locals():\n",
    "    print(\"Stopping existing Spark session...\")\n",
    "    spark.stop()  # Stop any existing Spark session\n",
    "\n",
    "\n",
    "# Create basic Spark session\n",
    "# We set the driver memory to 4GB for better performance\n",
    "# We use local[*] to utilize all available cores on our machine\n",
    "spark = SparkSession.builder.appName(\"Banking Analysis\").config(\"spark.driver.memory\", \"4g\").master(\"local[*]\").getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"WARN\") # Set log level to WARN to reduce noise\n",
    "\n",
    "print(\"‚úÖ Spark ready!\")\n",
    "print(f\"Version: {spark.version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f01ebe08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Loading banking dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:====================================================>     (9 + 1) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã Schema:\n",
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- date: timestamp (nullable = true)\n",
      " |-- client_id: integer (nullable = true)\n",
      " |-- card_id: integer (nullable = true)\n",
      " |-- amount: string (nullable = true)\n",
      " |-- use_chip: string (nullable = true)\n",
      " |-- merchant_id: integer (nullable = true)\n",
      " |-- merchant_city: string (nullable = true)\n",
      " |-- merchant_state: string (nullable = true)\n",
      " |-- zip: double (nullable = true)\n",
      " |-- mcc: integer (nullable = true)\n",
      " |-- errors: string (nullable = true)\n",
      "\n",
      "type: <class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Load and prepare banking data\n",
    "print(\"üì¶ Loading banking dataset...\")\n",
    "\n",
    "# Simple data loading\n",
    "df = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(dataset_path)\n",
    "\n",
    "print(\"üìã Schema:\")\n",
    "df.printSchema()\n",
    "\n",
    "\n",
    "print(\"type:\", type(df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a11149d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:====================================================>     (9 + 1) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Loaded 13,305,915 transactions\n",
      "+--------------+-------------------+-----------+-------+-------+-----------------+-----------+-------------+--------------+-------+----+------+-------------------+\n",
      "|transaction_id|               date|customer_id|card_id| amount|         use_chip|merchant_id|merchant_city|merchant_state|    zip| mcc|errors|   transaction_date|\n",
      "+--------------+-------------------+-----------+-------+-------+-----------------+-----------+-------------+--------------+-------+----+------+-------------------+\n",
      "|       7475327|2010-01-01 00:01:00|       1556|   2972|$-77.00|Swipe Transaction|      59935|       Beulah|            ND|58523.0|5499|  NULL|2010-01-01 00:01:00|\n",
      "|       7475328|2010-01-01 00:02:00|        561|   4575| $14.57|Swipe Transaction|      67570|   Bettendorf|            IA|52722.0|5311|  NULL|2010-01-01 00:02:00|\n",
      "|       7475329|2010-01-01 00:02:00|       1129|    102| $80.00|Swipe Transaction|      27092|        Vista|            CA|92084.0|4829|  NULL|2010-01-01 00:02:00|\n",
      "|       7475331|2010-01-01 00:05:00|        430|   2860|$200.00|Swipe Transaction|      27092|  Crown Point|            IN|46307.0|4829|  NULL|2010-01-01 00:05:00|\n",
      "|       7475332|2010-01-01 00:06:00|        848|   3915| $46.41|Swipe Transaction|      13051|      Harwood|            MD|20776.0|5813|  NULL|2010-01-01 00:06:00|\n",
      "+--------------+-------------------+-----------+-------+-------+-----------------+-----------+-------------+--------------+-------+----+------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Basic column mapping\n",
    "df = (\n",
    "    df.withColumnRenamed(\"client_id\", \"customer_id\")\n",
    "    .withColumnRenamed(\"id\", \"transaction_id\")\n",
    "    .withColumn(\"transaction_date\", to_timestamp(col(\"date\")))\n",
    ")\n",
    "\n",
    "\n",
    "print(f\"üìä Loaded {df.count():,} transactions\")\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64adc563",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Adding basic features...\n",
      "‚úÖ Amount data types - USD: double\n",
      "‚úÖ Features added and temp view created!\n",
      "+-----------+----------+-----------------+----+----------+----------+\n",
      "|customer_id|amount_usd|merchant_category|hour|is_weekend|zip_region|\n",
      "+-----------+----------+-----------------+----+----------+----------+\n",
      "|       1556|     -77.0|            Other|   0|     false| Southeast|\n",
      "|        561|     14.57|            Other|   0|     false| Southeast|\n",
      "|       1129|      80.0|            Other|   0|     false|      West|\n",
      "|        430|     200.0|            Other|   0|     false| Southeast|\n",
      "|        848|     46.41|       Restaurant|   0|     false| Northeast|\n",
      "|       1807|      4.81|            Other|   0|     false| Northeast|\n",
      "|       1556|      77.0|            Other|   0|     false| Southeast|\n",
      "|       1684|     26.46|            Other|   0|     false|   Unknown|\n",
      "|        335|    261.58|            Other|   0|     false|   Unknown|\n",
      "|        351|     10.74|       Restaurant|   0|     false| Northeast|\n",
      "|        554|      3.51|            Other|   0|     false|   Central|\n",
      "|        605|      2.58|          Grocery|   0|     false| Northeast|\n",
      "|       1556|     39.63|            Other|   0|     false| Southeast|\n",
      "|       1797|     43.33|            Other|   0|     false|      West|\n",
      "|        114|     49.42|      Gas Station|   0|     false|      West|\n",
      "|       1634|      1.09|            Other|   0|     false|   Central|\n",
      "|        646|     73.79|            Other|   0|     false| Northeast|\n",
      "|       1129|     100.0|            Other|   0|     false|      West|\n",
      "|        394|     26.04|            Other|   0|     false|   Unknown|\n",
      "|        114|     -64.0|      Gas Station|   0|     false|      West|\n",
      "+-----------+----------+-----------------+----+----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add basic features\n",
    "print(\"üîß Adding basic features...\")\n",
    "\n",
    "# First, ensure amount is properly numeric - convert to amount_usd as double\n",
    "df = df.withColumn(\"amount_usd\", regexp_replace(col(\"amount\"), \"[$]\", \"\").cast(\"double\"))\n",
    "\n",
    "print(f\"‚úÖ Amount data types - USD: {dict(df.dtypes)['amount_usd']}\")\n",
    "\n",
    "# Add simple time features\n",
    "df = df.withColumn(\"hour\", hour(col(\"transaction_date\"))) \\\n",
    "       .withColumn(\"is_weekend\", dayofweek(col(\"transaction_date\")).isin([1, 7]))\n",
    "\n",
    "# Add merchant category (simplified)\n",
    "df = df.withColumn(\"merchant_category\",\n",
    "                   when(col(\"mcc\").isin(5411, 5441), \"Grocery\")\n",
    "                   .when(col(\"mcc\").isin(5812, 5813), \"Restaurant\") \n",
    "                   .when(col(\"mcc\").isin(5541, 5542), \"Gas Station\")\n",
    "                   .otherwise(\"Other\"))\n",
    "\n",
    "# Add zip_region column\n",
    "df = df.withColumn(\"zip_region\",\n",
    "                   when(col(\"zip\").substr(1,1).isin([\"0\", \"1\", \"2\"]), \"Northeast\")\n",
    "                   .when(col(\"zip\").substr(1,1).isin([\"3\", \"4\", \"5\"]), \"Southeast\") \n",
    "                   .when(col(\"zip\").substr(1,1).isin([\"6\", \"7\"]), \"Central\")\n",
    "                   .when(col(\"zip\").substr(1,1).isin([\"8\", \"9\"]), \"West\")\n",
    "                   .otherwise(\"Unknown\"))\n",
    "\n",
    "\n",
    "\n",
    "# Show sample with numeric amounts\n",
    "df.select(\"customer_id\", \"amount_usd\", \"merchant_category\", \"hour\", \"is_weekend\", \"zip_region\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9609b4d",
   "metadata": {},
   "source": [
    "## 2. Basic Data Exploration with Spark üêº‚û°Ô∏èüî•\n",
    "**Goal:** Explore the 1GB+ dataset with Spark (no pandas copies)\n",
    "\n",
    "### üéì Live Coding Exercise:\n",
    "- **Instructor:** Demonstrates Spark actions and SQL\n",
    "- **Students:** Build aggregations and quality checks at scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d2f757ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä BANKING DATA OVERVIEW\n",
      "üéØ LIVE CODING: We'll build a comprehensive exploration function together\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+\n",
      "|           min_date|           max_date|\n",
      "+-------------------+-------------------+\n",
      "|2010-01-01 00:01:00|2019-10-31 23:59:00|\n",
      "+-------------------+-------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+\n",
      "|weekday|  count|\n",
      "+-------+-------+\n",
      "|    Fri|1895372|\n",
      "|    Mon|1896914|\n",
      "|    Sat|1902370|\n",
      "|    Sun|1899044|\n",
      "|    Thu|1918666|\n",
      "|    Tue|1897678|\n",
      "|    Wed|1895871|\n",
      "+-------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 25:===================================================>     (9 + 1) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------------+--------------------+------+------+\n",
      "|n       |avg              |quantiles           |min   |max   |\n",
      "+--------+-----------------+--------------------+------+------+\n",
      "|13305915|42.97603902324682|[8.93, 28.99, 63.71]|-500.0|6820.2|\n",
      "+--------+-----------------+--------------------+------+------+\n",
      "\n",
      "‚úÖ Ready for live coding session!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# üßë‚Äçüè´ INSTRUCTOR: Basic Spark exploration (LIVE CODING)\n",
    "# TODO: Live code the data exploration function\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "print(\"üìä BANKING DATA OVERVIEW\")\n",
    "print(\"üéØ LIVE CODING: We'll build a comprehensive exploration function together\")\n",
    "\n",
    "# - Date range analysis with min/max\n",
    "df.select(\n",
    "    min(\"transaction_date\").alias(\"min_date\"),\n",
    "    max(\"transaction_date\").alias(\"max_date\")\n",
    ").show()\n",
    "# - Groupby operations for weekday analysis\n",
    "df.withColumn(\"weekday\", date_format(col(\"transaction_date\"), \"E\")).groupBy(\"weekday\").count().orderBy(\"weekday\").show()\n",
    "# - Distinct counts for unique customers\n",
    "df.select(F.countDistinct(\"customer_id\").alias(\"unique_customers\"))\n",
    "# - Statistical functions and percentiles\n",
    "# - Top N analysis with orderBy\n",
    "df.select(\n",
    "    F.count(\"amount_usd\").alias(\"n\"),\n",
    "    F.mean(\"amount_usd\").alias(\"avg\"),\n",
    "    F.expr(\"percentile_approx(amount_usd, array(0.25,0.5,0.75), 10000)\").alias(\"quantiles\"),\n",
    "    F.min(\"amount_usd\").alias(\"min\"),\n",
    "    F.max(\"amount_usd\").alias(\"max\"),\n",
    ").show(truncate=False)\n",
    "\n",
    "\n",
    "\n",
    "print(\"‚úÖ Ready for live coding session!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38f42ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üßë‚Äçüéì STUDENT EXERCISE: Basic data exploration\n",
    "print(\"üìä EXERCISE: Complete the basic dataset analysis\")\n",
    "\n",
    "# TODO: Students complete these basic operations\n",
    "print(\"‚úèÔ∏è YOUR TASKS:\")\n",
    "print(\"1. Show total transaction count\")\n",
    "print(\"2. Show unique customer count\") \n",
    "print(\"3. Create spending analysis by merchant category\")\n",
    "print(\"4. Order results by total spending\")\n",
    "\n",
    "print(\"\\nüéØ EXERCISE OBJECTIVES:\")\n",
    "print(\"‚Ä¢ Practice DataFrame operations\")\n",
    "print(\"‚Ä¢ Learn aggregation functions (sum, count)\")\n",
    "print(\"‚Ä¢ Use groupBy and orderBy\")\n",
    "print(\"‚Ä¢ Work with column aliases\")\n",
    "\n",
    "# Hint: Use df.count(), df.select().distinct().count()\n",
    "# Hint: Use df.groupBy().agg(sum(), count()).orderBy()\n",
    "\n",
    "print(\"‚úÖ Ready for your solution!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29205fbb",
   "metadata": {},
   "source": [
    "## 3. Spark Session Recap üöÄ\n",
    "Spark is already initialized. We‚Äôll keep this short and move to SQL analytics.\n",
    "\n",
    "- Session tuned for local development and large CSVs\n",
    "- Temp view `banking_transactions` is ready\n",
    "- Proceed to analytics at scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649b9020",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Optional) Spark utilities\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "print(\"‚ÑπÔ∏è Spark utilities available. Session already created above.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1792d1eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß LIVE CODING: Basic Spark operations\n",
      "üí∞ Amount statistics:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+\n",
      "|summary|       amount_usd|\n",
      "+-------+-----------------+\n",
      "|  count|         13305915|\n",
      "|   mean|42.97603902324682|\n",
      "| stddev|81.65574765375871|\n",
      "|    min|           -500.0|\n",
      "|    max|           6820.2|\n",
      "+-------+-----------------+\n",
      "\n",
      "‚úÖ Features added and temp view created!\n",
      "‚úèÔ∏è EXERCISE: Weekend vs Weekday Analysis\n",
      "TODO: Write SQL query to compare weekend vs weekday spending\n",
      "Hint: Use is_weekend column, COUNT(*), SUM(amount_usd)\n",
      "Hint: GROUP BY is_weekend\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 34:===================================================>     (9 + 1) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------+------------------+--------------------+---------------+\n",
      "|is_weekend|transaction_count|      avg_spending|      total_spending|median_spending|\n",
      "+----------+-----------------+------------------+--------------------+---------------+\n",
      "|      true|          3801414|43.101191701298404|1.6384547354999956E8|          29.06|\n",
      "|     false|          9504501| 42.92598304003574| 4.079900487300027E8|          28.96|\n",
      "+----------+-----------------+------------------+--------------------+---------------+\n",
      "\n",
      "‚úÖ Ready for your SQL solution!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Simple Spark DataFrame operations\n",
    "print(\"üîß LIVE CODING: Basic Spark operations\")\n",
    "\n",
    "# Show basic dataset statistics\n",
    "print(\"üí∞ Amount statistics:\")\n",
    "df.select(\"amount_usd\").describe().show()\n",
    "\n",
    "# Create temp view for SQL\n",
    "df.createOrReplaceTempView(\"transactions\")\n",
    "print(\"‚úÖ Features added and temp view created!\")\n",
    "\n",
    "print(\"‚úèÔ∏è EXERCISE: Weekend vs Weekday Analysis\")\n",
    "print(\"TODO: Write SQL query to compare weekend vs weekday spending\")\n",
    "print(\"Hint: Use is_weekend column, COUNT(*), SUM(amount_usd)\")\n",
    "print(\"Hint: GROUP BY is_weekend\")\n",
    "\n",
    "# Template:\n",
    "spark.sql(\n",
    "    \"\"\"\n",
    "SELECT \n",
    "    is_weekend,\n",
    "    COUNT(*) AS transaction_count,\n",
    "    AVG(amount_usd) AS avg_spending,\n",
    "    SUM(amount_usd) AS total_spending,\n",
    "    MEDIAN(amount_usd) AS median_spending \n",
    "FROM transactions \n",
    "GROUP BY is_weekend\n",
    "\"\"\"\n",
    ").show()\n",
    "\n",
    "print(\"‚úÖ Ready for your SQL solution!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1092b80a",
   "metadata": {},
   "source": [
    "## 4. Advanced Spark SQL Analytics üîç\n",
    "**Goal:** Complex banking analytics using SQL on big data\n",
    "\n",
    "### üè¶ Real Banking Use Cases:\n",
    "- **Fraud Detection:** Unusual spending patterns\n",
    "- **Customer Segmentation:** Spending behavior analysis\n",
    "- **Risk Assessment:** Transaction pattern analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1292fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üßë‚Äçüè´ INSTRUCTOR: Banking analytics (LIVE CODING)\n",
    "print(\"üîç LIVE CODING: Banking Analytics Session\")\n",
    "\n",
    "print(\"üéØ We'll build together:\")\n",
    "print(\"1. Top customers by spending\")\n",
    "print(\"2. Top merchants by transaction volume\")\n",
    "print(\"3. Revenue analysis by merchant\")\n",
    "\n",
    "print(\"‚Ä¢ Complex SQL with aggregate functions\")\n",
    "print(\"‚Ä¢ Window functions and ranking\")\n",
    "print(\"‚Ä¢ Business KPI calculations\")\n",
    "print(\"‚Ä¢ ORDER BY with LIMIT for top-N queries\")\n",
    "\n",
    "# TODO: Live code the customer analysis\n",
    "print(\"\\nüëë COMING UP: Top customers analysis\")\n",
    "\n",
    "# TODO: Live code the merchant analysis  \n",
    "print(\"üè™ COMING UP: Merchant performance analysis\")\n",
    "\n",
    "print(\"‚úÖ Ready for live banking analytics session!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d106fcbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üö® BASIC FRAUD DETECTION\n",
      "üí∞ Transactions above $200:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+-----------+\n",
      "|customer_id|amount_usd|merchant_id|\n",
      "+-----------+----------+-----------+\n",
      "|        708|    6820.2|      34524|\n",
      "|       1081|   6613.44|       9026|\n",
      "|       1259|   5913.37|      85983|\n",
      "|       1487|   5813.78|       9026|\n",
      "|        278|   5696.78|       7202|\n",
      "+-----------+----------+-----------+\n",
      "\n",
      "‚úÖ Basic fraud detection complete!\n",
      "üö® POTENTIAL FRAUD DETECTION:\n",
      "Find customers with transactions > 3 standard deviations from their average\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 47:>                                                         (0 + 8) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------------+--------------------+------------------+\n",
      "| risk_level|transaction_count|        total_amount|        avg_amount|\n",
      "+-----------+-----------------+--------------------+------------------+\n",
      "|  HIGH_RISK|           209928| 5.057713439000009E7|240.92610032963725|\n",
      "|MEDIUM_RISK|           249851|2.8490776459999997E7| 114.0310683567406|\n",
      "|     NORMAL|         12846136|4.9276761142999566E8|  38.3592086702177|\n",
      "+-----------+-----------------+--------------------+------------------+\n",
      "\n",
      "‚úÖ Basic fraud detection complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# üßë‚Äçüéì STUDENT EXERCISE: Fraud Detection\n",
    "print(\"üö® BASIC FRAUD DETECTION\")\n",
    "\n",
    "# High amount transactions (potential fraud) using numeric amounts\n",
    "print(\"üí∞ Transactions above $200:\")\n",
    "spark.sql(\n",
    "    \"\"\"\n",
    "SELECT customer_id, \n",
    "       ROUND(amount_usd, 2) as amount_usd,\n",
    "       merchant_id\n",
    "FROM transactions \n",
    "WHERE amount_usd > 200\n",
    "ORDER BY amount_usd DESC\n",
    "LIMIT 5\n",
    "\"\"\"\n",
    ").show()\n",
    "\n",
    "print(\"‚úÖ Basic fraud detection complete!\")\n",
    "print(\"üö® POTENTIAL FRAUD DETECTION:\")\n",
    "print(\"Find customers with transactions > 3 standard deviations from their average\")\n",
    "\n",
    "fraud_query = \"\"\"\n",
    "WITH customer_stats AS (\n",
    "    SELECT \n",
    "        customer_id,\n",
    "        transaction_date,\n",
    "        amount_usd,\n",
    "        AVG(amount_usd) OVER (PARTITION BY customer_id) as avg_amount,\n",
    "        ABS(STDDEV_POP(amount_usd) OVER (PARTITION BY customer_id)) as stddev_amount\n",
    "    FROM transactions\n",
    "),\n",
    "potential_fraud AS (\n",
    "    SELECT \n",
    "        customer_id,\n",
    "        transaction_date,\n",
    "        amount_usd,\n",
    "        avg_amount,\n",
    "        stddev_amount,\n",
    "        ABS(amount_usd - avg_amount) as deviation,\n",
    "        CASE \n",
    "            WHEN ABS(amount_usd - avg_amount) > 3 * stddev_amount \n",
    "            THEN 'HIGH_RISK'\n",
    "            WHEN ABS(amount_usd - avg_amount) > 2 * stddev_amount \n",
    "            THEN 'MEDIUM_RISK'\n",
    "            ELSE 'NORMAL'\n",
    "        END as risk_level\n",
    "    FROM customer_stats\n",
    ")\n",
    "SELECT \n",
    "    risk_level, \n",
    "    COUNT(*) as transaction_count,\n",
    "    SUM(amount_usd) as total_amount,\n",
    "    AVG(amount_usd) as avg_amount\n",
    "FROM potential_fraud\n",
    "GROUP BY risk_level\n",
    "ORDER BY risk_level\n",
    "\"\"\"\n",
    "spark.sql(fraud_query).show()\n",
    "print(\"‚úÖ Basic fraud detection complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6c301f",
   "metadata": {},
   "source": [
    "## 5. Simple Cloud Deployment üå•Ô∏è\n",
    "**Goal:** Basic overview of deploying to cloud\n",
    "\n",
    "### üåü Why Cloud?\n",
    "- **Scale:** Handle big datasets\n",
    "- **Storage:** Secure data storage  \n",
    "- **Compute:** More processing power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3b21eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üßë‚Äçüè´ INSTRUCTOR: Simple cloud overview (demo only)\n",
    "print(\"‚òÅÔ∏è CLOUD DEPLOYMENT BASICS\")\n",
    "print(\"Key concepts:\")\n",
    "print(\"1. Upload data to cloud storage\")\n",
    "print(\"2. Create compute cluster\") \n",
    "print(\"3. Run Spark jobs\")\n",
    "print(\"4. Store results\")\n",
    "\n",
    "print(\"\\n‚úÖ Cloud overview complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f6ea5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ COMPLETE SOLUTION: Customize GCP deployment\n",
    "print(\"üéØ COMPLETE: Customize the deployment configuration!\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Solution 1: Update configuration for your environment\n",
    "print(\"‚öôÔ∏è CUSTOMIZE YOUR CONFIGURATION:\")\n",
    "my_gcp_config = {\n",
    "    \"project_id\": \"banking-analytics-demo-2025\",  # Example project ID\n",
    "    \"bucket_name\": \"banking-data-workshop-eu\",   # Example bucket name\n",
    "    \"region\": \"europe-west3\",                    # Frankfurt region for GDPR compliance\n",
    "    \"dataset_location\": \"EU\",                    # European Union for data residency\n",
    "    \"service_account_email\": \"banking-analytics@banking-analytics-demo-2025.iam.gserviceaccount.com\",\n",
    "    \"vpc_network\": \"banking-vpc\",                # Custom VPC for security\n",
    "    \"subnet\": \"banking-subnet-eu-west3\"          # Specific subnet\n",
    "}\n",
    "\n",
    "print(\"üìù Your GCP Config:\")\n",
    "import json\n",
    "print(json.dumps(my_gcp_config, indent=2))\n",
    "\n",
    "# Solution 2: Create a deployment checklist\n",
    "print(\"\\n‚úÖ DEPLOYMENT CHECKLIST:\")\n",
    "deployment_checklist = [\n",
    "    \"GCP project created and billing enabled\",\n",
    "    \"Service account created with necessary permissions\",\n",
    "    \"Cloud Storage bucket created in EU region\",\n",
    "    \"Databricks workspace provisioned\"\n",
    "]\n",
    "print(\"\\n‚úÖ Cloud overview complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc056ce1",
   "metadata": {},
   "source": [
    "## 6. Simple Data Integration üîó\n",
    "**Goal:** Combine banking data with external sources\n",
    "\n",
    "### üí° Real Banking Examples:\n",
    "- **Exchange Rates:** Convert international transactions\n",
    "- **Interest Rates:** Economic impact on spending\n",
    "- **Merchant Data:** Enhanced merchant information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd481a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üßë‚Äçüè´ INSTRUCTOR: API Integration (LIVE CODING)\n",
    "print(\"üîó LIVE CODING: External Data Integration\")\n",
    "\n",
    "print(\"üéØ We'll demonstrate:\")\n",
    "print(\"1. Making API calls to financial services\")\n",
    "print(\"2. Handling JSON responses\")\n",
    "print(\"3. Error handling and fallbacks\")\n",
    "print(\"4. Integrating live data into Spark DataFrames\")\n",
    "\n",
    "# Students will learn:\n",
    "import requests\n",
    "\n",
    "print(\"üìä COMING UP: Live exchange rate API integration\")\n",
    "print(\"üí° Key concepts:\")\n",
    "print(\"‚Ä¢ REST API calls with requests library\")\n",
    "print(\"‚Ä¢ JSON data parsing\")\n",
    "print(\"‚Ä¢ Try/except error handling\")\n",
    "print(\"‚Ä¢ DataFrame transformations with live data\")\n",
    "\n",
    "# TODO: Live code the API integration\n",
    "api_url = \"https://api.exchangeratesapi.io/v1/latest?access_key=24da234d4ded987472b5ece3b4981c9b&format=1\"\n",
    "\n",
    "print(\"‚úÖ Ready for live API integration session!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24d8bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üßë‚Äçüéì FINAL EXERCISE: Integration Analysis\n",
    "print(\"üìä FINAL CHALLENGE: Live Exchange Rate Analysis\")\n",
    "\n",
    "print(\"üéØ CAPSTONE EXERCISE:\")\n",
    "print(\"Combine everything you've learned to analyze currency impact!\")\n",
    "\n",
    "print(\"\\n‚úèÔ∏è YOUR MISSION:\")\n",
    "print(\"1. Use the live exchange rate data we just integrated\")\n",
    "print(\"2. Compare USD vs EUR spending by merchant category\") \n",
    "print(\"3. Calculate currency impact percentages\")\n",
    "print(\"4. Generate business insights\")\n",
    "\n",
    "print(\"\\nüí° REQUIRED QUERIES:\")\n",
    "print(\"Query 1: Basic comparison\")\n",
    "print(\"‚Ä¢ SELECT merchant_category, SUM(amount_usd), SUM(amount_eur)\")\n",
    "print(\"‚Ä¢ Calculate totals and averages for both currencies\")\n",
    "print(\"‚Ä¢ GROUP BY merchant_category\")\n",
    "\n",
    "print(\"\\nQuery 2: Advanced analysis\")  \n",
    "print(\"‚Ä¢ Calculate (amount_usd - amount_eur) differences\")\n",
    "print(\"‚Ä¢ Compute percentage impact: (usd-eur)/eur * 100\")\n",
    "print(\"‚Ä¢ Order by currency impact\")\n",
    "\n",
    "print(\"\\nüéØ LEARNING OBJECTIVES:\")\n",
    "print(\"‚Ä¢ Integration of live external data\")\n",
    "print(\"‚Ä¢ Multi-currency financial analysis\") \n",
    "print(\"‚Ä¢ Business insight generation\")\n",
    "print(\"‚Ä¢ Advanced SQL calculations\")\n",
    "\n",
    "# TODO: Students write their analysis queries\n",
    "live_rate_analysis = \"\"\"\n",
    "-- Write your currency comparison query here\n",
    "-- Include: merchant_category, totals, averages, differences\n",
    "\"\"\"\n",
    "\n",
    "currency_impact_query = \"\"\"\n",
    "-- Write your advanced impact analysis here\n",
    "-- Calculate percentage impacts and ranking\n",
    "\"\"\"\n",
    "\n",
    "print(\"‚úÖ Ready for your final analysis challenge!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f38621",
   "metadata": {},
   "source": [
    "## üéØ Workshop Summary\n",
    "\n",
    "You've learned the essentials of big data analytics:\n",
    "\n",
    "1. **PySpark Basics:** Loading and processing data\n",
    "2. **SQL Analysis:** Simple aggregations and insights\n",
    "3. **Banking Analytics:** Basic fraud detection\n",
    "4. **Cloud Concepts:** Deployment overview"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
