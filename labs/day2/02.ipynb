{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 2: Advanced Analytics & Machine Learning\n",
    "\n",
    "**Workshop Schedule:**\n",
    "- 13:00-13:45: Regressionsanalyse\n",
    "- 13:55-14:40: Unstructured Data Analytics  \n",
    "- 14:50-15:40: Data Visualization & Pandas Deep-Dive\n",
    "\n",
    "## Dataset Overview\n",
    "\n",
    "We'll be working with a large transactions dataset containing:\n",
    "- **13M+ transaction records**\n",
    "- **Fields:** id, date, client_id, card_id, amount, use_chip, merchant_id, merchant_city, merchant_state, zip, mcc, errors\n",
    "- **Time Range:** 2010+ banking transactions\n",
    "- **Use Cases:** Fraud detection, customer analytics, risk scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score, classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset (sample for performance)\n",
    "# In production, you'd use Spark for the full 13M records\n",
    "df = pd.read_csv('../data/transactions_data.csv', nrows=100000)\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nColumns: {list(df.columns)}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 📊 13:00-13:45: Regressionsanalyse\n",
    "\n",
    "## 🎯 Lernziele:\n",
    "- Lineare Regression für Customer Lifetime Value\n",
    "- Logistische Regression für Fraud Detection\n",
    "- Credit Risk Scoring\n",
    "- Model Evaluation Metriken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Datenaufbereitung für Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preprocessing\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df['amount_numeric'] = df['amount'].str.replace('$', '').str.replace(',', '').astype(float)\n",
    "df['is_online'] = (df['merchant_city'] == 'ONLINE').astype(int)\n",
    "df['is_weekend'] = df['date'].dt.weekday.isin([5, 6]).astype(int)\n",
    "df['hour'] = df['date'].dt.hour\n",
    "\n",
    "print(\"Preprocessing completed!\")\n",
    "df[['amount', 'amount_numeric', 'is_online', 'is_weekend', 'hour']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Customer Lifetime Value Prediction (Linear Regression)\n",
    "\n",
    "### 📝 **EXERCISE 1: Customer Analytics**\n",
    "\n",
    "Erstellen Sie Features für Customer Lifetime Value Prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customer aggregation for CLV prediction\n",
    "customer_features = df.groupby('client_id').agg({\n",
    "    'amount_numeric': ['sum', 'mean', 'count', 'std'],\n",
    "    'is_online': 'mean',\n",
    "    'is_weekend': 'mean',\n",
    "    'merchant_id': 'nunique',\n",
    "    'date': ['min', 'max']\n",
    "}).round(2)\n",
    "\n",
    "customer_features.columns = ['total_spend', 'avg_transaction', 'transaction_count', 'spend_volatility',\n",
    "                           'online_ratio', 'weekend_ratio', 'merchant_diversity', 'first_transaction', 'last_transaction']\n",
    "\n",
    "customer_features['days_active'] = (customer_features['last_transaction'] - customer_features['first_transaction']).dt.days\n",
    "customer_features['spend_per_day'] = customer_features['total_spend'] / (customer_features['days_active'] + 1)\n",
    "\n",
    "# Remove customers with insufficient data\n",
    "customer_features = customer_features[customer_features['transaction_count'] >= 5]\n",
    "\n",
    "print(f\"Customer features shape: {customer_features.shape}\")\n",
    "customer_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🔍 **YOUR TASK:** \n",
    "Complete the linear regression model to predict customer lifetime value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Complete the linear regression implementation\n",
    "\n",
    "# 1. Define features (X) and target (y)\n",
    "# Features: avg_transaction, transaction_count, merchant_diversity, online_ratio, weekend_ratio\n",
    "# Target: total_spend (as proxy for CLV)\n",
    "\n",
    "X = customer_features[['avg_transaction', 'transaction_count', 'merchant_diversity', 'online_ratio', 'weekend_ratio']]\n",
    "y = customer_features['total_spend']\n",
    "\n",
    "# 2. Split data into train/test\n",
    "# YOUR CODE HERE\n",
    "\n",
    "# 3. Create and train the model\n",
    "# YOUR CODE HERE\n",
    "\n",
    "# 4. Make predictions\n",
    "# YOUR CODE HERE\n",
    "\n",
    "print(\"Linear Regression Model trained!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model evaluation\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Model Performance:\")\n",
    "print(f\"MSE: ${mse:,.2f}\")\n",
    "print(f\"R² Score: {r2:.3f}\")\n",
    "\n",
    "# Feature importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'coefficient': lr_model.coef_\n",
    "}).sort_values('coefficient', key=abs, ascending=False)\n",
    "\n",
    "print(f\"\\nFeature Importance:\")\n",
    "print(feature_importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(y_test, y_pred, alpha=0.6)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "plt.xlabel('Actual CLV')\n",
    "plt.ylabel('Predicted CLV')\n",
    "plt.title(f'CLV Prediction (R² = {r2:.3f})')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "feature_importance.plot(x='feature', y='coefficient', kind='barh', ax=plt.gca())\n",
    "plt.title('Feature Importance')\n",
    "plt.xlabel('Coefficient Value')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Fraud Detection (Logistic Regression)\n",
    "\n",
    "### 📝 **EXERCISE 2: Binary Classification**\n",
    "\n",
    "Create a fraud detection model using suspicious transaction patterns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create fraud indicators (synthetic for demonstration)\n",
    "np.random.seed(42)\n",
    "\n",
    "df['unusual_amount'] = (df['amount_numeric'] > df['amount_numeric'].quantile(0.95)).astype(int)\n",
    "df['night_transaction'] = ((df['hour'] < 6) | (df['hour'] > 22)).astype(int)\n",
    "df['round_amount'] = (df['amount_numeric'] % 10 == 0).astype(int)\n",
    "\n",
    "# Synthetic fraud labels based on risk factors\n",
    "fraud_probability = (\n",
    "    df['unusual_amount'] * 0.3 + \n",
    "    df['night_transaction'] * 0.2 + \n",
    "    df['is_online'] * 0.1 + \n",
    "    df['round_amount'] * 0.1\n",
    ")\n",
    "\n",
    "df['is_fraud'] = (np.random.random(len(df)) < fraud_probability * 0.1).astype(int)\n",
    "\n",
    "print(f\"Fraud rate: {df['is_fraud'].mean():.1%}\")\n",
    "print(f\"Total fraud cases: {df['is_fraud'].sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🔍 **YOUR TASK:** \n",
    "Implement logistic regression for fraud detection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Complete the logistic regression implementation\n",
    "\n",
    "# 1. Prepare features for fraud detection\n",
    "fraud_features = ['amount_numeric', 'is_online', 'is_weekend', 'hour', 'unusual_amount', 'night_transaction', 'round_amount']\n",
    "\n",
    "X_fraud = df[fraud_features]\n",
    "y_fraud = df['is_fraud']\n",
    "\n",
    "# 2. Split the data\n",
    "# YOUR CODE HERE\n",
    "\n",
    "# 3. Scale the features\n",
    "# YOUR CODE HERE\n",
    "\n",
    "# 4. Create and train logistic regression model\n",
    "# YOUR CODE HERE\n",
    "\n",
    "# 5. Make predictions\n",
    "# YOUR CODE HERE\n",
    "\n",
    "print(\"Fraud Detection Model trained!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model evaluation with detailed metrics\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, roc_curve\n",
    "\n",
    "precision = precision_score(y_test_fraud, y_pred_fraud)\n",
    "recall = recall_score(y_test_fraud, y_pred_fraud)\n",
    "f1 = f1_score(y_test_fraud, y_pred_fraud)\n",
    "auc = roc_auc_score(y_test_fraud, y_pred_proba)\n",
    "\n",
    "print(\"Fraud Detection Performance:\")\n",
    "print(f\"Precision: {precision:.3f}\")\n",
    "print(f\"Recall: {recall:.3f}\")\n",
    "print(f\"F1-Score: {f1:.3f}\")\n",
    "print(f\"AUC-ROC: {auc:.3f}\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_fraud, y_pred_fraud))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizations\n",
    "plt.figure(figsize=(15, 4))\n",
    "\n",
    "# Confusion Matrix\n",
    "plt.subplot(1, 3, 1)\n",
    "cm = confusion_matrix(y_test_fraud, y_pred_fraud)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "\n",
    "# ROC Curve\n",
    "plt.subplot(1, 3, 2)\n",
    "fpr, tpr, _ = roc_curve(y_test_fraud, y_pred_proba)\n",
    "plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {auc:.3f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend()\n",
    "\n",
    "# Feature Importance\n",
    "plt.subplot(1, 3, 3)\n",
    "feature_importance_fraud = pd.DataFrame({\n",
    "    'feature': fraud_features,\n",
    "    'importance': abs(log_model.coef_[0])\n",
    "}).sort_values('importance', ascending=True)\n",
    "\n",
    "plt.barh(feature_importance_fraud['feature'], feature_importance_fraud['importance'])\n",
    "plt.title('Feature Importance (Fraud Detection)')\n",
    "plt.xlabel('Absolute Coefficient')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 📄 13:55-14:40: Unstructured Data Analytics\n",
    "\n",
    "## 🎯 Lernziele:\n",
    "- Text Data Processing für Banking\n",
    "- Transaction Description Mining\n",
    "- Sentiment Analysis\n",
    "- Text-based Fraud Indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text processing libraries\n",
    "import re\n",
    "from collections import Counter\n",
    "try:\n",
    "    from wordcloud import WordCloud\n",
    "except ImportError:\n",
    "    print(\"WordCloud not available - install with: pip install wordcloud\")\n",
    "\n",
    "try:\n",
    "    import nltk\n",
    "    # Download NLTK data (run once)\n",
    "    try:\n",
    "        nltk.data.find('tokenizers/punkt')\n",
    "    except LookupError:\n",
    "        nltk.download('punkt')\n",
    "        nltk.download('stopwords')\n",
    "        nltk.download('vader_lexicon')\n",
    "\n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "except ImportError:\n",
    "    print(\"NLTK not available - install with: pip install nltk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Synthetic Transaction Descriptions\n",
    "\n",
    "Create realistic transaction descriptions for text analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic transaction descriptions\n",
    "np.random.seed(42)\n",
    "\n",
    "merchant_types = {\n",
    "    5411: ['GROCERY STORE', 'SUPERMARKET', 'FOOD MART'],\n",
    "    5812: ['RESTAURANT', 'FAST FOOD', 'CAFE', 'DINER'],\n",
    "    4121: ['TAXI SERVICE', 'UBER', 'LYFT', 'CAB COMPANY'],\n",
    "    5541: ['GAS STATION', 'FUEL STOP', 'PETROL'],\n",
    "    5942: ['BOOKSTORE', 'LIBRARY', 'READING CORNER'],\n",
    "    5499: ['CONVENIENCE STORE', 'CORNER SHOP', '24/7 MART'],\n",
    "    7801: ['ONLINE PAYMENT', 'DIGITAL TRANSACTION', 'WEB PURCHASE'],\n",
    "    4784: ['ATM WITHDRAWAL', 'CASH ADVANCE', 'ATM TRANSACTION']\n",
    "}\n",
    "\n",
    "def generate_description(row):\n",
    "    mcc = row.get('mcc', 5499)\n",
    "    amount = row['amount_numeric']\n",
    "    \n",
    "    if mcc in merchant_types:\n",
    "        base_desc = np.random.choice(merchant_types[mcc])\n",
    "    else:\n",
    "        base_desc = \"MERCHANT TRANSACTION\"\n",
    "    \n",
    "    # Add suspicious patterns for some transactions\n",
    "    if np.random.random() < 0.05:  # 5% suspicious\n",
    "        suspicious_words = ['URGENT', 'IMMEDIATE', 'FINAL NOTICE', 'VERIFY ACCOUNT', 'SECURITY ALERT']\n",
    "        base_desc += ' ' + np.random.choice(suspicious_words)\n",
    "    \n",
    "    return f\"{base_desc} ${amount:.2f}\"\n",
    "\n",
    "# Generate descriptions for sample data\n",
    "sample_df = df.sample(10000, random_state=42).copy()\n",
    "sample_df['description'] = sample_df.apply(generate_description, axis=1)\n",
    "\n",
    "print(\"Sample transaction descriptions:\")\n",
    "print(sample_df[['amount', 'description', 'is_fraud']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Text Data Processing\n",
    "\n",
    "### 📝 **EXERCISE 4: Transaction Description Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text preprocessing function\n",
    "def preprocess_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove special characters and digits\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    # Remove extra whitespace\n",
    "    text = ' '.join(text.split())\n",
    "    return text\n",
    "\n",
    "sample_df['description_clean'] = sample_df['description'].apply(preprocess_text)\n",
    "\n",
    "print(\"Text preprocessing examples:\")\n",
    "print(sample_df[['description', 'description_clean']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🔍 **YOUR TASK:** \n",
    "Implement text mining for transaction categories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Complete text mining analysis\n",
    "\n",
    "# 1. Extract most common words\n",
    "# Define stop words (use simple list if NLTK not available)\n",
    "try:\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "except:\n",
    "    stop_words = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'is', 'are', 'was', 'were'}\n",
    "\n",
    "all_words = []\n",
    "for desc in sample_df['description_clean']:\n",
    "    words = desc.split()\n",
    "    # YOUR CODE HERE: Filter out stop words and short words\n",
    "    filtered_words = [word for word in words if word not in stop_words and len(word) > 2]\n",
    "    all_words.extend(filtered_words)\n",
    "\n",
    "# 2. Count word frequencies\n",
    "# YOUR CODE HERE\n",
    "word_freq = Counter(all_words)\n",
    "top_words = word_freq.most_common(20)\n",
    "\n",
    "print(\"Top 20 words in transaction descriptions:\")\n",
    "for word, count in top_words:\n",
    "    print(f\"{word}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word cloud visualization (if available)\n",
    "try:\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(word_freq)\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title('Transaction Description Word Cloud')\n",
    "    plt.show()\n",
    "except:\n",
    "    # Alternative visualization with bar chart\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    words, counts = zip(*top_words[:15])\n",
    "    plt.barh(words, counts)\n",
    "    plt.title('Top 15 Words in Transaction Descriptions')\n",
    "    plt.xlabel('Frequency')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Sentiment Analysis für Banking Communications\n",
    "\n",
    "### 📝 **EXERCISE 5: Sentiment-based Risk Assessment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple sentiment analysis (if NLTK VADER not available)\n",
    "def simple_sentiment_score(text):\n",
    "    positive_words = ['good', 'great', 'excellent', 'amazing', 'wonderful', 'fantastic', 'love', 'like']\n",
    "    negative_words = ['bad', 'terrible', 'awful', 'hate', 'dislike', 'horrible', 'urgent', 'alert', 'warning', 'security']\n",
    "    \n",
    "    words = text.lower().split()\n",
    "    pos_count = sum(1 for word in words if word in positive_words)\n",
    "    neg_count = sum(1 for word in words if word in negative_words)\n",
    "    \n",
    "    if pos_count + neg_count == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    return (pos_count - neg_count) / (pos_count + neg_count)\n",
    "\n",
    "try:\n",
    "    # Use NLTK VADER if available\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    \n",
    "    def get_sentiment_scores(text):\n",
    "        scores = sia.polarity_scores(text)\n",
    "        return scores\n",
    "    \n",
    "    sentiment_scores = sample_df['description'].apply(get_sentiment_scores)\n",
    "    sample_df['sentiment_compound'] = [score['compound'] for score in sentiment_scores]\n",
    "    sample_df['sentiment_positive'] = [score['pos'] for score in sentiment_scores]\n",
    "    sample_df['sentiment_negative'] = [score['neg'] for score in sentiment_scores]\n",
    "    sample_df['sentiment_neutral'] = [score['neu'] for score in sentiment_scores]\n",
    "    \n",
    "except:\n",
    "    # Use simple sentiment if NLTK not available\n",
    "    sample_df['sentiment_compound'] = sample_df['description'].apply(simple_sentiment_score)\n",
    "    sample_df['sentiment_positive'] = (sample_df['sentiment_compound'] > 0).astype(float)\n",
    "    sample_df['sentiment_negative'] = (sample_df['sentiment_compound'] < 0).astype(float)\n",
    "    sample_df['sentiment_neutral'] = (sample_df['sentiment_compound'] == 0).astype(float)\n",
    "\n",
    "print(\"Sentiment analysis results:\")\n",
    "print(sample_df[['description', 'sentiment_compound', 'is_fraud']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🔍 **YOUR TASK:**\n",
    "Analyze correlation between sentiment and fraud:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Analyze sentiment patterns in fraud vs legitimate transactions\n",
    "\n",
    "# 1. Group by fraud status and analyze sentiment\n",
    "# YOUR CODE HERE\n",
    "sentiment_by_fraud = sample_df.groupby('is_fraud')[['sentiment_compound', 'sentiment_positive', 'sentiment_negative']].mean()\n",
    "\n",
    "print(\"Sentiment Analysis by Fraud Status:\")\n",
    "print(sentiment_by_fraud)\n",
    "\n",
    "# 2. Statistical test for sentiment differences\n",
    "from scipy import stats\n",
    "\n",
    "fraud_sentiment = sample_df[sample_df['is_fraud'] == 1]['sentiment_compound']\n",
    "legit_sentiment = sample_df[sample_df['is_fraud'] == 0]['sentiment_compound']\n",
    "\n",
    "# YOUR CODE HERE: Perform t-test\n",
    "if len(fraud_sentiment) > 0 and len(legit_sentiment) > 0:\n",
    "    t_stat, p_value = stats.ttest_ind(fraud_sentiment, legit_sentiment)\n",
    "    \n",
    "    print(f\"\\nT-test results:\")\n",
    "    print(f\"T-statistic: {t_stat:.4f}\")\n",
    "    print(f\"P-value: {p_value:.4f}\")\n",
    "    print(f\"Significant difference: {'Yes' if p_value < 0.05 else 'No'}\")\n",
    "else:\n",
    "    print(\"\\nInsufficient data for statistical test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 📊 14:50-15:40: Data Visualization & Pandas Deep-Dive\n",
    "\n",
    "## 🎯 Lernziele:\n",
    "- Pandas Advanced Techniques\n",
    "- Time Series Analysis\n",
    "- Statistical Analysis Methods\n",
    "- Interactive Banking KPI Dashboards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Time Series Analysis\n",
    "\n",
    "### 📝 **EXERCISE 6: Banking Time Series Analytics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time series preparation\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df['year'] = df['date'].dt.year\n",
    "df['month'] = df['date'].dt.month\n",
    "df['day'] = df['date'].dt.day\n",
    "df['weekday'] = df['date'].dt.day_name()\n",
    "df['hour'] = df['date'].dt.hour\n",
    "\n",
    "# Set date as index for time series operations\n",
    "df_ts = df.set_index('date').copy()\n",
    "\n",
    "print(f\"Time series data prepared: {df_ts.index.min()} to {df_ts.index.max()}\")\n",
    "print(f\"Total time span: {(df_ts.index.max() - df_ts.index.min()).days} days\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🔍 **YOUR TASK:**\n",
    "Implement comprehensive time series analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Complete time series analysis\n",
    "\n",
    "# 1. Daily transaction patterns\n",
    "# YOUR CODE HERE\n",
    "daily_metrics = df_ts.groupby(df_ts.index.date).agg({\n",
    "    'amount_numeric': ['sum', 'mean', 'count'],\n",
    "    'is_fraud': 'sum',\n",
    "    'is_online': 'sum'\n",
    "})\n",
    "\n",
    "daily_metrics.columns = ['daily_volume', 'avg_transaction', 'transaction_count', 'fraud_count', 'online_count']\n",
    "daily_metrics['fraud_rate'] = daily_metrics['fraud_count'] / daily_metrics['transaction_count'] * 100\n",
    "daily_metrics['online_rate'] = daily_metrics['online_count'] / daily_metrics['transaction_count'] * 100\n",
    "\n",
    "print(\"Daily metrics calculated:\")\n",
    "print(daily_metrics.head())\n",
    "\n",
    "# 2. Weekly patterns\n",
    "# YOUR CODE HERE\n",
    "weekly_patterns = df.groupby('weekday').agg({\n",
    "    'amount_numeric': ['mean', 'count'],\n",
    "    'is_fraud': 'mean',\n",
    "    'is_online': 'mean'\n",
    "}).round(3)\n",
    "\n",
    "weekly_patterns.columns = ['avg_amount', 'transaction_count', 'fraud_rate', 'online_rate']\n",
    "print(\"\\nWeekly patterns:\")\n",
    "print(weekly_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced time series visualizations\n",
    "plt.figure(figsize=(15, 12))\n",
    "\n",
    "# Daily transaction volume\n",
    "plt.subplot(3, 2, 1)\n",
    "daily_metrics['daily_volume'].plot()\n",
    "plt.title('Daily Transaction Volume')\n",
    "plt.ylabel('Total Volume ($)')\n",
    "\n",
    "# Daily fraud rate\n",
    "plt.subplot(3, 2, 2)\n",
    "daily_metrics['fraud_rate'].plot(color='red')\n",
    "plt.title('Daily Fraud Rate')\n",
    "plt.ylabel('Fraud Rate (%)')\n",
    "\n",
    "# Hourly patterns\n",
    "plt.subplot(3, 2, 3)\n",
    "hourly_patterns = df.groupby('hour')['amount_numeric'].sum()\n",
    "hourly_patterns.plot(kind='bar')\n",
    "plt.title('Transaction Volume by Hour')\n",
    "plt.xlabel('Hour of Day')\n",
    "plt.ylabel('Volume ($)')\n",
    "\n",
    "# Weekly patterns\n",
    "plt.subplot(3, 2, 4)\n",
    "days_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "weekly_ordered = weekly_patterns.reindex(days_order)\n",
    "weekly_ordered['avg_amount'].plot(kind='bar', color='green')\n",
    "plt.title('Average Transaction Amount by Day')\n",
    "plt.xlabel('Day of Week')\n",
    "plt.ylabel('Average Amount ($)')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Moving averages\n",
    "plt.subplot(3, 2, 5)\n",
    "daily_metrics['transaction_count'].rolling(window=7).mean().plot(label='7-day MA')\n",
    "daily_metrics['transaction_count'].rolling(window=30).mean().plot(label='30-day MA')\n",
    "plt.title('Transaction Count Moving Averages')\n",
    "plt.ylabel('Transaction Count')\n",
    "plt.legend()\n",
    "\n",
    "# Correlation heatmap\n",
    "plt.subplot(3, 2, 6)\n",
    "correlation_matrix = daily_metrics[['daily_volume', 'avg_transaction', 'fraud_rate', 'online_rate']].corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0)\n",
    "plt.title('Daily Metrics Correlation')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Banking KPI Dashboard\n",
    "\n",
    "### 📝 **EXERCISE 7: Comprehensive KPI Visualization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate key banking KPIs\n",
    "def calculate_banking_kpis(df):\n",
    "    kpis = {}\n",
    "    \n",
    "    # Volume metrics\n",
    "    kpis['total_volume'] = df['amount_numeric'].sum()\n",
    "    kpis['total_transactions'] = len(df)\n",
    "    kpis['avg_transaction_size'] = df['amount_numeric'].mean()\n",
    "    \n",
    "    # Customer metrics\n",
    "    kpis['active_customers'] = df['client_id'].nunique()\n",
    "    kpis['avg_transactions_per_customer'] = len(df) / df['client_id'].nunique()\n",
    "    kpis['avg_customer_value'] = df.groupby('client_id')['amount_numeric'].sum().mean()\n",
    "    \n",
    "    # Risk metrics\n",
    "    kpis['fraud_rate'] = df['is_fraud'].mean() * 100\n",
    "    kpis['fraud_volume'] = df[df['is_fraud'] == 1]['amount_numeric'].sum()\n",
    "    kpis['high_value_transactions'] = (df['amount_numeric'] > df['amount_numeric'].quantile(0.95)).mean() * 100\n",
    "    \n",
    "    # Channel metrics\n",
    "    kpis['online_percentage'] = df['is_online'].mean() * 100\n",
    "    kpis['weekend_percentage'] = df['is_weekend'].mean() * 100\n",
    "    \n",
    "    return kpis\n",
    "\n",
    "# Calculate current KPIs\n",
    "current_kpis = calculate_banking_kpis(df)\n",
    "\n",
    "print(\"Banking KPI Dashboard\")\n",
    "print(\"=\" * 50)\n",
    "for kpi, value in current_kpis.items():\n",
    "    if 'rate' in kpi or 'percentage' in kpi:\n",
    "        print(f\"{kpi.replace('_', ' ').title()}: {value:.2f}%\")\n",
    "    elif 'volume' in kpi or 'value' in kpi or 'size' in kpi:\n",
    "        print(f\"{kpi.replace('_', ' ').title()}: ${value:,.2f}\")\n",
    "    else:\n",
    "        print(f\"{kpi.replace('_', ' ').title()}: {value:,.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🔍 **YOUR TASK:**\n",
    "Create a comprehensive KPI dashboard with visualizations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create comprehensive KPI dashboard\n",
    "\n",
    "# 1. Create figure with multiple subplots\n",
    "# YOUR CODE HERE\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "fig.suptitle('Banking KPI Dashboard', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 2. Daily volume trend\n",
    "axes[0, 0].plot(daily_metrics.index, daily_metrics['daily_volume'], linewidth=2)\n",
    "axes[0, 0].set_title('Daily Transaction Volume')\n",
    "axes[0, 0].set_ylabel('Volume ($)')\n",
    "axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 3. Fraud rate by day of week\n",
    "fraud_by_day = df.groupby('weekday')['is_fraud'].mean() * 100\n",
    "fraud_by_day_ordered = fraud_by_day.reindex(days_order)\n",
    "axes[0, 1].bar(range(len(fraud_by_day_ordered)), fraud_by_day_ordered.values, color='red', alpha=0.7)\n",
    "axes[0, 1].set_title('Fraud Rate by Day of Week')\n",
    "axes[0, 1].set_ylabel('Fraud Rate (%)')\n",
    "axes[0, 1].set_xticks(range(len(days_order)))\n",
    "axes[0, 1].set_xticklabels([day[:3] for day in days_order])\n",
    "\n",
    "# 4. Transaction volume by hour\n",
    "hourly_volume = df.groupby('hour')['amount_numeric'].sum()\n",
    "axes[0, 2].bar(hourly_volume.index, hourly_volume.values, color='blue', alpha=0.7)\n",
    "axes[0, 2].set_title('Transaction Volume by Hour')\n",
    "axes[0, 2].set_xlabel('Hour of Day')\n",
    "axes[0, 2].set_ylabel('Volume ($)')\n",
    "\n",
    "# 5. Online vs Offline transactions\n",
    "channel_data = df['is_online'].value_counts()\n",
    "axes[1, 0].pie(channel_data.values, labels=['Offline', 'Online'], autopct='%1.1f%%', colors=['lightblue', 'orange'])\n",
    "axes[1, 0].set_title('Transaction Channels')\n",
    "\n",
    "# 6. Top states by volume\n",
    "top_states_volume = df.groupby('merchant_state')['amount_numeric'].sum().nlargest(10)\n",
    "axes[1, 1].barh(range(len(top_states_volume)), top_states_volume.values)\n",
    "axes[1, 1].set_title('Top 10 States by Volume')\n",
    "axes[1, 1].set_yticks(range(len(top_states_volume)))\n",
    "axes[1, 1].set_yticklabels(top_states_volume.index)\n",
    "axes[1, 1].set_xlabel('Volume ($)')\n",
    "\n",
    "# 7. Amount distribution\n",
    "axes[1, 2].hist(df['amount_numeric'], bins=50, alpha=0.7, color='green')\n",
    "axes[1, 2].set_title('Transaction Amount Distribution')\n",
    "axes[1, 2].set_xlabel('Amount ($)')\n",
    "axes[1, 2].set_ylabel('Frequency')\n",
    "axes[1, 2].set_yscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 🎯 Workshop Summary & Next Steps\n",
    "\n",
    "## ✅ What We Accomplished Today:\n",
    "\n",
    "### 1. **Regression Analysis (13:00-13:45)**\n",
    "- ✅ Customer Lifetime Value Prediction with Linear Regression\n",
    "- ✅ Fraud Detection using Logistic Regression\n",
    "- ✅ Model Evaluation: Precision, Recall, F1-Score, ROC-AUC\n",
    "\n",
    "### 2. **Unstructured Data Analytics (13:55-14:40)**\n",
    "- ✅ Transaction Description Text Mining\n",
    "- ✅ Sentiment Analysis for Risk Assessment\n",
    "- ✅ Text-based Feature Engineering\n",
    "\n",
    "### 3. **Data Visualization & Pandas Deep-Dive (14:50-15:40)**\n",
    "- ✅ Advanced Time Series Analysis\n",
    "- ✅ Banking KPI Dashboard Creation\n",
    "- ✅ Statistical Analysis and Correlation Studies\n",
    "\n",
    "## 🚀 **Key Skills Developed:**\n",
    "1. **Machine Learning:** Linear/Logistic Regression, Classification metrics\n",
    "2. **Text Analytics:** Text mining, Sentiment Analysis, Feature Engineering\n",
    "3. **Data Visualization:** Advanced Matplotlib/Seaborn, Dashboard Design\n",
    "4. **Pandas Mastery:** GroupBy, Time Series, Statistical Functions\n",
    "5. **Banking Domain:** Risk Assessment, Fraud Detection, Customer Analytics\n",
    "\n",
    "## 🎓 **Homework Challenges:**\n",
    "1. Complete all the TODO sections in the exercises\n",
    "2. Extend the fraud detection model with additional features\n",
    "3. Create a time-series forecasting model for transaction volumes\n",
    "4. Implement clustering algorithms for customer segmentation\n",
    "5. Build a Streamlit dashboard for interactive analytics\n",
    "\n",
    "## 📚 **Additional Resources:**\n",
    "- **Documentation:** pandas.pydata.org, scikit-learn.org\n",
    "- **Books:** \"Python for Data Analysis\" by Wes McKinney\n",
    "- **Practice:** Kaggle competitions on financial data\n",
    "- **Tools:** Apache Spark for big data processing\n",
    "\n",
    "**Great work today! You've built a comprehensive analytics pipeline for banking data! 🎉**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
