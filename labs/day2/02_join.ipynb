{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "076ea279",
   "metadata": {},
   "source": [
    "# PySpark Data Joins and DataFrame Conversions\n",
    "\n",
    "## Learning Goals:\n",
    "- Load and join multiple datasets with Spark\n",
    "- Understand different join types and strategies\n",
    "- Convert between pandas and Spark DataFrames\n",
    "- Work with JSON and CSV data formats\n",
    "- Create comprehensive joined datasets for analysis\n",
    "\n",
    "## Available Datasets:\n",
    "- `transactions_data.csv` - Transaction records\n",
    "- `cards_data.csv` - Card information \n",
    "- `exchange_rates.csv` - Currency exchange rates\n",
    "- `mcc.json` - Merchant category codes\n",
    "- `train_fraud_labels.json` - Fraud labels for training\n",
    "\n",
    "Let's explore and join all these datasets!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7c6464e6",
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[Errno 61] Connection refused",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 20\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Initialize Spark session with optimized settings for joins\u001b[39;00m\n\u001b[1;32m     12\u001b[0m spark \u001b[38;5;241m=\u001b[39m \u001b[43mSparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuilder\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappName\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mDataJoinsAndConversions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmaster\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlocal[4]\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.sql.adaptive.enabled\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrue\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.sql.adaptive.coalescePartitions.enabled\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrue\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.sql.adaptive.skewJoin.enabled\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrue\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.driver.bindAddress\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m127.0.0.1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.sql.shuffle.partitions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m200\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m---> 20\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m spark\u001b[38;5;241m.\u001b[39msparkContext\u001b[38;5;241m.\u001b[39msetLogLevel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWARN\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mâœ“ Spark \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mspark\u001b[38;5;241m.\u001b[39mversion\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m initialized with join optimizations\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/Coding/big-data-analytics/.venv/lib/python3.12/site-packages/pyspark/sql/session.py:493\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    491\u001b[0m session \u001b[38;5;241m=\u001b[39m SparkSession\u001b[38;5;241m.\u001b[39m_instantiatedSession\n\u001b[1;32m    492\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m session \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m session\u001b[38;5;241m.\u001b[39m_sc\u001b[38;5;241m.\u001b[39m_jsc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 493\u001b[0m     sparkConf \u001b[38;5;241m=\u001b[39m \u001b[43mSparkConf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    494\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    495\u001b[0m         sparkConf\u001b[38;5;241m.\u001b[39mset(key, value)\n",
      "File \u001b[0;32m~/Documents/Coding/big-data-analytics/.venv/lib/python3.12/site-packages/pyspark/conf.py:132\u001b[0m, in \u001b[0;36mSparkConf.__init__\u001b[0;34m(self, loadDefaults, _jvm, _jconf)\u001b[0m\n\u001b[1;32m    128\u001b[0m _jvm \u001b[38;5;241m=\u001b[39m _jvm \u001b[38;5;129;01mor\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_jvm\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;66;03m# JVM is created, so create self._jconf directly through JVM\u001b[39;00m\n\u001b[0;32m--> 132\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jconf \u001b[38;5;241m=\u001b[39m \u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSparkConf\u001b[49m(loadDefaults)\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;66;03m# JVM is not created, so store data in self._conf first\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Coding/big-data-analytics/.venv/lib/python3.12/site-packages/py4j/java_gateway.py:1712\u001b[0m, in \u001b[0;36mJVMView.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1709\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;241m==\u001b[39m UserHelpAutoCompletion\u001b[38;5;241m.\u001b[39mKEY:\n\u001b[1;32m   1710\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m UserHelpAutoCompletion()\n\u001b[0;32m-> 1712\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1713\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproto\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mREFLECTION_COMMAND_NAME\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\n\u001b[1;32m   1714\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproto\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mREFL_GET_UNKNOWN_SUB_COMMAND_NAME\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_id\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\n\u001b[1;32m   1715\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mproto\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEND_COMMAND_PART\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1716\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer \u001b[38;5;241m==\u001b[39m proto\u001b[38;5;241m.\u001b[39mSUCCESS_PACKAGE:\n\u001b[1;32m   1717\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m JavaPackage(name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gateway_client, jvm_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_id)\n",
      "File \u001b[0;32m~/Documents/Coding/big-data-analytics/.venv/lib/python3.12/site-packages/py4j/java_gateway.py:1036\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msend_command\u001b[39m(\u001b[38;5;28mself\u001b[39m, command, retry\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, binary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Sends a command to the JVM. This method is not intended to be\u001b[39;00m\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;124;03m       called directly by Py4J users. It is usually called by\u001b[39;00m\n\u001b[1;32m   1018\u001b[0m \u001b[38;5;124;03m       :class:`JavaMember` instances.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;124;03m     if `binary` is `True`.\u001b[39;00m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1036\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1037\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1038\u001b[0m         response \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39msend_command(command)\n",
      "File \u001b[0;32m~/Documents/Coding/big-data-analytics/.venv/lib/python3.12/site-packages/py4j/clientserver.py:284\u001b[0m, in \u001b[0;36mJavaClient._get_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m connection\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 284\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_new_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m~/Documents/Coding/big-data-analytics/.venv/lib/python3.12/site-packages/py4j/clientserver.py:291\u001b[0m, in \u001b[0;36mJavaClient._create_new_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_create_new_connection\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    288\u001b[0m     connection \u001b[38;5;241m=\u001b[39m ClientServerConnection(\n\u001b[1;32m    289\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_parameters, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_parameters,\n\u001b[1;32m    290\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_property, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 291\u001b[0m     \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect_to_java_server\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    292\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_thread_connection(connection)\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m~/Documents/Coding/big-data-analytics/.venv/lib/python3.12/site-packages/py4j/clientserver.py:438\u001b[0m, in \u001b[0;36mClientServerConnection.connect_to_java_server\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context:\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context\u001b[38;5;241m.\u001b[39mwrap_socket(\n\u001b[1;32m    437\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket, server_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_address)\n\u001b[0;32m--> 438\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_address\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_port\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket\u001b[38;5;241m.\u001b[39mmakefile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_connected \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 61] Connection refused"
     ]
    }
   ],
   "source": [
    "# Essential imports for data joining and conversions\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "# Initialize Spark session with optimized settings for joins\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DataJoinsAndConversions\") \\\n",
    "    .master(\"local[4]\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.skewJoin.enabled\", \"true\") \\\n",
    "    .config(\"spark.driver.bindAddress\", \"127.0.0.1\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "print(f\"âœ“ Spark {spark.version} initialized with join optimizations\")\n",
    "print(f\"âœ“ Adaptive Query Execution enabled for optimal join performance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee31542",
   "metadata": {},
   "source": [
    "## 1. Loading All Available Datasets\n",
    "\n",
    "Let's load each dataset and explore their structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad772fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading CSV datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Transactions: 13,305,915 rows, 12 columns\n",
      "âœ“ Cards: 6,146 rows, 13 columns\n",
      "âœ“ Cards: 6,146 rows, 13 columns\n",
      "âœ“ Exchange rates: 12,175 rows, 27 columns\n",
      "\n",
      "Dataset schemas:\n",
      "==================================================\n",
      "Transactions schema:\n",
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- date: timestamp (nullable = true)\n",
      " |-- client_id: integer (nullable = true)\n",
      " |-- card_id: integer (nullable = true)\n",
      " |-- amount: string (nullable = true)\n",
      " |-- use_chip: string (nullable = true)\n",
      " |-- merchant_id: integer (nullable = true)\n",
      " |-- merchant_city: string (nullable = true)\n",
      " |-- merchant_state: string (nullable = true)\n",
      " |-- zip: double (nullable = true)\n",
      " |-- mcc: integer (nullable = true)\n",
      " |-- errors: string (nullable = true)\n",
      "\n",
      "\n",
      "Cards schema:\n",
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- client_id: integer (nullable = true)\n",
      " |-- card_brand: string (nullable = true)\n",
      " |-- card_type: string (nullable = true)\n",
      " |-- card_number: long (nullable = true)\n",
      " |-- expires: string (nullable = true)\n",
      " |-- cvv: integer (nullable = true)\n",
      " |-- has_chip: string (nullable = true)\n",
      " |-- num_cards_issued: integer (nullable = true)\n",
      " |-- credit_limit: string (nullable = true)\n",
      " |-- acct_open_date: string (nullable = true)\n",
      " |-- year_pin_last_changed: integer (nullable = true)\n",
      " |-- card_on_dark_web: string (nullable = true)\n",
      "\n",
      "\n",
      "Exchange rates schema:\n",
      "root\n",
      " |-- Series Description: string (nullable = true)\n",
      " |-- SPOT EXCHANGE RATE - EURO AREA : string (nullable = true)\n",
      " |-- UNITED KINGDOM -- SPOT EXCHANGE RATE, US$/POUND (1/RXI_N.B.UK): string (nullable = true)\n",
      " |-- SPOT EXCHANGE RATE - BRAZIL : string (nullable = true)\n",
      " |-- CHINA -- SPOT EXCHANGE RATE, YUAN/US$ P.R. : string (nullable = true)\n",
      " |-- DENMARK -- SPOT EXCHANGE RATE, KRONER/US$ : string (nullable = true)\n",
      " |-- INDIA -- SPOT EXCHANGE RATE, RUPEES/US$ : string (nullable = true)\n",
      " |-- JAPAN -- SPOT EXCHANGE RATE, YEN/US$ : string (nullable = true)\n",
      " |-- KOREA -- SPOT EXCHANGE RATE, WON/US$ : string (nullable = true)\n",
      " |-- MALAYSIA -- SPOT EXCHANGE RATE, RINGGIT/US$ : string (nullable = true)\n",
      " |-- MEXICO -- SPOT EXCHANGE RATE, PESOS/US$ : string (nullable = true)\n",
      " |-- NORWAY -- SPOT EXCHANGE RATE, KRONER/US$ : string (nullable = true)\n",
      " |-- SWEDEN -- SPOT EXCHANGE RATE, KRONOR/US$ : string (nullable = true)\n",
      " |-- SOUTH AFRICA -- SPOT EXCHANGE RATE, RAND/$US: string (nullable = true)\n",
      " |-- SINGAPORE -- SPOT EXCHANGE RATE, SINGAPORE $/US$ : string (nullable = true)\n",
      " |-- SWITZERLAND -- SPOT EXCHANGE RATE, FRANCS/US$ : string (nullable = true)\n",
      " |-- TAIWAN -- SPOT EXCHANGE RATE, NT$/US$ : string (nullable = true)\n",
      " |-- THAILAND -- SPOT EXCHANGE RATE, BAHT/US$ : string (nullable = true)\n",
      " |-- SPOT EXCHANGE RATE - VENEZUELA : string (nullable = true)\n",
      " |-- Nominal Broad Dollar Index : string (nullable = true)\n",
      " |-- Nominal Major Currencies Dollar Index : string (nullable = true)\n",
      " |-- Nominal Other Important Trading Partners Dollar Index : string (nullable = true)\n",
      " |-- AUSTRALIA -- SPOT EXCHANGE RATE US$/AU$ (RECIPROCAL OF RXI_N.B.AL): string (nullable = true)\n",
      " |-- NEW ZEALAND -- SPOT EXCHANGE RATE, US$/NZ$ RECIPROCAL OF RXI_N.B.NZ : string (nullable = true)\n",
      " |-- CANADA -- SPOT EXCHANGE RATE, CANADIAN $/US$ : string (nullable = true)\n",
      " |-- HONG KONG -- SPOT EXCHANGE RATE, HK$/US$ : string (nullable = true)\n",
      " |-- SRI LANKA -- SPOT EXCHANGE RATE, RUPEES/US$ : string (nullable = true)\n",
      "\n",
      "âœ“ Exchange rates: 12,175 rows, 27 columns\n",
      "\n",
      "Dataset schemas:\n",
      "==================================================\n",
      "Transactions schema:\n",
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- date: timestamp (nullable = true)\n",
      " |-- client_id: integer (nullable = true)\n",
      " |-- card_id: integer (nullable = true)\n",
      " |-- amount: string (nullable = true)\n",
      " |-- use_chip: string (nullable = true)\n",
      " |-- merchant_id: integer (nullable = true)\n",
      " |-- merchant_city: string (nullable = true)\n",
      " |-- merchant_state: string (nullable = true)\n",
      " |-- zip: double (nullable = true)\n",
      " |-- mcc: integer (nullable = true)\n",
      " |-- errors: string (nullable = true)\n",
      "\n",
      "\n",
      "Cards schema:\n",
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- client_id: integer (nullable = true)\n",
      " |-- card_brand: string (nullable = true)\n",
      " |-- card_type: string (nullable = true)\n",
      " |-- card_number: long (nullable = true)\n",
      " |-- expires: string (nullable = true)\n",
      " |-- cvv: integer (nullable = true)\n",
      " |-- has_chip: string (nullable = true)\n",
      " |-- num_cards_issued: integer (nullable = true)\n",
      " |-- credit_limit: string (nullable = true)\n",
      " |-- acct_open_date: string (nullable = true)\n",
      " |-- year_pin_last_changed: integer (nullable = true)\n",
      " |-- card_on_dark_web: string (nullable = true)\n",
      "\n",
      "\n",
      "Exchange rates schema:\n",
      "root\n",
      " |-- Series Description: string (nullable = true)\n",
      " |-- SPOT EXCHANGE RATE - EURO AREA : string (nullable = true)\n",
      " |-- UNITED KINGDOM -- SPOT EXCHANGE RATE, US$/POUND (1/RXI_N.B.UK): string (nullable = true)\n",
      " |-- SPOT EXCHANGE RATE - BRAZIL : string (nullable = true)\n",
      " |-- CHINA -- SPOT EXCHANGE RATE, YUAN/US$ P.R. : string (nullable = true)\n",
      " |-- DENMARK -- SPOT EXCHANGE RATE, KRONER/US$ : string (nullable = true)\n",
      " |-- INDIA -- SPOT EXCHANGE RATE, RUPEES/US$ : string (nullable = true)\n",
      " |-- JAPAN -- SPOT EXCHANGE RATE, YEN/US$ : string (nullable = true)\n",
      " |-- KOREA -- SPOT EXCHANGE RATE, WON/US$ : string (nullable = true)\n",
      " |-- MALAYSIA -- SPOT EXCHANGE RATE, RINGGIT/US$ : string (nullable = true)\n",
      " |-- MEXICO -- SPOT EXCHANGE RATE, PESOS/US$ : string (nullable = true)\n",
      " |-- NORWAY -- SPOT EXCHANGE RATE, KRONER/US$ : string (nullable = true)\n",
      " |-- SWEDEN -- SPOT EXCHANGE RATE, KRONOR/US$ : string (nullable = true)\n",
      " |-- SOUTH AFRICA -- SPOT EXCHANGE RATE, RAND/$US: string (nullable = true)\n",
      " |-- SINGAPORE -- SPOT EXCHANGE RATE, SINGAPORE $/US$ : string (nullable = true)\n",
      " |-- SWITZERLAND -- SPOT EXCHANGE RATE, FRANCS/US$ : string (nullable = true)\n",
      " |-- TAIWAN -- SPOT EXCHANGE RATE, NT$/US$ : string (nullable = true)\n",
      " |-- THAILAND -- SPOT EXCHANGE RATE, BAHT/US$ : string (nullable = true)\n",
      " |-- SPOT EXCHANGE RATE - VENEZUELA : string (nullable = true)\n",
      " |-- Nominal Broad Dollar Index : string (nullable = true)\n",
      " |-- Nominal Major Currencies Dollar Index : string (nullable = true)\n",
      " |-- Nominal Other Important Trading Partners Dollar Index : string (nullable = true)\n",
      " |-- AUSTRALIA -- SPOT EXCHANGE RATE US$/AU$ (RECIPROCAL OF RXI_N.B.AL): string (nullable = true)\n",
      " |-- NEW ZEALAND -- SPOT EXCHANGE RATE, US$/NZ$ RECIPROCAL OF RXI_N.B.NZ : string (nullable = true)\n",
      " |-- CANADA -- SPOT EXCHANGE RATE, CANADIAN $/US$ : string (nullable = true)\n",
      " |-- HONG KONG -- SPOT EXCHANGE RATE, HK$/US$ : string (nullable = true)\n",
      " |-- SRI LANKA -- SPOT EXCHANGE RATE, RUPEES/US$ : string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load all CSV datasets\n",
    "print(\"Loading CSV datasets...\")\n",
    "\n",
    "# 1. Transaction data (main dataset)\n",
    "transactions_df = spark.read.csv(\"../data/transactions_data.csv\", header=True, inferSchema=True)\n",
    "print(f\"âœ“ Transactions: {transactions_df.count():,} rows, {len(transactions_df.columns)} columns\")\n",
    "\n",
    "# 2. Cards data\n",
    "cards_df = spark.read.csv(\"../data/cards_data.csv\", header=True, inferSchema=True)  \n",
    "print(f\"âœ“ Cards: {cards_df.count():,} rows, {len(cards_df.columns)} columns\")\n",
    "\n",
    "# 3. Exchange rates\n",
    "exchange_rates_df = spark.read.csv(\"../data/exchange_rates.csv\", header=True, inferSchema=True)\n",
    "print(f\"âœ“ Exchange rates: {exchange_rates_df.count():,} rows, {len(exchange_rates_df.columns)} columns\")\n",
    "\n",
    "print(\"\\nDataset schemas:\")\n",
    "print(\"=\"*50)\n",
    "print(\"Transactions schema:\")\n",
    "transactions_df.printSchema()\n",
    "print(\"\\nCards schema:\")\n",
    "cards_df.printSchema()\n",
    "print(\"\\nExchange rates schema:\")\n",
    "exchange_rates_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b40b9ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading JSON datasets...\n",
      "âœ“ MCC data: 111 rows, 1 columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/12 01:34:33 ERROR Executor: Exception in task 0.0 in stage 19.0 (TID 33)8]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat org.apache.spark.sql.catalyst.expressions.UnsafeRow.getBinary(UnsafeRow.java:393)\n",
      "\tat org.apache.spark.sql.catalyst.json.CreateJacksonParser$.internalRow(CreateJacksonParser.scala:84)\n",
      "\tat org.apache.spark.sql.execution.datasources.json.TextInputJsonDataSource$.$anonfun$inferFromDataset$4(JsonDataSource.scala:106)\n",
      "\tat org.apache.spark.sql.execution.datasources.json.TextInputJsonDataSource$$$Lambda$3764/0x0000000801bbcb58.apply(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema.$anonfun$infer$3(JsonInferSchema.scala:90)\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema$$Lambda$3769/0x0000000801bbe910.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithResource(SparkErrorUtils.scala:47)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithResource$(SparkErrorUtils.scala:46)\n",
      "\tat org.apache.spark.util.Utils$.tryWithResource(Utils.scala:94)\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema.$anonfun$infer$2(JsonInferSchema.scala:90)\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema$$Lambda$3768/0x0000000801bbe530.apply(Unknown Source)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.isEmpty(Iterator.scala:387)\n",
      "\tat scala.collection.Iterator.isEmpty$(Iterator.scala:387)\n",
      "\tat scala.collection.AbstractIterator.isEmpty(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.reduceLeftOption(TraversableOnce.scala:249)\n",
      "\tat scala.collection.TraversableOnce.reduceLeftOption$(TraversableOnce.scala:248)\n",
      "\tat scala.collection.AbstractIterator.reduceLeftOption(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.reduceOption(TraversableOnce.scala:256)\n",
      "\tat scala.collection.TraversableOnce.reduceOption$(TraversableOnce.scala:256)\n",
      "\tat scala.collection.AbstractIterator.reduceOption(Iterator.scala:1431)\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema.$anonfun$infer$1(JsonInferSchema.scala:107)\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema$$Lambda$3766/0x0000000801bbd6d0.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$2983/0x0000000801a73418.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "25/08/12 01:34:33 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 0.0 in stage 19.0 (TID 33),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat org.apache.spark.sql.catalyst.expressions.UnsafeRow.getBinary(UnsafeRow.java:393)\n",
      "\tat org.apache.spark.sql.catalyst.json.CreateJacksonParser$.internalRow(CreateJacksonParser.scala:84)\n",
      "\tat org.apache.spark.sql.execution.datasources.json.TextInputJsonDataSource$.$anonfun$inferFromDataset$4(JsonDataSource.scala:106)\n",
      "\tat org.apache.spark.sql.execution.datasources.json.TextInputJsonDataSource$$$Lambda$3764/0x0000000801bbcb58.apply(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema.$anonfun$infer$3(JsonInferSchema.scala:90)\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema$$Lambda$3769/0x0000000801bbe910.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithResource(SparkErrorUtils.scala:47)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithResource$(SparkErrorUtils.scala:46)\n",
      "\tat org.apache.spark.util.Utils$.tryWithResource(Utils.scala:94)\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema.$anonfun$infer$2(JsonInferSchema.scala:90)\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema$$Lambda$3768/0x0000000801bbe530.apply(Unknown Source)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.isEmpty(Iterator.scala:387)\n",
      "\tat scala.collection.Iterator.isEmpty$(Iterator.scala:387)\n",
      "\tat scala.collection.AbstractIterator.isEmpty(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.reduceLeftOption(TraversableOnce.scala:249)\n",
      "\tat scala.collection.TraversableOnce.reduceLeftOption$(TraversableOnce.scala:248)\n",
      "\tat scala.collection.AbstractIterator.reduceLeftOption(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.reduceOption(TraversableOnce.scala:256)\n",
      "\tat scala.collection.TraversableOnce.reduceOption$(TraversableOnce.scala:256)\n",
      "\tat scala.collection.AbstractIterator.reduceOption(Iterator.scala:1431)\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema.$anonfun$infer$1(JsonInferSchema.scala:107)\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema$$Lambda$3766/0x0000000801bbd6d0.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$2983/0x0000000801a73418.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "25/08/12 01:34:33 WARN TaskSetManager: Lost task 0.0 in stage 19.0 (TID 33) (mx1.asteas.com executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat org.apache.spark.sql.catalyst.expressions.UnsafeRow.getBinary(UnsafeRow.java:393)\n",
      "\tat org.apache.spark.sql.catalyst.json.CreateJacksonParser$.internalRow(CreateJacksonParser.scala:84)\n",
      "\tat org.apache.spark.sql.execution.datasources.json.TextInputJsonDataSource$.$anonfun$inferFromDataset$4(JsonDataSource.scala:106)\n",
      "\tat org.apache.spark.sql.execution.datasources.json.TextInputJsonDataSource$$$Lambda$3764/0x0000000801bbcb58.apply(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema.$anonfun$infer$3(JsonInferSchema.scala:90)\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema$$Lambda$3769/0x0000000801bbe910.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithResource(SparkErrorUtils.scala:47)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithResource$(SparkErrorUtils.scala:46)\n",
      "\tat org.apache.spark.util.Utils$.tryWithResource(Utils.scala:94)\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema.$anonfun$infer$2(JsonInferSchema.scala:90)\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema$$Lambda$3768/0x0000000801bbe530.apply(Unknown Source)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.isEmpty(Iterator.scala:387)\n",
      "\tat scala.collection.Iterator.isEmpty$(Iterator.scala:387)\n",
      "\tat scala.collection.AbstractIterator.isEmpty(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.reduceLeftOption(TraversableOnce.scala:249)\n",
      "\tat scala.collection.TraversableOnce.reduceLeftOption$(TraversableOnce.scala:248)\n",
      "\tat scala.collection.AbstractIterator.reduceLeftOption(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.reduceOption(TraversableOnce.scala:256)\n",
      "\tat scala.collection.TraversableOnce.reduceOption$(TraversableOnce.scala:256)\n",
      "\tat scala.collection.AbstractIterator.reduceOption(Iterator.scala:1431)\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema.$anonfun$infer$1(JsonInferSchema.scala:107)\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema$$Lambda$3766/0x0000000801bbd6d0.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$2983/0x0000000801a73418.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\n",
      "25/08/12 01:34:33 ERROR TaskSetManager: Task 0 in stage 19.0 failed 1 times; aborting job\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/julienlook/Documents/Coding/big-data-analytics/.venv/lib/python3.12/site-packages/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/julienlook/Documents/Coding/big-data-analytics/.venv/lib/python3.12/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/julienlook/Documents/Coding/big-data-analytics/.venv/lib/python3.12/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/julienlook/Documents/Coding/big-data-analytics/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/z7/qy1l6b2n5ljd7dqzvm5cd2f40000gn/T/ipykernel_59740/2527424293.py\", line 9, in <module>\n",
      "    fraud_labels_df = spark.read.json(\"../data/train_fraud_labels.json\")\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/julienlook/Documents/Coding/big-data-analytics/.venv/lib/python3.12/site-packages/pyspark/sql/readwriter.py\", line 425, in json\n",
      "    return self._df(self._jreader.json(self._spark._sc._jvm.PythonUtils.toSeq(path)))\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/julienlook/Documents/Coding/big-data-analytics/.venv/lib/python3.12/site-packages/py4j/java_gateway.py\", line 1322, in __call__\n",
      "    return_value = get_return_value(\n",
      "                   ^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/julienlook/Documents/Coding/big-data-analytics/.venv/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py\", line 179, in deco\n",
      "    return f(*a, **kw)\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/Users/julienlook/Documents/Coding/big-data-analytics/.venv/lib/python3.12/site-packages/py4j/protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: <exception str() failed>\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/julienlook/Documents/Coding/big-data-analytics/.venv/lib/python3.12/site-packages/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/julienlook/Documents/Coding/big-data-analytics/.venv/lib/python3.12/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/julienlook/Documents/Coding/big-data-analytics/.venv/lib/python3.12/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/julienlook/Documents/Coding/big-data-analytics/.venv/lib/python3.12/site-packages/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/julienlook/Documents/Coding/big-data-analytics/.venv/lib/python3.12/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/julienlook/Documents/Coding/big-data-analytics/.venv/lib/python3.12/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/julienlook/Documents/Coding/big-data-analytics/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/z7/qy1l6b2n5ljd7dqzvm5cd2f40000gn/T/ipykernel_59740/2527424293.py\", line 9, in <module>\n",
      "    fraud_labels_df = spark.read.json(\"../data/train_fraud_labels.json\")\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/julienlook/Documents/Coding/big-data-analytics/.venv/lib/python3.12/site-packages/pyspark/sql/readwriter.py\", line 425, in json\n",
      "    return self._df(self._jreader.json(self._spark._sc._jvm.PythonUtils.toSeq(path)))\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/julienlook/Documents/Coding/big-data-analytics/.venv/lib/python3.12/site-packages/py4j/java_gateway.py\", line 1322, in __call__\n",
      "    return_value = get_return_value(\n",
      "                   ^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/julienlook/Documents/Coding/big-data-analytics/.venv/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py\", line 179, in deco\n",
      "    return f(*a, **kw)\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/Users/julienlook/Documents/Coding/big-data-analytics/.venv/lib/python3.12/site-packages/py4j/protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: <exception str() failed>\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/julienlook/Documents/Coding/big-data-analytics/.venv/lib/python3.12/site-packages/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/julienlook/Documents/Coding/big-data-analytics/.venv/lib/python3.12/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/julienlook/Documents/Coding/big-data-analytics/.venv/lib/python3.12/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    },
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[Errno 61] Connection refused",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[3], line 9\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# 5. Fraud labels - JSON format  \u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m fraud_labels_df \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../data/train_fraud_labels.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mâœ“ Fraud labels: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfraud_labels_df\u001b[38;5;241m.\u001b[39mcount()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m rows, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(fraud_labels_df\u001b[38;5;241m.\u001b[39mcolumns)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m columns\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/Coding/big-data-analytics/.venv/lib/python3.12/site-packages/pyspark/sql/readwriter.py:425\u001b[0m, in \u001b[0;36mDataFrameReader.json\u001b[0;34m(self, path, schema, primitivesAsString, prefersDecimal, allowComments, allowUnquotedFieldNames, allowSingleQuotes, allowNumericLeadingZero, allowBackslashEscapingAnyCharacter, mode, columnNameOfCorruptRecord, dateFormat, timestampFormat, multiLine, allowUnquotedControlChars, lineSep, samplingRatio, dropFieldIfAllNull, encoding, locale, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, allowNonNumericNumbers)\u001b[0m\n\u001b[1;32m    424\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_spark\u001b[38;5;241m.\u001b[39m_sc\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 425\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_spark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonUtils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoSeq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    426\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, RDD):\n",
      "File \u001b[0;32m~/Documents/Coding/big-data-analytics/.venv/lib/python3.12/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m~/Documents/Coding/big-data-analytics/.venv/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/Documents/Coding/big-data-analytics/.venv/lib/python3.12/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31m<class 'str'>\u001b[0m: (<class 'ConnectionRefusedError'>, ConnectionRefusedError(61, 'Connection refused'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m~/Documents/Coding/big-data-analytics/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py:2179\u001b[0m, in \u001b[0;36mInteractiveShell.showtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2176\u001b[0m         traceback\u001b[38;5;241m.\u001b[39mprint_exc()\n\u001b[1;32m   2177\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 2179\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_showtraceback\u001b[49m\u001b[43m(\u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2180\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall_pdb:\n\u001b[1;32m   2181\u001b[0m     \u001b[38;5;66;03m# drop into debugger\u001b[39;00m\n\u001b[1;32m   2182\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdebugger(force\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/Documents/Coding/big-data-analytics/.venv/lib/python3.12/site-packages/ipykernel/zmqshell.py:587\u001b[0m, in \u001b[0;36mZMQInteractiveShell._showtraceback\u001b[0;34m(self, etype, evalue, stb)\u001b[0m\n\u001b[1;32m    581\u001b[0m sys\u001b[38;5;241m.\u001b[39mstdout\u001b[38;5;241m.\u001b[39mflush()\n\u001b[1;32m    582\u001b[0m sys\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mflush()\n\u001b[1;32m    584\u001b[0m exc_content \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraceback\u001b[39m\u001b[38;5;124m\"\u001b[39m: stb,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mename\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(etype\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m),\n\u001b[0;32m--> 587\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevalue\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mevalue\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    588\u001b[0m }\n\u001b[1;32m    590\u001b[0m dh \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdisplayhook\n\u001b[1;32m    591\u001b[0m \u001b[38;5;66;03m# Send exception info over pub socket for other clients than the caller\u001b[39;00m\n\u001b[1;32m    592\u001b[0m \u001b[38;5;66;03m# to pick up\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Coding/big-data-analytics/.venv/lib/python3.12/site-packages/py4j/protocol.py:471\u001b[0m, in \u001b[0;36mPy4JJavaError.__str__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    469\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__str__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    470\u001b[0m     gateway_client \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_exception\u001b[38;5;241m.\u001b[39m_gateway_client\n\u001b[0;32m--> 471\u001b[0m     answer \u001b[38;5;241m=\u001b[39m \u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexception_cmd\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    472\u001b[0m     return_value \u001b[38;5;241m=\u001b[39m get_return_value(answer, gateway_client, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    473\u001b[0m     \u001b[38;5;66;03m# Note: technically this should return a bytestring 'str' rather than\u001b[39;00m\n\u001b[1;32m    474\u001b[0m     \u001b[38;5;66;03m# unicodes in Python 2; however, it can return unicodes for now.\u001b[39;00m\n\u001b[1;32m    475\u001b[0m     \u001b[38;5;66;03m# See https://github.com/bartdag/py4j/issues/306 for more details.\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Coding/big-data-analytics/.venv/lib/python3.12/site-packages/py4j/java_gateway.py:1036\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msend_command\u001b[39m(\u001b[38;5;28mself\u001b[39m, command, retry\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, binary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Sends a command to the JVM. This method is not intended to be\u001b[39;00m\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;124;03m       called directly by Py4J users. It is usually called by\u001b[39;00m\n\u001b[1;32m   1018\u001b[0m \u001b[38;5;124;03m       :class:`JavaMember` instances.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;124;03m     if `binary` is `True`.\u001b[39;00m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1036\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1037\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1038\u001b[0m         response \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39msend_command(command)\n",
      "File \u001b[0;32m~/Documents/Coding/big-data-analytics/.venv/lib/python3.12/site-packages/py4j/clientserver.py:284\u001b[0m, in \u001b[0;36mJavaClient._get_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m connection\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 284\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_new_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m~/Documents/Coding/big-data-analytics/.venv/lib/python3.12/site-packages/py4j/clientserver.py:291\u001b[0m, in \u001b[0;36mJavaClient._create_new_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_create_new_connection\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    288\u001b[0m     connection \u001b[38;5;241m=\u001b[39m ClientServerConnection(\n\u001b[1;32m    289\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_parameters, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_parameters,\n\u001b[1;32m    290\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_property, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 291\u001b[0m     \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect_to_java_server\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    292\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_thread_connection(connection)\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m~/Documents/Coding/big-data-analytics/.venv/lib/python3.12/site-packages/py4j/clientserver.py:438\u001b[0m, in \u001b[0;36mClientServerConnection.connect_to_java_server\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context:\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context\u001b[38;5;241m.\u001b[39mwrap_socket(\n\u001b[1;32m    437\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket, server_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_address)\n\u001b[0;32m--> 438\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_address\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_port\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket\u001b[38;5;241m.\u001b[39mmakefile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_connected \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 61] Connection refused"
     ]
    }
   ],
   "source": [
    "# Load JSON datasets\n",
    "print(\"Loading JSON datasets...\")\n",
    "\n",
    "# Check if Spark session is active\n",
    "try:\n",
    "\tspark.version\n",
    "except Exception as e:\n",
    "\traise RuntimeError(\"Spark session is not active. Please restart the kernel and re-run the Spark initialization cell.\") from e\n",
    "\n",
    "# 4. MCC (Merchant Category Codes) - JSON format\n",
    "mcc_df = spark.read.json(\"../data/mcc.json\")\n",
    "print(f\"âœ“ MCC data: {mcc_df.count():,} rows, {len(mcc_df.columns)} columns\")\n",
    "\n",
    "# 5. Fraud labels - JSON format  \n",
    "fraud_labels_df = spark.read.json(\"../data/train_fraud_labels.json\")\n",
    "print(f\"âœ“ Fraud labels: {fraud_labels_df.count():,} rows, {len(fraud_labels_df.columns)} columns\")\n",
    "\n",
    "print(\"\\nJSON schemas:\")\n",
    "print(\"=\"*50)\n",
    "print(\"MCC schema:\")\n",
    "mcc_df.printSchema()\n",
    "print(\"\\nFraud labels schema:\")\n",
    "fraud_labels_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8e15a7",
   "metadata": {},
   "source": [
    "## 2. Data Exploration and Preparation\n",
    "\n",
    "Let's examine sample data from each dataset to understand relationships:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30480116",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore sample data from each dataset\n",
    "print(\"Sample data from each dataset:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"1. TRANSACTIONS (first 5 rows):\")\n",
    "transactions_df.show(5)\n",
    "\n",
    "print(\"2. CARDS (first 5 rows):\")\n",
    "cards_df.show(5)\n",
    "\n",
    "print(\"3. EXCHANGE RATES (first 5 rows):\")\n",
    "exchange_rates_df.show(5)\n",
    "\n",
    "print(\"4. MCC CODES (first 5 rows):\")\n",
    "mcc_df.show(5)\n",
    "\n",
    "print(\"5. FRAUD LABELS (first 5 rows):\")\n",
    "fraud_labels_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bcc3dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze join keys and relationships\n",
    "print(\"Analyzing potential join relationships:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Check common columns between datasets\n",
    "tx_cols = set(transactions_df.columns)\n",
    "cards_cols = set(cards_df.columns)\n",
    "rates_cols = set(exchange_rates_df.columns)\n",
    "mcc_cols = set(mcc_df.columns)\n",
    "fraud_cols = set(fraud_labels_df.columns)\n",
    "\n",
    "print(\"Common columns for joins:\")\n",
    "print(f\"Transactions âˆ© Cards: {tx_cols.intersection(cards_cols)}\")\n",
    "print(f\"Transactions âˆ© Exchange Rates: {tx_cols.intersection(rates_cols)}\")\n",
    "print(f\"Transactions âˆ© MCC: {tx_cols.intersection(mcc_cols)}\")\n",
    "print(f\"Transactions âˆ© Fraud Labels: {tx_cols.intersection(fraud_cols)}\")\n",
    "\n",
    "# Check unique counts for potential join keys\n",
    "print(f\"\\nJoin key analysis:\")\n",
    "if 'client_id' in transactions_df.columns:\n",
    "    tx_clients = transactions_df.select(\"client_id\").distinct().count()\n",
    "    print(f\"Unique clients in transactions: {tx_clients:,}\")\n",
    "\n",
    "if 'client_id' in cards_df.columns:\n",
    "    card_clients = cards_df.select(\"client_id\").distinct().count()\n",
    "    print(f\"Unique clients in cards: {card_clients:,}\")\n",
    "\n",
    "if 'mcc' in transactions_df.columns and 'mcc' in mcc_df.columns:\n",
    "    tx_mccs = transactions_df.select(\"mcc\").distinct().count()\n",
    "    mcc_codes = mcc_df.select(\"mcc\").distinct().count()\n",
    "    print(f\"Unique MCCs in transactions: {tx_mccs:,}\")\n",
    "    print(f\"Unique MCCs in MCC reference: {mcc_codes:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6f1460",
   "metadata": {},
   "source": [
    "## 3. Pandas to Spark DataFrame Conversion Examples\n",
    "\n",
    "Let's demonstrate multiple ways to convert between pandas and Spark DataFrames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2925b65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Create pandas DataFrame and convert to Spark\n",
    "print(\"Example 1: Creating pandas DataFrame and converting to Spark\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create a sample pandas DataFrame with banking data\n",
    "banking_products_pd = pd.DataFrame({\n",
    "    'product_id': ['CC_001', 'CC_002', 'SA_001', 'SA_002', 'LN_001'],\n",
    "    'product_type': ['Credit Card', 'Credit Card', 'Savings Account', 'Savings Account', 'Loan'],\n",
    "    'interest_rate': [18.99, 15.49, 2.5, 3.0, 8.75],\n",
    "    'min_balance': [0, 1000, 100, 500, 0],\n",
    "    'annual_fee': [95, 0, 0, 0, 200],\n",
    "    'rewards_rate': [1.5, 2.0, 0.0, 0.0, 0.0]\n",
    "})\n",
    "\n",
    "print(\"Original pandas DataFrame:\")\n",
    "print(banking_products_pd)\n",
    "print(f\"pandas DataFrame shape: {banking_products_pd.shape}\")\n",
    "\n",
    "# Convert to Spark DataFrame\n",
    "banking_products_spark = spark.createDataFrame(banking_products_pd)\n",
    "print(f\"\\nConverted to Spark DataFrame:\")\n",
    "banking_products_spark.show()\n",
    "print(f\"Spark DataFrame count: {banking_products_spark.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88cde8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Create Spark DataFrame from Python data structures\n",
    "print(\"Example 2: Creating Spark DataFrame from Python structures\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Method 1: From list of tuples with explicit schema\n",
    "customer_data = [\n",
    "    ('CUST_001', 'John Doe', 45, 'Premium', 85000.0, 'New York'),\n",
    "    ('CUST_002', 'Jane Smith', 32, 'Gold', 62000.0, 'California'), \n",
    "    ('CUST_003', 'Bob Johnson', 28, 'Standard', 45000.0, 'Texas'),\n",
    "    ('CUST_004', 'Alice Brown', 55, 'Premium', 120000.0, 'Florida'),\n",
    "    ('CUST_005', 'Charlie Wilson', 38, 'Gold', 75000.0, 'Illinois')\n",
    "]\n",
    "\n",
    "# Define schema explicitly for better performance\n",
    "customer_schema = StructType([\n",
    "    StructField(\"customer_id\", StringType(), False),\n",
    "    StructField(\"full_name\", StringType(), False),\n",
    "    StructField(\"age\", IntegerType(), False),\n",
    "    StructField(\"tier\", StringType(), False),\n",
    "    StructField(\"annual_income\", DoubleType(), True),\n",
    "    StructField(\"state\", StringType(), True)\n",
    "])\n",
    "\n",
    "customers_spark = spark.createDataFrame(customer_data, customer_schema)\n",
    "print(\"Spark DataFrame from tuples with explicit schema:\")\n",
    "customers_spark.show()\n",
    "\n",
    "# Method 2: From list of dictionaries (inferred schema)\n",
    "account_data = [\n",
    "    {'account_id': 'ACC_001', 'customer_id': 'CUST_001', 'account_type': 'Checking', 'balance': 5420.50},\n",
    "    {'account_id': 'ACC_002', 'customer_id': 'CUST_001', 'account_type': 'Savings', 'balance': 25600.75},\n",
    "    {'account_id': 'ACC_003', 'customer_id': 'CUST_002', 'account_type': 'Checking', 'balance': 8950.25},\n",
    "    {'account_id': 'ACC_004', 'customer_id': 'CUST_003', 'account_type': 'Savings', 'balance': 15750.00},\n",
    "    {'account_id': 'ACC_005', 'customer_id': 'CUST_004', 'account_type': 'Investment', 'balance': 95400.80}\n",
    "]\n",
    "\n",
    "accounts_spark = spark.createDataFrame(account_data)\n",
    "print(\"Spark DataFrame from dictionaries (inferred schema):\")\n",
    "accounts_spark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09417ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3: Converting Spark DataFrame back to pandas\n",
    "print(\"Example 3: Converting Spark DataFrame to pandas\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Convert small Spark DataFrame to pandas (be careful with large datasets!)\n",
    "customers_pd = customers_spark.toPandas()\n",
    "print(\"Converted back to pandas:\")\n",
    "print(customers_pd)\n",
    "print(f\"pandas DataFrame info:\")\n",
    "print(customers_pd.info())\n",
    "\n",
    "# Convert with specific columns only\n",
    "accounts_subset_pd = accounts_spark.select(\"customer_id\", \"account_type\", \"balance\").toPandas()\n",
    "print(f\"\\nPandas DataFrame from selected Spark columns:\")\n",
    "print(accounts_subset_pd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d9a101",
   "metadata": {},
   "source": [
    "## 4. Comprehensive Data Joins\n",
    "\n",
    "Now let's join our banking datasets using different join strategies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25763e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for joins - clean and standardize\n",
    "print(\"Preparing datasets for joins...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Clean transactions data\n",
    "transactions_clean = transactions_df \\\n",
    "    .withColumn(\"amount_numeric\", regexp_replace(col(\"amount\"), \"[\\$,]\", \"\").cast(\"double\")) \\\n",
    "    .withColumn(\"transaction_date\", to_date(col(\"date\"))) \\\n",
    "    .filter(col(\"amount_numeric\") > 0)\n",
    "\n",
    "print(f\"âœ“ Transactions cleaned: {transactions_clean.count():,} records\")\n",
    "\n",
    "# Cache datasets that will be used multiple times\n",
    "transactions_clean.cache()\n",
    "cards_df.cache()\n",
    "mcc_df.cache()\n",
    "if fraud_labels_df.count() > 0:\n",
    "    fraud_labels_df.cache()\n",
    "\n",
    "print(\"âœ“ Key datasets cached for join performance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1266fe51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join 1: Transactions with Card Information\n",
    "print(\"Join 1: Transactions âŸ• Cards (LEFT JOIN)\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Left join to keep all transactions, add card info where available\n",
    "tx_with_cards = transactions_clean.alias(\"tx\") \\\n",
    "    .join(cards_df.alias(\"cards\"), \n",
    "          col(\"tx.client_id\") == col(\"cards.client_id\"), \n",
    "          \"left\")\n",
    "\n",
    "print(f\"Result: {tx_with_cards.count():,} records\")\n",
    "print(\"Sample joined data:\")\n",
    "tx_with_cards.select(\"tx.client_id\", \"tx.amount_numeric\", \"cards.card_type\", \"cards.credit_limit\").show(5)\n",
    "\n",
    "# Analyze join effectiveness\n",
    "cards_matched = tx_with_cards.filter(col(\"cards.client_id\").isNotNull()).count()\n",
    "join_rate = (cards_matched / tx_with_cards.count()) * 100\n",
    "print(f\"Join effectiveness: {cards_matched:,} transactions matched cards ({join_rate:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330aa540",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join 2: Add MCC (Merchant Category Code) information\n",
    "print(\"Join 2: Transactions + Cards âŸ• MCC (LEFT JOIN)\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Join with MCC data to get merchant category descriptions\n",
    "tx_cards_mcc = tx_with_cards.alias(\"base\") \\\n",
    "    .join(mcc_df.alias(\"mcc\"),\n",
    "          col(\"base.mcc\") == col(\"mcc.mcc\"),\n",
    "          \"left\")\n",
    "\n",
    "print(f\"Result: {tx_cards_mcc.count():,} records\")\n",
    "print(\"Sample with MCC info:\")\n",
    "tx_cards_mcc.select(\n",
    "    \"base.client_id\", \n",
    "    \"base.amount_numeric\", \n",
    "    \"base.mcc\",\n",
    "    \"mcc.category_description\"\n",
    ").show(5, truncate=False)\n",
    "\n",
    "# Analyze MCC join coverage\n",
    "mcc_matched = tx_cards_mcc.filter(col(\"mcc.mcc\").isNotNull()).count()\n",
    "mcc_join_rate = (mcc_matched / tx_cards_mcc.count()) * 100\n",
    "print(f\"MCC join effectiveness: {mcc_matched:,} transactions have category info ({mcc_join_rate:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e7d370",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join 3: Add Exchange Rate information (if applicable)\n",
    "print(\"Join 3: Add Exchange Rates (LEFT JOIN)\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Check if we have currency information to join on\n",
    "if \"currency\" in exchange_rates_df.columns:\n",
    "    # If transactions have currency info, join on currency and date\n",
    "    if \"currency\" in transactions_clean.columns:\n",
    "        tx_full = tx_cards_mcc.alias(\"main\") \\\n",
    "            .join(exchange_rates_df.alias(\"rates\"),\n",
    "                  (col(\"main.currency\") == col(\"rates.currency\")) & \n",
    "                  (col(\"main.transaction_date\") == col(\"rates.date\")),\n",
    "                  \"left\")\n",
    "        print(\"Joined on currency and date\")\n",
    "    else:\n",
    "        # If no currency column, add USD rates by date\n",
    "        tx_full = tx_cards_mcc.alias(\"main\") \\\n",
    "            .join(exchange_rates_df.alias(\"rates\"),\n",
    "                  col(\"main.transaction_date\") == col(\"rates.date\"),\n",
    "                  \"left\")\n",
    "        print(\"Joined on date only (assuming USD)\")\n",
    "else:\n",
    "    # If no proper join key, just keep the previous result\n",
    "    tx_full = tx_cards_mcc\n",
    "    print(\"Exchange rates data doesn't have suitable join keys - skipped\")\n",
    "\n",
    "print(f\"Final joined dataset: {tx_full.count():,} records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc3ae21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join 4: Add Fraud Labels (INNER JOIN for training data)\n",
    "print(\"Join 4: Add Fraud Labels (INNER JOIN)\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Create training dataset by inner joining with fraud labels\n",
    "if fraud_labels_df.count() > 0 and \"client_id\" in fraud_labels_df.columns:\n",
    "    training_data = tx_full.alias(\"main\") \\\n",
    "        .join(fraud_labels_df.alias(\"fraud\"),\n",
    "              col(\"main.client_id\") == col(\"fraud.client_id\"),\n",
    "              \"inner\")\n",
    "    \n",
    "    print(f\"Training dataset: {training_data.count():,} records with fraud labels\")\n",
    "    \n",
    "    # Analyze fraud distribution\n",
    "    fraud_stats = training_data.groupBy(\"fraud.is_fraud\").count()\n",
    "    print(\"Fraud label distribution:\")\n",
    "    fraud_stats.show()\n",
    "else:\n",
    "    print(\"No suitable fraud labels found - creating synthetic labels for demo\")\n",
    "    training_data = tx_full.withColumn(\"is_fraud\", (rand(42) < 0.05).cast(\"int\"))\n",
    "    print(f\"Training dataset with synthetic fraud labels: {training_data.count():,} records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5654e1",
   "metadata": {},
   "source": [
    "## 5. Advanced Join Patterns and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc3673e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate different join types with our sample data\n",
    "print(\"Advanced Join Patterns Demo:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Create sample datasets for join demonstrations\n",
    "customers_sample = customers_spark.select(\"customer_id\", \"full_name\", \"tier\")\n",
    "accounts_sample = accounts_spark.select(\"customer_id\", \"account_type\", \"balance\")\n",
    "\n",
    "# Add a customer without accounts and account without customer for demo\n",
    "additional_customer = spark.createDataFrame([(\"CUST_999\", \"No Account User\", \"Standard\")], \n",
    "                                          [\"customer_id\", \"full_name\", \"tier\"])\n",
    "customers_extended = customers_sample.union(additional_customer)\n",
    "\n",
    "additional_account = spark.createDataFrame([(\"CUST_888\", \"Orphaned\", 1000.0)],\n",
    "                                         [\"customer_id\", \"account_type\", \"balance\"])\n",
    "accounts_extended = accounts_sample.union(additional_account)\n",
    "\n",
    "print(\"1. INNER JOIN - Only matching records:\")\n",
    "inner_result = customers_extended.join(accounts_extended, \"customer_id\", \"inner\")\n",
    "print(f\"Inner join result: {inner_result.count()} records\")\n",
    "inner_result.show()\n",
    "\n",
    "print(\"2. LEFT JOIN - All customers, with account info where available:\")\n",
    "left_result = customers_extended.join(accounts_extended, \"customer_id\", \"left\")\n",
    "print(f\"Left join result: {left_result.count()} records\")\n",
    "left_result.show()\n",
    "\n",
    "print(\"3. RIGHT JOIN - All accounts, with customer info where available:\")\n",
    "right_result = customers_extended.join(accounts_extended, \"customer_id\", \"right\")\n",
    "print(f\"Right join result: {right_result.count()} records\")\n",
    "right_result.show()\n",
    "\n",
    "print(\"4. FULL OUTER JOIN - All records from both sides:\")\n",
    "full_result = customers_extended.join(accounts_extended, \"customer_id\", \"full_outer\")\n",
    "print(f\"Full outer join result: {full_result.count()} records\")\n",
    "full_result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0e0767",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create final comprehensive dataset with all joins\n",
    "print(\"Creating Final Comprehensive Dataset:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create the most complete dataset possible\n",
    "final_dataset = tx_full \\\n",
    "    .withColumn(\"transaction_year\", year(col(\"transaction_date\"))) \\\n",
    "    .withColumn(\"transaction_month\", month(col(\"transaction_date\"))) \\\n",
    "    .withColumn(\"is_weekend\", dayofweek(col(\"transaction_date\")).isin([1, 7]).cast(\"int\")) \\\n",
    "    .withColumn(\"is_high_value\", (col(\"amount_numeric\") > 1000).cast(\"int\"))\n",
    "\n",
    "# Add our synthetic customers and accounts data\n",
    "final_with_customers = final_dataset.alias(\"main\") \\\n",
    "    .join(customers_spark.alias(\"cust\"), \n",
    "          col(\"main.client_id\") == col(\"cust.customer_id\"), \n",
    "          \"left\") \\\n",
    "    .join(accounts_spark.alias(\"acc\"),\n",
    "          col(\"main.client_id\") == col(\"acc.customer_id\"),\n",
    "          \"left\")\n",
    "\n",
    "print(f\"Final comprehensive dataset: {final_with_customers.count():,} records\")\n",
    "print(f\"Columns: {len(final_with_customers.columns)}\")\n",
    "\n",
    "# Show sample of final dataset\n",
    "print(\"\\nSample of final comprehensive dataset:\")\n",
    "final_with_customers.select(\n",
    "    \"client_id\", \"amount_numeric\", \"transaction_date\",\n",
    "    \"category_description\", \"cust.tier\", \"acc.account_type\", \"acc.balance\"\n",
    ").show(10, truncate=False)\n",
    "\n",
    "# Dataset statistics\n",
    "print(\"\\nDataset Statistics:\")\n",
    "print(\"=\"*50)\n",
    "stats = final_with_customers.agg(\n",
    "    count(\"*\").alias(\"total_records\"),\n",
    "    countDistinct(\"client_id\").alias(\"unique_customers\"),\n",
    "    sum(\"amount_numeric\").alias(\"total_volume\"),\n",
    "    avg(\"amount_numeric\").alias(\"avg_transaction\"),\n",
    "    min(\"transaction_date\").alias(\"earliest_date\"),\n",
    "    max(\"transaction_date\").alias(\"latest_date\")\n",
    ").collect()[0]\n",
    "\n",
    "print(f\"Total records: {stats.total_records:,}\")\n",
    "print(f\"Unique customers: {stats.unique_customers:,}\")\n",
    "print(f\"Total transaction volume: ${stats.total_volume:,.2f}\")\n",
    "print(f\"Average transaction: ${stats.avg_transaction:,.2f}\")\n",
    "print(f\"Date range: {stats.earliest_date} to {stats.latest_date}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77039575",
   "metadata": {},
   "source": [
    "## 6. Performance Optimization for Joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94938c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join performance optimization techniques\n",
    "print(\"Join Performance Optimization Techniques:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 1. Broadcast joins for small datasets\n",
    "print(\"1. Broadcast Join Optimization:\")\n",
    "small_mcc = mcc_df.filter(col(\"mcc\").isNotNull())  # Ensure clean data\n",
    "if small_mcc.count() < 10000:  # Broadcast if small enough\n",
    "    broadcast_join = transactions_clean \\\n",
    "        .join(broadcast(small_mcc), \"mcc\", \"left\")\n",
    "    print(f\"âœ“ MCC data broadcasted for efficient joins ({small_mcc.count()} records)\")\n",
    "else:\n",
    "    print(\"âœ“ MCC data too large for broadcast, using regular join\")\n",
    "\n",
    "# 2. Bucketing for repeated joins (demonstration)\n",
    "print(\"\\n2. Partitioning Strategy:\")\n",
    "partitioned_tx = transactions_clean.repartition(4, \"client_id\")\n",
    "print(f\"âœ“ Transactions repartitioned by client_id for better join locality\")\n",
    "\n",
    "# 3. Join hints and strategies\n",
    "print(\"\\n3. Join Performance Analysis:\")\n",
    "print(\"Current Spark configurations for joins:\")\n",
    "print(f\"  - Adaptive Query Execution: {spark.conf.get('spark.sql.adaptive.enabled')}\")\n",
    "print(f\"  - Adaptive Coalesce: {spark.conf.get('spark.sql.adaptive.coalescePartitions.enabled')}\")\n",
    "print(f\"  - Skew Join Detection: {spark.conf.get('spark.sql.adaptive.skewJoin.enabled')}\")\n",
    "\n",
    "# 4. Analyze join performance\n",
    "print(\"\\n4. Join Execution Analysis:\")\n",
    "# This would show the query plan (commented out to avoid verbose output)\n",
    "# tx_with_cards.explain(True)\n",
    "print(\"âœ“ Use .explain(True) on DataFrames to see detailed execution plans\")\n",
    "print(\"âœ“ Monitor Spark UI for join shuffle operations and data skew\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073a3a80",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### What we accomplished:\n",
    "\n",
    "1. **Multi-format Data Loading**: CSV and JSON files\n",
    "2. **Comprehensive Joins**: LEFT, RIGHT, INNER, and FULL OUTER joins\n",
    "3. **Pandas â†” Spark Conversions**: Multiple methods and examples\n",
    "4. **Real-world Join Scenarios**: Transactions with cards, MCC codes, exchange rates\n",
    "5. **Performance Optimization**: Broadcast joins, partitioning, and caching\n",
    "6. **Data Quality Analysis**: Join effectiveness and coverage metrics\n",
    "\n",
    "### Key Learning Points:\n",
    "\n",
    "- **Join Strategy Selection**: Choose appropriate join types based on data relationships\n",
    "- **Performance Considerations**: Use broadcast joins for small datasets, cache frequently accessed data\n",
    "- **Schema Management**: Explicit schemas vs. inference for better performance\n",
    "- **Data Conversion**: When to use pandas vs Spark based on data size\n",
    "- **Real-world Applications**: Building comprehensive datasets for ML and analytics\n",
    "\n",
    "### Next Steps:\n",
    "- Experiment with different join conditions and strategies\n",
    "- Practice with window functions over joined datasets\n",
    "- Explore advanced optimization techniques like bucketing\n",
    "- Build machine learning pipelines on joined datasets\n",
    "\n",
    "ðŸŽ‰ **Successfully joined multiple datasets and demonstrated DataFrame conversions!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0ee4812b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning up Spark resources...\n"
     ]
    },
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[Errno 61] Connection refused",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Clean up resources\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCleaning up Spark resources...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mâœ… Spark session terminated successfully!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mðŸŽŠ Data joins and conversions workshop completed!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/Coding/big-data-analytics/.venv/lib/python3.12/site-packages/pyspark/sql/session.py:1799\u001b[0m, in \u001b[0;36mSparkSession.stop\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1797\u001b[0m \u001b[38;5;66;03m# We should clean the default session up. See SPARK-23228.\u001b[39;00m\n\u001b[1;32m   1798\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1799\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSparkSession\u001b[49m\u001b[38;5;241m.\u001b[39mclearDefaultSession()\n\u001b[1;32m   1800\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mSparkSession\u001b[38;5;241m.\u001b[39mclearActiveSession()\n\u001b[1;32m   1801\u001b[0m SparkSession\u001b[38;5;241m.\u001b[39m_instantiatedSession \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Coding/big-data-analytics/.venv/lib/python3.12/site-packages/py4j/java_gateway.py:1712\u001b[0m, in \u001b[0;36mJVMView.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1709\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;241m==\u001b[39m UserHelpAutoCompletion\u001b[38;5;241m.\u001b[39mKEY:\n\u001b[1;32m   1710\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m UserHelpAutoCompletion()\n\u001b[0;32m-> 1712\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1713\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproto\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mREFLECTION_COMMAND_NAME\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\n\u001b[1;32m   1714\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproto\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mREFL_GET_UNKNOWN_SUB_COMMAND_NAME\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_id\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\n\u001b[1;32m   1715\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mproto\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEND_COMMAND_PART\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1716\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer \u001b[38;5;241m==\u001b[39m proto\u001b[38;5;241m.\u001b[39mSUCCESS_PACKAGE:\n\u001b[1;32m   1717\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m JavaPackage(name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gateway_client, jvm_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_id)\n",
      "File \u001b[0;32m~/Documents/Coding/big-data-analytics/.venv/lib/python3.12/site-packages/py4j/java_gateway.py:1036\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msend_command\u001b[39m(\u001b[38;5;28mself\u001b[39m, command, retry\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, binary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Sends a command to the JVM. This method is not intended to be\u001b[39;00m\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;124;03m       called directly by Py4J users. It is usually called by\u001b[39;00m\n\u001b[1;32m   1018\u001b[0m \u001b[38;5;124;03m       :class:`JavaMember` instances.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;124;03m     if `binary` is `True`.\u001b[39;00m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1036\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1037\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1038\u001b[0m         response \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39msend_command(command)\n",
      "File \u001b[0;32m~/Documents/Coding/big-data-analytics/.venv/lib/python3.12/site-packages/py4j/clientserver.py:284\u001b[0m, in \u001b[0;36mJavaClient._get_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m connection\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 284\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_new_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m~/Documents/Coding/big-data-analytics/.venv/lib/python3.12/site-packages/py4j/clientserver.py:291\u001b[0m, in \u001b[0;36mJavaClient._create_new_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_create_new_connection\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    288\u001b[0m     connection \u001b[38;5;241m=\u001b[39m ClientServerConnection(\n\u001b[1;32m    289\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_parameters, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_parameters,\n\u001b[1;32m    290\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_property, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 291\u001b[0m     \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect_to_java_server\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    292\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_thread_connection(connection)\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m~/Documents/Coding/big-data-analytics/.venv/lib/python3.12/site-packages/py4j/clientserver.py:438\u001b[0m, in \u001b[0;36mClientServerConnection.connect_to_java_server\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context:\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context\u001b[38;5;241m.\u001b[39mwrap_socket(\n\u001b[1;32m    437\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket, server_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_address)\n\u001b[0;32m--> 438\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_address\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_port\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket\u001b[38;5;241m.\u001b[39mmakefile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_connected \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 61] Connection refused"
     ]
    }
   ],
   "source": [
    "# Clean up resources\n",
    "print(\"Cleaning up Spark resources...\")\n",
    "spark.stop()\n",
    "print(\"âœ… Spark session terminated successfully!\")\n",
    "print(\"ðŸŽŠ Data joins and conversions workshop completed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
