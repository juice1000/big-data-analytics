{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "076ea279",
   "metadata": {},
   "source": [
    "# PySpark Data Joins - Simple Example\n",
    "\n",
    "## Goal:\n",
    "Join two datasets: **Transactions** + **Cards** to create enriched transaction data.\n",
    "\n",
    "## Files we'll use:\n",
    "- `transactions_data.csv` - Transaction records\n",
    "- `cards_data.csv` - Card information \n",
    "\n",
    "Let's see how to join these two datasets!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c6464e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Spark 3.5.3 initialized\n"
     ]
    }
   ],
   "source": [
    "# Simple imports for joining data\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SimpleDataJoin\") \\\n",
    "    .master(\"local[2]\") \\\n",
    "    .config(\"spark.driver.bindAddress\", \"127.0.0.1\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "print(f\"✓ Spark {spark.version} initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee31542",
   "metadata": {},
   "source": [
    "## 1. Load the Two Datasets\n",
    "\n",
    "Load transactions and cards data to understand what we're working with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ad772fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Transactions: 13,305,915 rows, 12 columns\n",
      "✓ Cards: 6,146 rows, 13 columns\n",
      "\n",
      "Transactions schema:\n",
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- date: timestamp (nullable = true)\n",
      " |-- client_id: integer (nullable = true)\n",
      " |-- card_id: integer (nullable = true)\n",
      " |-- amount: string (nullable = true)\n",
      " |-- use_chip: string (nullable = true)\n",
      " |-- merchant_id: integer (nullable = true)\n",
      " |-- merchant_city: string (nullable = true)\n",
      " |-- merchant_state: string (nullable = true)\n",
      " |-- zip: double (nullable = true)\n",
      " |-- mcc: integer (nullable = true)\n",
      " |-- errors: string (nullable = true)\n",
      "\n",
      "\n",
      "Cards schema:\n",
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- client_id: integer (nullable = true)\n",
      " |-- card_brand: string (nullable = true)\n",
      " |-- card_type: string (nullable = true)\n",
      " |-- card_number: long (nullable = true)\n",
      " |-- expires: string (nullable = true)\n",
      " |-- cvv: integer (nullable = true)\n",
      " |-- has_chip: string (nullable = true)\n",
      " |-- num_cards_issued: integer (nullable = true)\n",
      " |-- credit_limit: string (nullable = true)\n",
      " |-- acct_open_date: string (nullable = true)\n",
      " |-- year_pin_last_changed: integer (nullable = true)\n",
      " |-- card_on_dark_web: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the two datasets we want to join\n",
    "print(\"Loading datasets...\")\n",
    "\n",
    "# 1. Transaction data\n",
    "transactions_df = spark.read.csv(\"../data/transactions_data.csv\", header=True, inferSchema=True)\n",
    "print(f\"✓ Transactions: {transactions_df.count():,} rows, {len(transactions_df.columns)} columns\")\n",
    "\n",
    "# 2. Cards data\n",
    "cards_df = spark.read.csv(\"../data/cards_data.csv\", header=True, inferSchema=True)  \n",
    "print(f\"✓ Cards: {cards_df.count():,} rows, {len(cards_df.columns)} columns\")\n",
    "\n",
    "# Look at the structure\n",
    "print(\"\\nTransactions schema:\")\n",
    "transactions_df.printSchema()\n",
    "\n",
    "print(\"\\nCards schema:\")\n",
    "cards_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3b40b9ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample data from each dataset:\n",
      "========================================\n",
      "TRANSACTIONS (first 5 rows):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+---------+-------+-------+-----------------+-----------+-------------+--------------+-------+----+------+\n",
      "|     id|               date|client_id|card_id| amount|         use_chip|merchant_id|merchant_city|merchant_state|    zip| mcc|errors|\n",
      "+-------+-------------------+---------+-------+-------+-----------------+-----------+-------------+--------------+-------+----+------+\n",
      "|7475327|2010-01-01 00:01:00|     1556|   2972|$-77.00|Swipe Transaction|      59935|       Beulah|            ND|58523.0|5499|  NULL|\n",
      "|7475328|2010-01-01 00:02:00|      561|   4575| $14.57|Swipe Transaction|      67570|   Bettendorf|            IA|52722.0|5311|  NULL|\n",
      "|7475329|2010-01-01 00:02:00|     1129|    102| $80.00|Swipe Transaction|      27092|        Vista|            CA|92084.0|4829|  NULL|\n",
      "|7475331|2010-01-01 00:05:00|      430|   2860|$200.00|Swipe Transaction|      27092|  Crown Point|            IN|46307.0|4829|  NULL|\n",
      "|7475332|2010-01-01 00:06:00|      848|   3915| $46.41|Swipe Transaction|      13051|      Harwood|            MD|20776.0|5813|  NULL|\n",
      "+-------+-------------------+---------+-------+-------+-----------------+-----------+-------------+--------------+-------+----+------+\n",
      "only showing top 5 rows\n",
      "\n",
      "CARDS (first 5 rows):\n",
      "+----+---------+----------+---------------+----------------+-------+---+--------+----------------+------------+--------------+---------------------+----------------+\n",
      "|  id|client_id|card_brand|      card_type|     card_number|expires|cvv|has_chip|num_cards_issued|credit_limit|acct_open_date|year_pin_last_changed|card_on_dark_web|\n",
      "+----+---------+----------+---------------+----------------+-------+---+--------+----------------+------------+--------------+---------------------+----------------+\n",
      "|4524|      825|      Visa|          Debit|4344676511950444|12/2022|623|     YES|               2|      $24295|       09/2002|                 2008|              No|\n",
      "|2731|      825|      Visa|          Debit|4956965974959986|12/2020|393|     YES|               2|      $21968|       04/2014|                 2014|              No|\n",
      "|3701|      825|      Visa|          Debit|4582313478255491|02/2024|719|     YES|               2|      $46414|       07/2003|                 2004|              No|\n",
      "|  42|      825|      Visa|         Credit|4879494103069057|08/2024|693|      NO|               1|      $12400|       01/2003|                 2012|              No|\n",
      "|4659|      825|Mastercard|Debit (Prepaid)|5722874738736011|03/2009| 75|     YES|               1|         $28|       09/2008|                 2009|              No|\n",
      "+----+---------+----------+---------------+----------------+-------+---+--------+----------------+------------+--------------+---------------------+----------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Common columns for joining: {'id', 'client_id'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 12:===================================================>     (9 + 1) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique clients in transactions: 1,219\n",
      "Unique clients in cards: 2,000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Explore sample data to understand the relationship\n",
    "print(\"Sample data from each dataset:\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "print(\"TRANSACTIONS (first 5 rows):\")\n",
    "transactions_df.show(5)\n",
    "\n",
    "print(\"CARDS (first 5 rows):\")\n",
    "cards_df.show(5)\n",
    "\n",
    "# Find the common column for joining\n",
    "tx_cols = set(transactions_df.columns)\n",
    "cards_cols = set(cards_df.columns)\n",
    "common_cols = tx_cols.intersection(cards_cols)\n",
    "\n",
    "print(f\"Common columns for joining: {common_cols}\")\n",
    "\n",
    "# Check join key statistics\n",
    "if 'client_id' in common_cols:\n",
    "    tx_clients = transactions_df.select(\"client_id\").distinct().count()\n",
    "    card_clients = cards_df.select(\"client_id\").distinct().count()\n",
    "    print(f\"Unique clients in transactions: {tx_clients:,}\")\n",
    "    print(f\"Unique clients in cards: {card_clients:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8e15a7",
   "metadata": {},
   "source": [
    "## 2. Join the Datasets\n",
    "\n",
    "Join transactions with cards to get enriched transaction data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "30480116",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 25:===================================================>     (9 + 1) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Join completed: 51,115,337 records\n",
      "\n",
      "Sample joined data:\n",
      "+---------+-------+----+---------------+\n",
      "|client_id| amount| mcc|      card_type|\n",
      "+---------+-------+----+---------------+\n",
      "|     1556|$-77.00|5499|         Credit|\n",
      "|     1556|$-77.00|5499|Debit (Prepaid)|\n",
      "|     1556|$-77.00|5499|         Credit|\n",
      "|     1556|$-77.00|5499|          Debit|\n",
      "|      561| $14.57|5311|         Credit|\n",
      "+---------+-------+----+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Perform the join - LEFT JOIN to keep all transactions\n",
    "joined_data = transactions_df.alias(\"tx\") \\\n",
    "    .join(cards_df.alias(\"cards\"), \n",
    "          col(\"tx.client_id\") == col(\"cards.client_id\"), \n",
    "          \"left\")\n",
    "\n",
    "print(f\"✓ Join completed: {joined_data.count():,} records\")\n",
    "\n",
    "# Show sample joined data\n",
    "print(\"\\nSample joined data:\")\n",
    "joined_data.select(\"tx.client_id\", \"tx.amount\", \"tx.mcc\", \"cards.card_type\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5654e1",
   "metadata": {},
   "source": [
    "## 3. Join with JSON Data (MCC Codes)\n",
    "\n",
    "Now let's add merchant category codes from the JSON file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9bc3673e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ MCC JSON loaded\n",
      "✓ MCC codes processed: 109 records\n",
      "\n",
      "MCC data sample:\n",
      "+----+--------------------+\n",
      "| mcc|category_description|\n",
      "+----+--------------------+\n",
      "|5812|Eating Places and...|\n",
      "|5541|    Service Stations|\n",
      "|7996|Amusement Parks, ...|\n",
      "|5411|Grocery Stores, S...|\n",
      "|4784|Tolls and Bridge ...|\n",
      "+----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+----+--------------------+\n",
      "| mcc|category_description|\n",
      "+----+--------------------+\n",
      "|5812|Eating Places and...|\n",
      "|5541|    Service Stations|\n",
      "|7996|Amusement Parks, ...|\n",
      "|5411|Grocery Stores, S...|\n",
      "|4784|Tolls and Bridge ...|\n",
      "+----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 66:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Final dataset with MCC: 51,115,337 records\n",
      "\n",
      "Enriched transaction data:\n",
      "+---------+-------+---------------+--------------------+\n",
      "|client_id| amount|      card_type|category_description|\n",
      "+---------+-------+---------------+--------------------+\n",
      "|     1556|$-77.00|         Credit|Miscellaneous Foo...|\n",
      "|     1556|$-77.00|Debit (Prepaid)|Miscellaneous Foo...|\n",
      "|     1556|$-77.00|         Credit|Miscellaneous Foo...|\n",
      "|     1556|$-77.00|          Debit|Miscellaneous Foo...|\n",
      "|      561| $14.57|         Credit|   Department Stores|\n",
      "|      561| $14.57|          Debit|   Department Stores|\n",
      "|      561| $14.57|Debit (Prepaid)|   Department Stores|\n",
      "|      561| $14.57|          Debit|   Department Stores|\n",
      "|      561| $14.57|          Debit|   Department Stores|\n",
      "|     1129| $80.00|         Credit|      Money Transfer|\n",
      "+---------+-------+---------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Load MCC codes from JSON (key-value format)\n",
    "mcc_raw = spark.read.json(\"../data/mcc.json\")\n",
    "print(f\"✓ MCC JSON loaded\")\n",
    "\n",
    "# The JSON is a single object with key-value pairs, we need to transform it\n",
    "# Convert to proper DataFrame structure\n",
    "import json\n",
    "\n",
    "# Read as text first to parse the key-value structure\n",
    "with open(\"../data/mcc.json\", \"r\") as f:\n",
    "    mcc_dict = json.load(f)\n",
    "\n",
    "# Convert to list of tuples for Spark DataFrame\n",
    "mcc_data = [(int(mcc_code), description) for mcc_code, description in mcc_dict.items()]\n",
    "\n",
    "# Create DataFrame with proper schema\n",
    "mcc_df = spark.createDataFrame(mcc_data, [\"mcc\", \"category_description\"])\n",
    "print(f\"✓ MCC codes processed: {mcc_df.count():,} records\")\n",
    "\n",
    "# Show MCC structure\n",
    "print(\"\\nMCC data sample:\")\n",
    "mcc_df.show(5)\n",
    "\n",
    "# Join transactions with MCC codes\n",
    "# Select only the columns you need from joined_data to avoid ambiguity\n",
    "joined_selected = joined_data.select(\n",
    "    col(\"tx.client_id\").alias(\"client_id\"),\n",
    "    col(\"tx.amount\").alias(\"amount\"),\n",
    "    col(\"tx.mcc\").alias(\"mcc\"),\n",
    "    col(\"cards.card_type\").alias(\"card_type\")\n",
    ")\n",
    "\n",
    "final_data = joined_selected.join(\n",
    "    mcc_df,\n",
    "    joined_selected.mcc == mcc_df.mcc,\n",
    "    \"left\"\n",
    ")\n",
    "\n",
    "print(f\"✓ Final dataset with MCC: {final_data.count():,} records\")\n",
    "\n",
    "# Show enriched data\n",
    "print(\"\\nEnriched transaction data:\")\n",
    "final_data.select(\n",
    "    \"client_id\", \n",
    "    \"amount\", \n",
    "    \"card_type\",\n",
    "    \"category_description\"\n",
    ").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073a3a80",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### What we accomplished:\n",
    "\n",
    "1. **Loaded CSV data**: Transactions and cards datasets\n",
    "2. **Performed LEFT JOIN**: Combined transactions with card information\n",
    "3. **Joined JSON data**: Added MCC category descriptions\n",
    "4. **Created enriched dataset**: Three-way join for complete transaction context\n",
    "\n",
    "### Key join concepts:\n",
    "- **LEFT JOIN**: Keep all records from left table, add matching from right\n",
    "- **Join keys**: Must match between datasets (`client_id`, `mcc`)\n",
    "- **Multi-format joins**: CSV + JSON in same workflow\n",
    "- **Table aliases**: `tx`, `cards`, `mcc` for cleaner syntax\n",
    "\n",
    "🎉 **Simple joins completed successfully!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee4812b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning up Spark resources...\n",
      "✅ Spark session terminated successfully!\n",
      "🎊 Data joins and conversions workshop completed!\n",
      "✅ Spark session terminated successfully!\n",
      "🎊 Data joins and conversions workshop completed!\n"
     ]
    }
   ],
   "source": [
    "# Clean up\n",
    "spark.stop()\n",
    "print(\"✅ Spark session terminated\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
