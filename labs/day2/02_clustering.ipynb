{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a1ed76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-means Clustering with PySpark - Simple Customer Segmentation\n",
    "\n",
    "# Import required libraries for big data processing and machine learning\n",
    "from pyspark.sql import SparkSession  # Main entry point for Spark SQL functionality\n",
    "from pyspark.sql.functions import *   # SQL functions for data transformations\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler  # Feature preprocessing tools\n",
    "from pyspark.ml.clustering import KMeans  # K-means clustering algorithm\n",
    "from pyspark.ml import Pipeline  # ML pipeline for chaining transformations\n",
    "import pandas as pd  # For local data manipulation and visualization\n",
    "import matplotlib.pyplot as plt  # For creating plots and charts\n",
    "import numpy as np  # For numerical computations\n",
    "\n",
    "# Initialize Spark session with specific configurations\n",
    "# These configs prevent common networking issues when running Spark locally\n",
    "spark = (\n",
    "\tSparkSession.builder\n",
    "\t.appName(\"CustomerSegmentation\")  # Give our Spark application a descriptive name\n",
    "\t.config(\"spark.driver.host\", \"127.0.0.1\")  # Force localhost binding to avoid network issues\n",
    "\t.config(\"spark.driver.bindAddress\", \"127.0.0.1\")  # Explicit bind address for driver\n",
    "\t.getOrCreate()  # Create new session or get existing one\n",
    ")\n",
    "\n",
    "# Reduce log noise by setting log level to WARN (only show warnings and errors)\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "print(f\"Spark Version: {spark.version}\")  # Display Spark version for debugging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ebb6a6",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Basic Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e08be10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load transaction data from CSV file\n",
    "print(\"Loading transactions...\")\n",
    "\n",
    "# Read CSV with automatic schema inference - Spark will detect column types\n",
    "# header=True: First row contains column names\n",
    "# inferSchema=True: Automatically detect data types (can be slow for large files)\n",
    "df = spark.read.csv(\"../data/transactions_data.csv\", header=True, inferSchema=True)\n",
    "print(f\"Raw data loaded: {df.count():,} transactions\")  # Count triggers full data scan\n",
    "\n",
    "# Create basic features through data cleaning and feature engineering\n",
    "df_processed = df \\\n",
    "    .withColumn(\"amount_numeric\", regexp_replace(col(\"amount\"), \"[\\$,]\", \"\").cast(\"double\")) \\\n",
    "    .withColumn(\"is_online\", (col(\"merchant_city\") == \"ONLINE\").cast(\"int\")) \\\n",
    "    .withColumn(\"is_weekend\", dayofweek(col(\"date\")).isin([1, 7]).cast(\"int\")) \\\n",
    "    .filter(col(\"amount_numeric\").isNotNull() & (col(\"amount_numeric\") > 0))\n",
    "    \n",
    "# Breaking down the transformations:\n",
    "# 1. amount_numeric: Remove $ and comma symbols, convert to double for math operations\n",
    "# 2. is_online: Create binary flag (1/0) for online transactions based on merchant_city\n",
    "# 3. is_weekend: Create binary flag for weekend transactions (Sunday=1, Saturday=7 in Spark)\n",
    "# 4. filter: Remove null amounts and negative/zero amounts (data quality step)\n",
    "\n",
    "# Cache the processed DataFrame in memory for faster repeated access\n",
    "# This is crucial for iterative ML algorithms like K-means\n",
    "df_processed.cache()\n",
    "print(f\"Processed {df_processed.count():,} transactions\")\n",
    "df_processed.show(5)  # Display first 5 rows to inspect data quality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58806df9",
   "metadata": {},
   "source": [
    "## 2. Customer Feature Engineering\n",
    "\n",
    "Create customer-level features for clustering based on spending patterns, behavioral patterns, and temporal patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339eb6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create customer-level aggregated features for clustering analysis\n",
    "print(\"Creating customer features...\")\n",
    "\n",
    "# Define required columns for feature creation - defensive programming approach\n",
    "required_cols = [\"client_id\", \"amount_numeric\", \"is_online\", \"merchant_id\"]\n",
    "available_cols = df_processed.columns\n",
    "missing_cols = [col for col in required_cols if col not in available_cols]\n",
    "\n",
    "# Validate that all required columns exist before proceeding\n",
    "if missing_cols:\n",
    "    print(f\"Missing columns: {missing_cols}\")\n",
    "    print(f\"Available columns: {available_cols}\")\n",
    "else:\n",
    "    print(\"All required columns found. Proceeding with feature creation...\")\n",
    "    \n",
    "    # TODO: Create customer_features DataFrame here\n",
    "    # Aggregate transaction-level data to customer-level features\n",
    "    # Hint: Use groupBy(\"client_id\").agg() with sum, avg, count, countDistinct\n",
    "    \n",
    "    # customer_features = ...\n",
    "    \n",
    "    # Display statistical summary of all features to understand data distribution\n",
    "    # customer_features.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8787fa6",
   "metadata": {},
   "source": [
    "## 3. K-means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c994600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features and run K-means clustering algorithm\n",
    "# Define the feature columns to use for clustering (all numerical features)\n",
    "feature_cols = [\"total_spend\", \"avg_transaction_amount\", \"transaction_count\", \"online_ratio\", \"merchant_diversity\"]\n",
    "\n",
    "# Defensive check to ensure customer_features DataFrame exists\n",
    "# This prevents runtime errors if cells are run out of order\n",
    "if 'customer_features' not in locals():\n",
    "    print(\"ERROR: customer_features not created. Please run the previous cell first.\")\n",
    "else:\n",
    "    print(f\"Using {customer_features.count():,} customers for clustering\")\n",
    "    \n",
    "    # Handle missing values by replacing nulls with 0.0\n",
    "    customer_features_clean = customer_features.fillna(0.0)\n",
    "    \n",
    "    # TODO: Create ML Pipeline for K-means clustering\n",
    "    # You'll need: VectorAssembler, StandardScaler, KMeans\n",
    "    \n",
    "    # assembler = VectorAssembler(...)\n",
    "    # scaler = StandardScaler(...)  \n",
    "    # kmeans = KMeans(...)\n",
    "    # pipeline = Pipeline(stages=[...])\n",
    "    \n",
    "    print(\"Training K-means model...\")\n",
    "    # model = pipeline.fit(customer_features_clean)\n",
    "    # predictions = model.transform(customer_features_clean)\n",
    "\n",
    "    print(\"K-means clustering completed!\")\n",
    "    # Show distribution of customers across clusters\n",
    "    # predictions.groupBy(\"prediction\").count().orderBy(\"prediction\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8b1322",
   "metadata": {},
   "source": [
    "## 4. Analyze Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24c7392",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze cluster characteristics to understand what each cluster represents\n",
    "# This step transforms cluster numbers into meaningful business insights\n",
    "\n",
    "# Calculate summary statistics for each cluster\n",
    "# groupBy(\"prediction\"): Group customers by their assigned cluster\n",
    "# alias(\"cluster\"): Rename prediction column to more meaningful \"cluster\" name\n",
    "cluster_summary = predictions.groupBy(col(\"prediction\").alias(\"cluster\")).agg(\n",
    "    count(\"*\").alias(\"customer_count\"),  # How many customers in each cluster\n",
    "    avg(\"total_spend\").alias(\"avg_total_spend\"),  # Average spending per cluster\n",
    "    avg(\"avg_transaction_amount\").alias(\"avg_transaction_amount\"),  # Average transaction size per cluster\n",
    "    avg(\"online_ratio\").alias(\"avg_online_ratio\")  # Average online shopping tendency per cluster\n",
    ").orderBy(\"cluster\")  # Sort by cluster number for easier interpretation\n",
    "\n",
    "# Why these metrics matter for business:\n",
    "# - customer_count: Shows relative size of each market segment\n",
    "# - avg_total_spend: Identifies high-value vs low-value customer segments\n",
    "# - avg_transaction_amount: Reveals spending behavior patterns per transaction\n",
    "# - avg_online_ratio: Distinguishes digital-native vs traditional customers\n",
    "\n",
    "print(\"Cluster Analysis:\")\n",
    "cluster_summary.show()  # Display the cluster analysis results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b66050",
   "metadata": {},
   "source": [
    "## 5. Simple Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08eb616d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualizations to better understand cluster characteristics\n",
    "# Sample 10% of data for visualization (full dataset would be too slow and cluttered)\n",
    "sample_data = predictions.sample(0.1, seed=42).toPandas()  # Convert to Pandas for matplotlib\n",
    "\n",
    "# TODO: Create visualizations for cluster analysis\n",
    "# Hint: Use plt.subplot(1, 2, 1) for side-by-side plots\n",
    "# Plot 1: Scatter plot of total_spend vs transaction_count colored by cluster\n",
    "# Plot 2: Bar chart showing customer count per cluster\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "# Your visualization code goes here...\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419677f3",
   "metadata": {},
   "source": [
    "## 6. Business Insights  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb42fb88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translate technical clustering results into actionable business insights\n",
    "# This section converts cluster numbers into meaningful customer personas\n",
    "\n",
    "print(\"Customer Segmentation Results:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Iterate through each cluster to create business interpretations\n",
    "# collect() brings Spark DataFrame to driver for local processing\n",
    "for row in cluster_summary.collect():\n",
    "    cluster_id = row['cluster']  # Get cluster number (0, 1, 2, 3)\n",
    "    \n",
    "    # Display key metrics for each cluster in business-friendly format\n",
    "    print(f\"\\nCluster {cluster_id}: {row['customer_count']:,} customers\")\n",
    "    print(f\"  Average spend: ${row['avg_total_spend']:,.2f}\")\n",
    "    print(f\"  Online ratio: {row['avg_online_ratio']*100:.1f}%\")\n",
    "    \n",
    "    # Simple persona assignment based on business rules\n",
    "    # This is a basic approach - real implementations would use more sophisticated logic\n",
    "    if row['avg_total_spend'] > 1000:\n",
    "        persona = \"High-Value Customer\"  # Focus on retention and premium services\n",
    "    elif row['avg_online_ratio'] > 0.5:\n",
    "        persona = \"Digital Customer\"  # Target with online promotions and mobile features  \n",
    "    else:\n",
    "        persona = \"Traditional Customer\"  # Engage through in-store experiences and phone support\n",
    "    \n",
    "    print(f\"  Persona: {persona}\")\n",
    "    \n",
    "    # Persona assignment rationale:\n",
    "    # - High spend threshold ($1000): Identifies most valuable customers requiring VIP treatment\n",
    "    # - Online ratio threshold (50%): Distinguishes digital-native vs traditional shoppers\n",
    "    # - This creates 3 distinct marketing strategies rather than generic approaches\n",
    "\n",
    "# Business value statement\n",
    "print(f\"\\nâœ… Clustering completed! Found meaningful customer segments for targeted marketing.\")\n",
    "\n",
    "# Resource cleanup - stops Spark session to free up memory and resources\n",
    "# Important for local development but may not be desired in production notebooks\n",
    "spark.stop()\n",
    "\n",
    "# Marketing strategy implications by persona:\n",
    "# 1. High-Value Customers: Premium loyalty programs, personal shoppers, exclusive events\n",
    "# 2. Digital Customers: Mobile app features, online-only deals, social media campaigns  \n",
    "# 3. Traditional Customers: In-store promotions, direct mail, phone-based customer service\n",
    "\n",
    "# Next steps for business implementation:\n",
    "# 1. Validate personas with business stakeholders\n",
    "# 2. Design targeted marketing campaigns for each segment\n",
    "# 3. Set up automated segmentation pipeline for new customers\n",
    "# 4. Measure campaign effectiveness by persona\n",
    "# 5. Refine clustering model based on business outcomes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
