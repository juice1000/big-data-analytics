{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset (sample for performance)\n",
    "# In production, you'd use Spark for the full 13M records\n",
    "df = pd.read_csv('../data/transactions_data.csv', nrows=100000)\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nColumns: {list(df.columns)}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 📄 Unstructured Data Analytics\n",
    "\n",
    "## 🎯 Lernziele:\n",
    "- Text Data Processing für Banking\n",
    "- Transaction Description Mining\n",
    "- Sentiment Analysis\n",
    "- Text-based Fraud Indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text processing libraries\n",
    "import re\n",
    "from collections import Counter\n",
    "try:\n",
    "    from wordcloud import WordCloud\n",
    "except ImportError:\n",
    "    print(\"WordCloud not available - install with: pip install wordcloud\")\n",
    "\n",
    "try:\n",
    "    import nltk\n",
    "    # Download NLTK data (run once)\n",
    "    try:\n",
    "        nltk.data.find('tokenizers/punkt')\n",
    "    except LookupError:\n",
    "        nltk.download('punkt')\n",
    "        nltk.download('stopwords')\n",
    "        nltk.download('vader_lexicon')\n",
    "\n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "except ImportError:\n",
    "    print(\"NLTK not available - install with: pip install nltk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Synthetic Transaction Descriptions\n",
    "\n",
    "Create realistic transaction descriptions for text analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic transaction descriptions\n",
    "np.random.seed(42)\n",
    "\n",
    "merchant_types = {\n",
    "    5411: ['GROCERY STORE', 'SUPERMARKET', 'FOOD MART'],\n",
    "    5812: ['RESTAURANT', 'FAST FOOD', 'CAFE', 'DINER'],\n",
    "    4121: ['TAXI SERVICE', 'UBER', 'LYFT', 'CAB COMPANY'],\n",
    "    5541: ['GAS STATION', 'FUEL STOP', 'PETROL'],\n",
    "    5942: ['BOOKSTORE', 'LIBRARY', 'READING CORNER'],\n",
    "    5499: ['CONVENIENCE STORE', 'CORNER SHOP', '24/7 MART'],\n",
    "    7801: ['ONLINE PAYMENT', 'DIGITAL TRANSACTION', 'WEB PURCHASE'],\n",
    "    4784: ['ATM WITHDRAWAL', 'CASH ADVANCE', 'ATM TRANSACTION']\n",
    "}\n",
    "\n",
    "def generate_description(row):\n",
    "    mcc = row.get('mcc', 5499)\n",
    "    amount = row['amount_numeric']\n",
    "    \n",
    "    if mcc in merchant_types:\n",
    "        base_desc = np.random.choice(merchant_types[mcc])\n",
    "    else:\n",
    "        base_desc = \"MERCHANT TRANSACTION\"\n",
    "    \n",
    "    # Add suspicious patterns for some transactions\n",
    "    if np.random.random() < 0.05:  # 5% suspicious\n",
    "        suspicious_words = ['URGENT', 'IMMEDIATE', 'FINAL NOTICE', 'VERIFY ACCOUNT', 'SECURITY ALERT']\n",
    "        base_desc += ' ' + np.random.choice(suspicious_words)\n",
    "    \n",
    "    return f\"{base_desc} ${amount:.2f}\"\n",
    "\n",
    "# Generate descriptions for sample data\n",
    "sample_df = df.sample(10000, random_state=42).copy()\n",
    "sample_df['description'] = sample_df.apply(generate_description, axis=1)\n",
    "\n",
    "print(\"Sample transaction descriptions:\")\n",
    "print(sample_df[['amount', 'description', 'is_fraud']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Text Data Processing\n",
    "\n",
    "### 📝 **EXERCISE 4: Transaction Description Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text preprocessing function\n",
    "def preprocess_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove special characters and digits\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    # Remove extra whitespace\n",
    "    text = ' '.join(text.split())\n",
    "    return text\n",
    "\n",
    "sample_df['description_clean'] = sample_df['description'].apply(preprocess_text)\n",
    "\n",
    "print(\"Text preprocessing examples:\")\n",
    "print(sample_df[['description', 'description_clean']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🔍 **YOUR TASK:** \n",
    "Implement text mining for transaction categories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Complete text mining analysis\n",
    "\n",
    "# 1. Extract most common words\n",
    "# Define stop words (use simple list if NLTK not available)\n",
    "try:\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "except:\n",
    "    stop_words = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'is', 'are', 'was', 'were'}\n",
    "\n",
    "all_words = []\n",
    "for desc in sample_df['description_clean']:\n",
    "    words = desc.split()\n",
    "    # YOUR CODE HERE: Filter out stop words and short words\n",
    "    filtered_words = [word for word in words if word not in stop_words and len(word) > 2]\n",
    "    all_words.extend(filtered_words)\n",
    "\n",
    "# 2. Count word frequencies\n",
    "# YOUR CODE HERE\n",
    "word_freq = Counter(all_words)\n",
    "top_words = word_freq.most_common(20)\n",
    "\n",
    "print(\"Top 20 words in transaction descriptions:\")\n",
    "for word, count in top_words:\n",
    "    print(f\"{word}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word cloud visualization (if available)\n",
    "try:\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(word_freq)\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title('Transaction Description Word Cloud')\n",
    "    plt.show()\n",
    "except:\n",
    "    # Alternative visualization with bar chart\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    words, counts = zip(*top_words[:15])\n",
    "    plt.barh(words, counts)\n",
    "    plt.title('Top 15 Words in Transaction Descriptions')\n",
    "    plt.xlabel('Frequency')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Sentiment Analysis für Banking Communications\n",
    "\n",
    "### 📝 **EXERCISE 5: Sentiment-based Risk Assessment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple sentiment analysis (if NLTK VADER not available)\n",
    "def simple_sentiment_score(text):\n",
    "    positive_words = ['good', 'great', 'excellent', 'amazing', 'wonderful', 'fantastic', 'love', 'like']\n",
    "    negative_words = ['bad', 'terrible', 'awful', 'hate', 'dislike', 'horrible', 'urgent', 'alert', 'warning', 'security']\n",
    "    \n",
    "    words = text.lower().split()\n",
    "    pos_count = sum(1 for word in words if word in positive_words)\n",
    "    neg_count = sum(1 for word in words if word in negative_words)\n",
    "    \n",
    "    if pos_count + neg_count == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    return (pos_count - neg_count) / (pos_count + neg_count)\n",
    "\n",
    "try:\n",
    "    # Use NLTK VADER if available\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    \n",
    "    def get_sentiment_scores(text):\n",
    "        scores = sia.polarity_scores(text)\n",
    "        return scores\n",
    "    \n",
    "    sentiment_scores = sample_df['description'].apply(get_sentiment_scores)\n",
    "    sample_df['sentiment_compound'] = [score['compound'] for score in sentiment_scores]\n",
    "    sample_df['sentiment_positive'] = [score['pos'] for score in sentiment_scores]\n",
    "    sample_df['sentiment_negative'] = [score['neg'] for score in sentiment_scores]\n",
    "    sample_df['sentiment_neutral'] = [score['neu'] for score in sentiment_scores]\n",
    "    \n",
    "except:\n",
    "    # Use simple sentiment if NLTK not available\n",
    "    sample_df['sentiment_compound'] = sample_df['description'].apply(simple_sentiment_score)\n",
    "    sample_df['sentiment_positive'] = (sample_df['sentiment_compound'] > 0).astype(float)\n",
    "    sample_df['sentiment_negative'] = (sample_df['sentiment_compound'] < 0).astype(float)\n",
    "    sample_df['sentiment_neutral'] = (sample_df['sentiment_compound'] == 0).astype(float)\n",
    "\n",
    "print(\"Sentiment analysis results:\")\n",
    "print(sample_df[['description', 'sentiment_compound', 'is_fraud']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🔍 **YOUR TASK:**\n",
    "Analyze correlation between sentiment and fraud:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Analyze sentiment patterns in fraud vs legitimate transactions\n",
    "\n",
    "# 1. Group by fraud status and analyze sentiment\n",
    "# YOUR CODE HERE\n",
    "sentiment_by_fraud = sample_df.groupby('is_fraud')[['sentiment_compound', 'sentiment_positive', 'sentiment_negative']].mean()\n",
    "\n",
    "print(\"Sentiment Analysis by Fraud Status:\")\n",
    "print(sentiment_by_fraud)\n",
    "\n",
    "# 2. Statistical test for sentiment differences\n",
    "from scipy import stats\n",
    "\n",
    "fraud_sentiment = sample_df[sample_df['is_fraud'] == 1]['sentiment_compound']\n",
    "legit_sentiment = sample_df[sample_df['is_fraud'] == 0]['sentiment_compound']\n",
    "\n",
    "# YOUR CODE HERE: Perform t-test\n",
    "if len(fraud_sentiment) > 0 and len(legit_sentiment) > 0:\n",
    "    t_stat, p_value = stats.ttest_ind(fraud_sentiment, legit_sentiment)\n",
    "    \n",
    "    print(f\"\\nT-test results:\")\n",
    "    print(f\"T-statistic: {t_stat:.4f}\")\n",
    "    print(f\"P-value: {p_value:.4f}\")\n",
    "    print(f\"Significant difference: {'Yes' if p_value < 0.05 else 'No'}\")\n",
    "else:\n",
    "    print(\"\\nInsufficient data for statistical test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 📊 14:50-15:40: Data Visualization & Pandas Deep-Dive\n",
    "\n",
    "## 🎯 Lernziele:\n",
    "- Pandas Advanced Techniques\n",
    "- Time Series Analysis\n",
    "- Statistical Analysis Methods\n",
    "- Interactive Banking KPI Dashboards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Time Series Analysis\n",
    "\n",
    "### 📝 **EXERCISE 6: Banking Time Series Analytics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time series preparation\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df['year'] = df['date'].dt.year\n",
    "df['month'] = df['date'].dt.month\n",
    "df['day'] = df['date'].dt.day\n",
    "df['weekday'] = df['date'].dt.day_name()\n",
    "df['hour'] = df['date'].dt.hour\n",
    "\n",
    "# Set date as index for time series operations\n",
    "df_ts = df.set_index('date').copy()\n",
    "\n",
    "print(f\"Time series data prepared: {df_ts.index.min()} to {df_ts.index.max()}\")\n",
    "print(f\"Total time span: {(df_ts.index.max() - df_ts.index.min()).days} days\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🔍 **YOUR TASK:**\n",
    "Implement comprehensive time series analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Complete time series analysis\n",
    "\n",
    "# 1. Daily transaction patterns\n",
    "# YOUR CODE HERE\n",
    "daily_metrics = df_ts.groupby(df_ts.index.date).agg({\n",
    "    'amount_numeric': ['sum', 'mean', 'count'],\n",
    "    'is_fraud': 'sum',\n",
    "    'is_online': 'sum'\n",
    "})\n",
    "\n",
    "daily_metrics.columns = ['daily_volume', 'avg_transaction', 'transaction_count', 'fraud_count', 'online_count']\n",
    "daily_metrics['fraud_rate'] = daily_metrics['fraud_count'] / daily_metrics['transaction_count'] * 100\n",
    "daily_metrics['online_rate'] = daily_metrics['online_count'] / daily_metrics['transaction_count'] * 100\n",
    "\n",
    "print(\"Daily metrics calculated:\")\n",
    "print(daily_metrics.head())\n",
    "\n",
    "# 2. Weekly patterns\n",
    "# YOUR CODE HERE\n",
    "weekly_patterns = df.groupby('weekday').agg({\n",
    "    'amount_numeric': ['mean', 'count'],\n",
    "    'is_fraud': 'mean',\n",
    "    'is_online': 'mean'\n",
    "}).round(3)\n",
    "\n",
    "weekly_patterns.columns = ['avg_amount', 'transaction_count', 'fraud_rate', 'online_rate']\n",
    "print(\"\\nWeekly patterns:\")\n",
    "print(weekly_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced time series visualizations\n",
    "plt.figure(figsize=(15, 12))\n",
    "\n",
    "# Daily transaction volume\n",
    "plt.subplot(3, 2, 1)\n",
    "daily_metrics['daily_volume'].plot()\n",
    "plt.title('Daily Transaction Volume')\n",
    "plt.ylabel('Total Volume ($)')\n",
    "\n",
    "# Daily fraud rate\n",
    "plt.subplot(3, 2, 2)\n",
    "daily_metrics['fraud_rate'].plot(color='red')\n",
    "plt.title('Daily Fraud Rate')\n",
    "plt.ylabel('Fraud Rate (%)')\n",
    "\n",
    "# Hourly patterns\n",
    "plt.subplot(3, 2, 3)\n",
    "hourly_patterns = df.groupby('hour')['amount_numeric'].sum()\n",
    "hourly_patterns.plot(kind='bar')\n",
    "plt.title('Transaction Volume by Hour')\n",
    "plt.xlabel('Hour of Day')\n",
    "plt.ylabel('Volume ($)')\n",
    "\n",
    "# Weekly patterns\n",
    "plt.subplot(3, 2, 4)\n",
    "days_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "weekly_ordered = weekly_patterns.reindex(days_order)\n",
    "weekly_ordered['avg_amount'].plot(kind='bar', color='green')\n",
    "plt.title('Average Transaction Amount by Day')\n",
    "plt.xlabel('Day of Week')\n",
    "plt.ylabel('Average Amount ($)')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Moving averages\n",
    "plt.subplot(3, 2, 5)\n",
    "daily_metrics['transaction_count'].rolling(window=7).mean().plot(label='7-day MA')\n",
    "daily_metrics['transaction_count'].rolling(window=30).mean().plot(label='30-day MA')\n",
    "plt.title('Transaction Count Moving Averages')\n",
    "plt.ylabel('Transaction Count')\n",
    "plt.legend()\n",
    "\n",
    "# Correlation heatmap\n",
    "plt.subplot(3, 2, 6)\n",
    "correlation_matrix = daily_metrics[['daily_volume', 'avg_transaction', 'fraud_rate', 'online_rate']].corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0)\n",
    "plt.title('Daily Metrics Correlation')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Banking KPI Dashboard\n",
    "\n",
    "### 📝 **EXERCISE 7: Comprehensive KPI Visualization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate key banking KPIs\n",
    "def calculate_banking_kpis(df):\n",
    "    kpis = {}\n",
    "    \n",
    "    # Volume metrics\n",
    "    kpis['total_volume'] = df['amount_numeric'].sum()\n",
    "    kpis['total_transactions'] = len(df)\n",
    "    kpis['avg_transaction_size'] = df['amount_numeric'].mean()\n",
    "    \n",
    "    # Customer metrics\n",
    "    kpis['active_customers'] = df['client_id'].nunique()\n",
    "    kpis['avg_transactions_per_customer'] = len(df) / df['client_id'].nunique()\n",
    "    kpis['avg_customer_value'] = df.groupby('client_id')['amount_numeric'].sum().mean()\n",
    "    \n",
    "    # Risk metrics\n",
    "    kpis['fraud_rate'] = df['is_fraud'].mean() * 100\n",
    "    kpis['fraud_volume'] = df[df['is_fraud'] == 1]['amount_numeric'].sum()\n",
    "    kpis['high_value_transactions'] = (df['amount_numeric'] > df['amount_numeric'].quantile(0.95)).mean() * 100\n",
    "    \n",
    "    # Channel metrics\n",
    "    kpis['online_percentage'] = df['is_online'].mean() * 100\n",
    "    kpis['weekend_percentage'] = df['is_weekend'].mean() * 100\n",
    "    \n",
    "    return kpis\n",
    "\n",
    "# Calculate current KPIs\n",
    "current_kpis = calculate_banking_kpis(df)\n",
    "\n",
    "print(\"Banking KPI Dashboard\")\n",
    "print(\"=\" * 50)\n",
    "for kpi, value in current_kpis.items():\n",
    "    if 'rate' in kpi or 'percentage' in kpi:\n",
    "        print(f\"{kpi.replace('_', ' ').title()}: {value:.2f}%\")\n",
    "    elif 'volume' in kpi or 'value' in kpi or 'size' in kpi:\n",
    "        print(f\"{kpi.replace('_', ' ').title()}: ${value:,.2f}\")\n",
    "    else:\n",
    "        print(f\"{kpi.replace('_', ' ').title()}: {value:,.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🔍 **YOUR TASK:**\n",
    "Create a comprehensive KPI dashboard with visualizations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create comprehensive KPI dashboard\n",
    "\n",
    "# 1. Create figure with multiple subplots\n",
    "# YOUR CODE HERE\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "fig.suptitle('Banking KPI Dashboard', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 2. Daily volume trend\n",
    "axes[0, 0].plot(daily_metrics.index, daily_metrics['daily_volume'], linewidth=2)\n",
    "axes[0, 0].set_title('Daily Transaction Volume')\n",
    "axes[0, 0].set_ylabel('Volume ($)')\n",
    "axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 3. Fraud rate by day of week\n",
    "fraud_by_day = df.groupby('weekday')['is_fraud'].mean() * 100\n",
    "fraud_by_day_ordered = fraud_by_day.reindex(days_order)\n",
    "axes[0, 1].bar(range(len(fraud_by_day_ordered)), fraud_by_day_ordered.values, color='red', alpha=0.7)\n",
    "axes[0, 1].set_title('Fraud Rate by Day of Week')\n",
    "axes[0, 1].set_ylabel('Fraud Rate (%)')\n",
    "axes[0, 1].set_xticks(range(len(days_order)))\n",
    "axes[0, 1].set_xticklabels([day[:3] for day in days_order])\n",
    "\n",
    "# 4. Transaction volume by hour\n",
    "hourly_volume = df.groupby('hour')['amount_numeric'].sum()\n",
    "axes[0, 2].bar(hourly_volume.index, hourly_volume.values, color='blue', alpha=0.7)\n",
    "axes[0, 2].set_title('Transaction Volume by Hour')\n",
    "axes[0, 2].set_xlabel('Hour of Day')\n",
    "axes[0, 2].set_ylabel('Volume ($)')\n",
    "\n",
    "# 5. Online vs Offline transactions\n",
    "channel_data = df['is_online'].value_counts()\n",
    "axes[1, 0].pie(channel_data.values, labels=['Offline', 'Online'], autopct='%1.1f%%', colors=['lightblue', 'orange'])\n",
    "axes[1, 0].set_title('Transaction Channels')\n",
    "\n",
    "# 6. Top states by volume\n",
    "top_states_volume = df.groupby('merchant_state')['amount_numeric'].sum().nlargest(10)\n",
    "axes[1, 1].barh(range(len(top_states_volume)), top_states_volume.values)\n",
    "axes[1, 1].set_title('Top 10 States by Volume')\n",
    "axes[1, 1].set_yticks(range(len(top_states_volume)))\n",
    "axes[1, 1].set_yticklabels(top_states_volume.index)\n",
    "axes[1, 1].set_xlabel('Volume ($)')\n",
    "\n",
    "# 7. Amount distribution\n",
    "axes[1, 2].hist(df['amount_numeric'], bins=50, alpha=0.7, color='green')\n",
    "axes[1, 2].set_title('Transaction Amount Distribution')\n",
    "axes[1, 2].set_xlabel('Amount ($)')\n",
    "axes[1, 2].set_ylabel('Frequency')\n",
    "axes[1, 2].set_yscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 🎯 Workshop Summary & Next Steps\n",
    "\n",
    "## ✅ What We Accomplished Today:\n",
    "\n",
    "### 1. **Regression Analysis (13:00-13:45)**\n",
    "- ✅ Customer Lifetime Value Prediction with Linear Regression\n",
    "- ✅ Fraud Detection using Logistic Regression\n",
    "- ✅ Model Evaluation: Precision, Recall, F1-Score, ROC-AUC\n",
    "\n",
    "### 2. **Unstructured Data Analytics (13:55-14:40)**\n",
    "- ✅ Transaction Description Text Mining\n",
    "- ✅ Sentiment Analysis for Risk Assessment\n",
    "- ✅ Text-based Feature Engineering\n",
    "\n",
    "### 3. **Data Visualization & Pandas Deep-Dive (14:50-15:40)**\n",
    "- ✅ Advanced Time Series Analysis\n",
    "- ✅ Banking KPI Dashboard Creation\n",
    "- ✅ Statistical Analysis and Correlation Studies\n",
    "\n",
    "## 🚀 **Key Skills Developed:**\n",
    "1. **Machine Learning:** Linear/Logistic Regression, Classification metrics\n",
    "2. **Text Analytics:** Text mining, Sentiment Analysis, Feature Engineering\n",
    "3. **Data Visualization:** Advanced Matplotlib/Seaborn, Dashboard Design\n",
    "4. **Pandas Mastery:** GroupBy, Time Series, Statistical Functions\n",
    "5. **Banking Domain:** Risk Assessment, Fraud Detection, Customer Analytics\n",
    "\n",
    "## 🎓 **Homework Challenges:**\n",
    "1. Complete all the TODO sections in the exercises\n",
    "2. Extend the fraud detection model with additional features\n",
    "3. Create a time-series forecasting model for transaction volumes\n",
    "4. Implement clustering algorithms for customer segmentation\n",
    "5. Build a Streamlit dashboard for interactive analytics\n",
    "\n",
    "## 📚 **Additional Resources:**\n",
    "- **Documentation:** pandas.pydata.org, scikit-learn.org\n",
    "- **Books:** \"Python for Data Analysis\" by Wes McKinney\n",
    "- **Practice:** Kaggle competitions on financial data\n",
    "- **Tools:** Apache Spark for big data processing\n",
    "\n",
    "**Great work today! You've built a comprehensive analytics pipeline for banking data! 🎉**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
