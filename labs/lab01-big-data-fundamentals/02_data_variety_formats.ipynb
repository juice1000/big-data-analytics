{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec5e9d0e",
   "metadata": {},
   "source": [
    "# Lab 01.2: Data Variety & Multi-Format Processing\n",
    "## Big Data Analytics Workshop - Banking Use Cases\n",
    "\n",
    "### üéØ **Learning Objectives**\n",
    "After completing this lab, you will understand:\n",
    "- The \"Variety\" dimension of Big Data (structured, semi-structured, unstructured data)\n",
    "- How different data formats impact processing strategies\n",
    "- Spark's unified approach to multi-format data processing\n",
    "- Real-world banking data integration challenges\n",
    "- Performance implications of different data formats\n",
    "\n",
    "### üìä **Banking Context: Multi-Format Data Reality**\n",
    "Modern banks process data from multiple sources and formats:\n",
    "- **Structured**: Core banking systems, transaction databases\n",
    "- **Semi-structured**: API responses, configuration files, logs\n",
    "- **Unstructured**: Customer emails, chat logs, regulatory documents\n",
    "\n",
    "### üõ† **Technical Skills Developed**\n",
    "- Multi-format data ingestion with Spark\n",
    "- Schema inference and evolution\n",
    "- Data format conversion and optimization\n",
    "- Performance comparison across formats\n",
    "- Banking data integration patterns\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4a44bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment Setup and Imports\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Add utils to path\n",
    "sys.path.append('../utils')\n",
    "from banking_data_generator import BankingDataGenerator\n",
    "from performance_monitor import PerformanceMonitor\n",
    "\n",
    "print(\"üöÄ Lab 01.2: Data Variety & Multi-Format Processing\")\n",
    "print(\"=\" * 60)\n",
    "print(\"üìä Focus: Structured, Semi-structured, and Unstructured Data\")\n",
    "print(\"üè¶ Context: Banking Data Integration\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a80bd19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Spark Session with Multi-Format Support\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Banking-Data-Variety-Lab\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"2g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Set log level to reduce noise\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "print(\"‚úÖ Spark Session Initialized\")\n",
    "print(f\"üéØ Spark Version: {spark.version}\")\n",
    "print(f\"üíª Application Name: {spark.sparkContext.appName}\")\n",
    "print(f\"üßÆ Available Cores: {spark.sparkContext.defaultParallelism}\")\n",
    "\n",
    "# Initialize utilities\n",
    "data_generator = BankingDataGenerator(seed=42)\n",
    "performance_monitor = PerformanceMonitor()\n",
    "\n",
    "print(\"\\nüõ† Utilities Ready:\")\n",
    "print(\"   üìä Banking Data Generator\")\n",
    "print(\"   üìà Performance Monitor\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b0b662",
   "metadata": {},
   "source": [
    "## 1. Understanding Data Variety in Banking\n",
    "\n",
    "### üìö **The Three Types of Data**\n",
    "\n",
    "**Structured Data** üóÉÔ∏è\n",
    "- Fixed schema (rows/columns)\n",
    "- Examples: Transaction tables, customer records, account balances\n",
    "- Formats: CSV, Parquet, SQL databases\n",
    "- ~20% of banking data\n",
    "\n",
    "**Semi-Structured Data** üìã\n",
    "- Flexible schema with some organization\n",
    "- Examples: JSON API responses, XML files, log files\n",
    "- Formats: JSON, XML, YAML, Avro\n",
    "- ~10% of banking data\n",
    "\n",
    "**Unstructured Data** üìÑ\n",
    "- No predefined schema\n",
    "- Examples: Customer emails, chat transcripts, documents, images\n",
    "- Formats: Text files, PDFs, emails, audio, video\n",
    "- ~70% of banking data\n",
    "\n",
    "### üè¶ **Banking Data Integration Challenge**\n",
    "Banks must combine all three types to get complete customer insights!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e718d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Sample Data in Multiple Formats\n",
    "print(\"üîÑ Generating banking data in multiple formats...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 1. Generate core datasets\n",
    "customers = data_generator.generate_customers(5000)\n",
    "transactions = data_generator.generate_transactions(50000, customers)\n",
    "market_data = data_generator.generate_market_data(365)\n",
    "unstructured_data = data_generator.generate_unstructured_data(1000)\n",
    "\n",
    "print(f\"‚úÖ Generated datasets:\")\n",
    "print(f\"   üë• Customers: {len(customers):,}\")\n",
    "print(f\"   üí≥ Transactions: {len(transactions):,}\")\n",
    "print(f\"   üìà Market Data: {len(market_data):,}\")\n",
    "print(f\"   üìÑ Unstructured: {len(unstructured_data):,}\")\n",
    "\n",
    "# 2. Create sample directory\n",
    "sample_dir = \"../data/variety_sample\"\n",
    "os.makedirs(sample_dir, exist_ok=True)\n",
    "print(f\"\\nüìÅ Sample data directory: {sample_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86720bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Data in Different Formats\n",
    "print(\"üíæ Saving data in multiple formats...\")\n",
    "\n",
    "# Convert to DataFrames for easier handling\n",
    "customers_df = pd.DataFrame(customers)\n",
    "transactions_df = pd.DataFrame(transactions)\n",
    "market_df = pd.DataFrame(market_data)\n",
    "\n",
    "# 1. STRUCTURED DATA FORMATS\n",
    "print(\"\\n1Ô∏è‚É£ Structured Data Formats:\")\n",
    "\n",
    "# CSV Format\n",
    "customers_df.to_csv(f\"{sample_dir}/customers.csv\", index=False)\n",
    "transactions_df.to_csv(f\"{sample_dir}/transactions.csv\", index=False)\n",
    "print(\"   ‚úÖ CSV files saved\")\n",
    "\n",
    "# Parquet Format (columnar, optimized)\n",
    "customers_df.to_parquet(f\"{sample_dir}/customers.parquet\")\n",
    "transactions_df.to_parquet(f\"{sample_dir}/transactions.parquet\")\n",
    "print(\"   ‚úÖ Parquet files saved\")\n",
    "\n",
    "# 2. SEMI-STRUCTURED DATA FORMATS\n",
    "print(\"\\n2Ô∏è‚É£ Semi-Structured Data Formats:\")\n",
    "\n",
    "# JSON Format\n",
    "with open(f\"{sample_dir}/customers.json\", 'w') as f:\n",
    "    json.dump(customers[:100], f, indent=2, default=str)  # Sample subset\n",
    "\n",
    "with open(f\"{sample_dir}/transactions.json\", 'w') as f:\n",
    "    json.dump(transactions[:1000], f, indent=2, default=str)  # Sample subset\n",
    "\n",
    "with open(f\"{sample_dir}/market_data.json\", 'w') as f:\n",
    "    json.dump(market_data, f, indent=2, default=str)\n",
    "print(\"   ‚úÖ JSON files saved\")\n",
    "\n",
    "# NDJSON (Newline Delimited JSON) - Common in streaming\n",
    "with open(f\"{sample_dir}/transactions.ndjson\", 'w') as f:\n",
    "    for transaction in transactions[:1000]:\n",
    "        f.write(json.dumps(transaction, default=str) + '\\n')\n",
    "print(\"   ‚úÖ NDJSON files saved\")\n",
    "\n",
    "# 3. UNSTRUCTURED DATA FORMATS\n",
    "print(\"\\n3Ô∏è‚É£ Unstructured Data Formats:\")\n",
    "\n",
    "# Text logs (simulated banking system logs)\n",
    "with open(f\"{sample_dir}/system_logs.txt\", 'w') as f:\n",
    "    for i, record in enumerate(unstructured_data[:500]):\n",
    "        if record['type'] == 'email':\n",
    "            f.write(f\"[{record['timestamp']}] EMAIL - Customer: {record['customer_id']} - Subject: {record['subject'][:50]}...\\n\")\n",
    "        elif record['type'] == 'chat_log':\n",
    "            f.write(f\"[{record['timestamp']}] CHAT - Customer: {record['customer_id']} - Duration: {record['session_duration']}s\\n\")\n",
    "        else:\n",
    "            f.write(f\"[{record['timestamp']}] CALL - Customer: {record['customer_id']} - Duration: {record['duration']}s\\n\")\n",
    "print(\"   ‚úÖ Text logs saved\")\n",
    "\n",
    "# Customer feedback (unstructured text)\n",
    "with open(f\"{sample_dir}/customer_feedback.txt\", 'w') as f:\n",
    "    feedback_samples = [\n",
    "        \"Great service! Very satisfied with the new mobile app features.\",\n",
    "        \"ATM was out of order for three days. Poor maintenance.\",\n",
    "        \"Credit card approval process was very fast and efficient.\",\n",
    "        \"Online banking system is slow during peak hours.\",\n",
    "        \"Excellent customer support. Representative was very helpful.\",\n",
    "        \"Transaction fees are too high compared to other banks.\",\n",
    "        \"New security features give me peace of mind.\",\n",
    "        \"Mortgage application process needs improvement.\"\n",
    "    ]\n",
    "    for i, feedback in enumerate(feedback_samples * 50):  # Repeat to create larger dataset\n",
    "        f.write(f\"Feedback-{i+1:04d}: {feedback}\\n\")\n",
    "\n",
    "print(\"   ‚úÖ Unstructured feedback saved\")\n",
    "\n",
    "print(f\"\\nüìä Data files created in: {sample_dir}\")\n",
    "\n",
    "# List all created files with sizes\n",
    "for file in os.listdir(sample_dir):\n",
    "    file_path = os.path.join(sample_dir, file)\n",
    "    size_mb = os.path.getsize(file_path) / (1024 * 1024)\n",
    "    print(f\"   üìÑ {file}: {size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e699b2",
   "metadata": {},
   "source": [
    "## 2. Structured Data Processing with Spark\n",
    "\n",
    "### üìã **CSV vs Parquet: The Great Format Debate**\n",
    "\n",
    "**CSV Characteristics:**\n",
    "- ‚úÖ Human-readable\n",
    "- ‚úÖ Universal compatibility\n",
    "- ‚ùå No schema enforcement\n",
    "- ‚ùå No compression\n",
    "- ‚ùå Row-based (inefficient for analytics)\n",
    "\n",
    "**Parquet Characteristics:**\n",
    "- ‚úÖ Schema enforcement\n",
    "- ‚úÖ Excellent compression\n",
    "- ‚úÖ Columnar format (analytics-optimized)\n",
    "- ‚úÖ Predicate pushdown\n",
    "- ‚ùå Not human-readable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6db8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance Comparison: CSV vs Parquet\n",
    "print(\"üèÅ Performance Comparison: CSV vs Parquet\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "def read_csv_transactions():\n",
    "    \"\"\"Read transactions from CSV format\"\"\"\n",
    "    df = spark.read \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .option(\"inferSchema\", \"true\") \\\n",
    "        .csv(f\"{sample_dir}/transactions.csv\")\n",
    "    \n",
    "    # Perform analytics operation\n",
    "    result = df.groupBy(\"merchant_category\") \\\n",
    "               .agg(count(\"*\").alias(\"transaction_count\"),\n",
    "                   sum(\"amount\").alias(\"total_amount\"),\n",
    "                   avg(\"amount\").alias(\"avg_amount\")) \\\n",
    "               .orderBy(desc(\"total_amount\"))\n",
    "    \n",
    "    return result.collect()\n",
    "\n",
    "def read_parquet_transactions():\n",
    "    \"\"\"Read transactions from Parquet format\"\"\"\n",
    "    df = spark.read.parquet(f\"{sample_dir}/transactions.parquet\")\n",
    "    \n",
    "    # Same analytics operation\n",
    "    result = df.groupBy(\"merchant_category\") \\\n",
    "               .agg(count(\"*\").alias(\"transaction_count\"),\n",
    "                   sum(\"amount\").alias(\"total_amount\"),\n",
    "                   avg(\"amount\").alias(\"avg_amount\")) \\\n",
    "               .orderBy(desc(\"total_amount\"))\n",
    "    \n",
    "    return result.collect()\n",
    "\n",
    "# Run performance comparison\n",
    "approaches = [\n",
    "    {\n",
    "        \"name\": \"CSV Format\",\n",
    "        \"function\": read_csv_transactions\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Parquet Format\", \n",
    "        \"function\": read_parquet_transactions\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"üîÑ Running format comparison...\")\n",
    "for approach in approaches:\n",
    "    result = performance_monitor.benchmark_function(\n",
    "        approach[\"function\"],\n",
    "        approach[\"name\"]\n",
    "    )\n",
    "    \n",
    "    if result[\"result\"]:\n",
    "        print(f\"   üìä {approach['name']}: {len(result['result'])} categories analyzed\")\n",
    "\n",
    "print(\"\\nüí° Key Insights:\")\n",
    "print(\"   ‚Ä¢ Parquet typically 2-5x faster for analytics queries\")\n",
    "print(\"   ‚Ä¢ Parquet files are 60-80% smaller due to compression\")\n",
    "print(\"   ‚Ä¢ CSV better for data exchange and debugging\")\n",
    "print(\"   ‚Ä¢ Parquet optimized for column-oriented operations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bff9279",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schema Analysis and Enforcement\n",
    "print(\"üîç Schema Analysis: CSV vs Parquet\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Load both formats\n",
    "csv_df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(f\"{sample_dir}/transactions.csv\")\n",
    "\n",
    "parquet_df = spark.read.parquet(f\"{sample_dir}/transactions.parquet\")\n",
    "\n",
    "print(\"üìã CSV Schema (inferred):\")\n",
    "csv_df.printSchema()\n",
    "\n",
    "print(\"\\nüìã Parquet Schema (stored):\")\n",
    "parquet_df.printSchema()\n",
    "\n",
    "print(\"\\nüî¨ Schema Comparison:\")\n",
    "csv_schema = {field.name: str(field.dataType) for field in csv_df.schema.fields}\n",
    "parquet_schema = {field.name: str(field.dataType) for field in parquet_df.schema.fields}\n",
    "\n",
    "schema_matches = all(csv_schema.get(field) == parquet_schema.get(field) for field in parquet_schema.keys())\n",
    "print(f\"   Schema consistency: {'‚úÖ Match' if schema_matches else '‚ùå Mismatch'}\")\n",
    "\n",
    "# Show data type differences if any\n",
    "print(\"\\nüìä Data Types:\")\n",
    "for field in parquet_df.schema.fields:\n",
    "    csv_type = csv_schema.get(field.name, \"Missing\")\n",
    "    parquet_type = str(field.dataType)\n",
    "    status = \"‚úÖ\" if csv_type == parquet_type else \"‚ö†Ô∏è\"\n",
    "    print(f\"   {status} {field.name}: CSV={csv_type}, Parquet={parquet_type}\")\n",
    "\n",
    "# Demonstrate schema evolution challenge\n",
    "print(f\"\\nüí° Key Points:\")\n",
    "print(f\"   ‚Ä¢ CSV schema inference can be inconsistent\")\n",
    "print(f\"   ‚Ä¢ Parquet enforces schema at write time\")\n",
    "print(f\"   ‚Ä¢ Type mismatches can cause processing errors\")\n",
    "print(f\"   ‚Ä¢ Schema evolution easier with Parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49eb562c",
   "metadata": {},
   "source": [
    "## 3. Semi-Structured Data: JSON Processing\n",
    "\n",
    "### üîß **JSON in Banking: API Responses & Configuration**\n",
    "\n",
    "JSON is everywhere in modern banking:\n",
    "- **API responses** from payment processors\n",
    "- **Configuration files** for banking applications  \n",
    "- **Log entries** from web services\n",
    "- **Mobile app** transaction data\n",
    "\n",
    "### üí° **Spark's JSON Capabilities**\n",
    "- Automatic schema inference\n",
    "- Nested structure handling\n",
    "- Complex data type support\n",
    "- Integration with DataFrame API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83cdd57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# JSON Data Processing with Spark\n",
    "print(\"üìÑ Processing JSON Banking Data\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# 1. Read JSON transaction data\n",
    "json_df = spark.read.json(f\"{sample_dir}/transactions.json\")\n",
    "\n",
    "print(\"üìã JSON Schema (auto-inferred):\")\n",
    "json_df.printSchema()\n",
    "\n",
    "print(f\"\\nüìä JSON DataFrame Info:\")\n",
    "print(f\"   Rows: {json_df.count():,}\")\n",
    "print(f\"   Columns: {len(json_df.columns)}\")\n",
    "\n",
    "# 2. Read Market Data JSON\n",
    "market_json_df = spark.read.json(f\"{sample_dir}/market_data.json\")\n",
    "\n",
    "print(f\"\\nüìà Market Data JSON:\")\n",
    "print(f\"   Rows: {market_json_df.count():,}\")\n",
    "print(f\"   Columns: {len(market_json_df.columns)}\")\n",
    "market_json_df.show(5, truncate=False)\n",
    "\n",
    "# 3. Complex JSON Operations\n",
    "print(\"\\nüîß JSON Data Transformations:\")\n",
    "\n",
    "# Extract date components from JSON timestamps\n",
    "json_enhanced = json_df.withColumn(\"transaction_date\", \n",
    "                                  to_date(col(\"timestamp\"))) \\\n",
    "                       .withColumn(\"transaction_hour\", \n",
    "                                  hour(col(\"timestamp\")))\n",
    "\n",
    "# Group by date and calculate daily metrics\n",
    "daily_metrics = json_enhanced.groupBy(\"transaction_date\") \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"daily_transactions\"),\n",
    "        sum(\"amount\").alias(\"daily_volume\"),\n",
    "        avg(\"amount\").alias(\"avg_transaction_amount\"),\n",
    "        countDistinct(\"customer_id\").alias(\"unique_customers\")\n",
    "    ) \\\n",
    "    .orderBy(\"transaction_date\")\n",
    "\n",
    "print(\"üìä Daily Transaction Metrics from JSON:\")\n",
    "daily_metrics.show(10)\n",
    "\n",
    "# 4. JSON vs Structured Performance\n",
    "print(\"\\n‚ö° Performance: JSON vs Structured Formats\")\n",
    "\n",
    "def process_json_data():\n",
    "    df = spark.read.json(f\"{sample_dir}/transactions.json\")\n",
    "    return df.groupBy(\"merchant_category\").count().collect()\n",
    "\n",
    "def process_parquet_data():\n",
    "    df = spark.read.parquet(f\"{sample_dir}/transactions.parquet\")\n",
    "    return df.groupBy(\"merchant_category\").count().collect()\n",
    "\n",
    "# Compare processing times\n",
    "json_result = performance_monitor.benchmark_function(process_json_data, \"JSON Processing\")\n",
    "parquet_result = performance_monitor.benchmark_function(process_parquet_data, \"Parquet Processing\")\n",
    "\n",
    "json_time = json_result[\"metrics\"][\"execution_time\"]\n",
    "parquet_time = parquet_result[\"metrics\"][\"execution_time\"]\n",
    "performance_ratio = json_time / parquet_time\n",
    "\n",
    "print(f\"\\nüìà Performance Comparison:\")\n",
    "print(f\"   JSON Time: {json_time:.4f}s\")\n",
    "print(f\"   Parquet Time: {parquet_time:.4f}s\") \n",
    "print(f\"   JSON is {performance_ratio:.1f}x slower than Parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ff6038",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NDJSON Processing (Common for Streaming)\n",
    "print(\"üåä NDJSON Processing for Streaming Data\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# NDJSON (Newline Delimited JSON) is common in streaming/logging systems\n",
    "ndjson_df = spark.read.json(f\"{sample_dir}/transactions.ndjson\")\n",
    "\n",
    "print(f\"üìä NDJSON Data:\")\n",
    "print(f\"   Format: One JSON object per line\")\n",
    "print(f\"   Use case: Streaming, log processing, real-time analytics\")\n",
    "print(f\"   Records: {ndjson_df.count():,}\")\n",
    "\n",
    "# Real-time analytics simulation\n",
    "print(\"\\nüìä Real-time Banking Analytics from NDJSON:\")\n",
    "\n",
    "# Fraud detection simulation\n",
    "fraud_indicators = ndjson_df.select(\n",
    "    \"transaction_id\",\n",
    "    \"customer_id\", \n",
    "    \"amount\",\n",
    "    \"timestamp\",\n",
    "    \"channel\",\n",
    "    # Flag high-value transactions\n",
    "    when(col(\"amount\") > 1000, 1).otherwise(0).alias(\"high_value_flag\"),\n",
    "    # Flag off-hours transactions\n",
    "    when(hour(col(\"timestamp\")).isin([22, 23, 0, 1, 2, 3, 4, 5]), 1).otherwise(0).alias(\"off_hours_flag\"),\n",
    "    # Flag foreign transactions\n",
    "    when(col(\"currency\") != \"EUR\", 1).otherwise(0).alias(\"foreign_currency_flag\")\n",
    ")\n",
    "\n",
    "# Calculate composite risk score\n",
    "risk_analysis = fraud_indicators.withColumn(\n",
    "    \"risk_score\", \n",
    "    col(\"high_value_flag\") + col(\"off_hours_flag\") + col(\"foreign_currency_flag\")\n",
    ")\n",
    "\n",
    "# Show high-risk transactions\n",
    "high_risk = risk_analysis.filter(col(\"risk_score\") >= 2).orderBy(desc(\"risk_score\"))\n",
    "\n",
    "print(\"üö® High-Risk Transactions Detected:\")\n",
    "high_risk.show(10, truncate=False)\n",
    "\n",
    "print(f\"üìà Risk Summary:\")\n",
    "risk_summary = risk_analysis.groupBy(\"risk_score\").count().orderBy(\"risk_score\")\n",
    "risk_summary.show()\n",
    "\n",
    "print(f\"‚ö†Ô∏è High-risk transactions: {high_risk.count():,} out of {ndjson_df.count():,} ({(high_risk.count()/ndjson_df.count()*100):.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299ce39c",
   "metadata": {},
   "source": [
    "## 4. Unstructured Data: Text Analytics\n",
    "\n",
    "### üìÑ **The Challenge of Unstructured Banking Data**\n",
    "\n",
    "70% of banking data is unstructured:\n",
    "- **Customer communications** (emails, chat logs)\n",
    "- **Regulatory documents** (compliance reports)\n",
    "- **Call center transcripts**\n",
    "- **Social media mentions**\n",
    "- **Financial news** and market reports\n",
    "\n",
    "### üß† **Text Analytics for Banking**\n",
    "- Sentiment analysis of customer feedback\n",
    "- Topic modeling for customer complaints  \n",
    "- Named entity recognition for compliance\n",
    "- Document classification and routing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3657bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Analytics on Banking Data\n",
    "print(\"üìÑ Processing Unstructured Banking Text Data\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 1. Load system logs as text data\n",
    "logs_df = spark.read.text(f\"{sample_dir}/system_logs.txt\")\n",
    "print(f\"üìã System Logs:\")\n",
    "print(f\"   Total lines: {logs_df.count():,}\")\n",
    "logs_df.show(5, truncate=False)\n",
    "\n",
    "# 2. Parse log entries using regex\n",
    "from pyspark.sql.functions import regexp_extract, split, trim\n",
    "\n",
    "parsed_logs = logs_df.select(\n",
    "    regexp_extract(\"value\", r'\\[(.*?)\\]', 1).alias(\"timestamp\"),\n",
    "    regexp_extract(\"value\", r'\\] (\\w+) -', 1).alias(\"event_type\"),\n",
    "    regexp_extract(\"value\", r'Customer: (\\w+)', 1).alias(\"customer_id\"),\n",
    "    regexp_extract(\"value\", r'Duration: (\\d+)', 1).cast(\"integer\").alias(\"duration\"),\n",
    "    \"value\"\n",
    ").filter(col(\"customer_id\") != \"\")\n",
    "\n",
    "print(\"\\nüìä Parsed Log Entries:\")\n",
    "parsed_logs.show(5, truncate=False)\n",
    "\n",
    "# 3. Analyze log patterns\n",
    "print(\"\\nüìà Log Analysis:\")\n",
    "event_counts = parsed_logs.groupBy(\"event_type\").count().orderBy(desc(\"count\"))\n",
    "print(\"Event Type Distribution:\")\n",
    "event_counts.show()\n",
    "\n",
    "# Customer activity analysis\n",
    "customer_activity = parsed_logs.groupBy(\"customer_id\") \\\n",
    "    .agg(count(\"*\").alias(\"total_events\"),\n",
    "         countDistinct(\"event_type\").alias(\"event_types\"),\n",
    "         avg(\"duration\").alias(\"avg_duration\")) \\\n",
    "    .orderBy(desc(\"total_events\"))\n",
    "\n",
    "print(\"Top Active Customers:\")\n",
    "customer_activity.show(10)\n",
    "\n",
    "# 4. Customer Feedback Sentiment Analysis (Simple)\n",
    "feedback_df = spark.read.text(f\"{sample_dir}/customer_feedback.txt\")\n",
    "\n",
    "print(f\"\\nüí¨ Customer Feedback Analysis:\")\n",
    "print(f\"   Total feedback entries: {feedback_df.count():,}\")\n",
    "\n",
    "# Simple sentiment analysis using keyword matching\n",
    "positive_keywords = [\"great\", \"excellent\", \"satisfied\", \"fast\", \"efficient\", \"helpful\", \"peace\"]\n",
    "negative_keywords = [\"poor\", \"slow\", \"out of order\", \"too high\", \"needs improvement\"]\n",
    "\n",
    "# Create sentiment flags\n",
    "sentiment_analysis = feedback_df.select(\n",
    "    \"value\",\n",
    "    # Count positive words\n",
    "    sum([when(lower(col(\"value\")).contains(word), 1).otherwise(0) for word in positive_keywords]).alias(\"positive_score\"),\n",
    "    # Count negative words  \n",
    "    sum([when(lower(col(\"value\")).contains(word), 1).otherwise(0) for word in negative_keywords]).alias(\"negative_score\")\n",
    ").withColumn(\n",
    "    \"sentiment\",\n",
    "    when(col(\"positive_score\") > col(\"negative_score\"), \"positive\")\n",
    "    .when(col(\"positive_score\") < col(\"negative_score\"), \"negative\") \n",
    "    .otherwise(\"neutral\")\n",
    ")\n",
    "\n",
    "print(\"üìä Sentiment Analysis Results:\")\n",
    "sentiment_summary = sentiment_analysis.groupBy(\"sentiment\").count()\n",
    "sentiment_summary.show()\n",
    "\n",
    "# Show examples of each sentiment\n",
    "print(\"\\nüí° Sample Feedback by Sentiment:\")\n",
    "for sentiment in [\"positive\", \"negative\", \"neutral\"]:\n",
    "    print(f\"\\n{sentiment.upper()} Examples:\")\n",
    "    examples = sentiment_analysis.filter(col(\"sentiment\") == sentiment).limit(3)\n",
    "    for row in examples.collect():\n",
    "        print(f\"   ‚Ä¢ {row.value[:80]}...\")\n",
    "\n",
    "# 5. Topic Analysis (Simple keyword extraction)\n",
    "print(\"\\nüè∑ Topic Analysis - Key Themes:\")\n",
    "\n",
    "# Define banking topics\n",
    "topics = {\n",
    "    \"mobile_app\": [\"mobile\", \"app\", \"application\"],\n",
    "    \"atm_services\": [\"atm\", \"cash\", \"withdrawal\", \"machine\"],\n",
    "    \"customer_service\": [\"service\", \"support\", \"representative\", \"help\"],\n",
    "    \"fees_charges\": [\"fee\", \"charge\", \"cost\", \"expensive\", \"high\"],\n",
    "    \"online_banking\": [\"online\", \"website\", \"internet\", \"digital\"],\n",
    "    \"credit_mortgage\": [\"credit\", \"mortgage\", \"loan\", \"approval\"],\n",
    "    \"security\": [\"security\", \"fraud\", \"safe\", \"protection\"]\n",
    "}\n",
    "\n",
    "# Calculate topic scores\n",
    "topic_scores = feedback_df\n",
    "for topic, keywords in topics.items():\n",
    "    topic_scores = topic_scores.withColumn(\n",
    "        f\"{topic}_score\",\n",
    "        sum([when(lower(col(\"value\")).contains(word), 1).otherwise(0) for word in keywords])\n",
    "    )\n",
    "\n",
    "# Aggregate topic mentions\n",
    "topic_summary = topic_scores.select(*[sum(col(f\"{topic}_score\")).alias(topic) for topic in topics.keys()])\n",
    "\n",
    "print(\"üìä Banking Topics Mentioned:\")\n",
    "topic_results = topic_summary.collect()[0].asDict()\n",
    "sorted_topics = sorted(topic_results.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "for topic, count in sorted_topics:\n",
    "    print(f\"   {topic.replace('_', ' ').title()}: {count} mentions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce57ec0",
   "metadata": {},
   "source": [
    "## 5. Multi-Format Data Integration\n",
    "\n",
    "### üîó **The Banking Data Integration Challenge**\n",
    "\n",
    "Real banking analytics requires combining:\n",
    "- **Transaction data** (structured) from core banking systems\n",
    "- **Market data** (semi-structured) from external APIs\n",
    "- **Customer feedback** (unstructured) from various channels\n",
    "- **System logs** (semi-structured) for operational insights\n",
    "\n",
    "### üí° **Spark's Unified Data Processing**\n",
    "Spark provides a single API to:\n",
    "- Read from multiple data sources\n",
    "- Apply consistent transformations\n",
    "- Join across different formats\n",
    "- Output to optimal formats for downstream use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c57ea10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive Multi-Format Data Integration\n",
    "print(\"üîó Banking Data Integration: Combining All Formats\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "# 1. Load all data formats\n",
    "print(\"üìä Loading data from multiple formats...\")\n",
    "\n",
    "# Structured data\n",
    "transactions_df = spark.read.parquet(f\"{sample_dir}/transactions.parquet\")\n",
    "customers_df = spark.read.parquet(f\"{sample_dir}/customers.parquet\")\n",
    "\n",
    "# Semi-structured data\n",
    "market_data_df = spark.read.json(f\"{sample_dir}/market_data.json\")\n",
    "logs_parsed_df = parsed_logs  # From previous cell\n",
    "\n",
    "# Processed unstructured data\n",
    "feedback_sentiment_df = sentiment_analysis\n",
    "\n",
    "print(\"‚úÖ All data sources loaded\")\n",
    "print(f\"   üìä Transactions: {transactions_df.count():,} records\")\n",
    "print(f\"   üë• Customers: {customers_df.count():,} records\")\n",
    "print(f\"   üìà Market Data: {market_data_df.count():,} records\")\n",
    "print(f\"   üìã Logs: {logs_parsed_df.count():,} records\")\n",
    "print(f\"   üí¨ Feedback: {feedback_sentiment_df.count():,} records\")\n",
    "\n",
    "# 2. Create a unified customer 360 view\n",
    "print(f\"\\nüéØ Creating Unified Customer 360 View...\")\n",
    "\n",
    "# Start with customer base data\n",
    "customer_360 = customers_df\n",
    "\n",
    "# Add transaction metrics\n",
    "transaction_metrics = transactions_df.groupBy(\"customer_id\").agg(\n",
    "    count(\"*\").alias(\"total_transactions\"),\n",
    "    sum(\"amount\").alias(\"total_spent\"),\n",
    "    avg(\"amount\").alias(\"avg_transaction_amount\"),\n",
    "    max(\"amount\").alias(\"max_transaction\"),\n",
    "    countDistinct(\"merchant\").alias(\"unique_merchants\"),\n",
    "    countDistinct(\"channel\").alias(\"channels_used\")\n",
    ")\n",
    "\n",
    "customer_360 = customer_360.join(transaction_metrics, \"customer_id\", \"left\")\n",
    "\n",
    "# Add activity metrics from logs\n",
    "activity_metrics = logs_parsed_df.groupBy(\"customer_id\").agg(\n",
    "    count(\"*\").alias(\"system_interactions\"),\n",
    "    countDistinct(\"event_type\").alias(\"interaction_types\"),\n",
    "    avg(\"duration\").alias(\"avg_interaction_duration\")\n",
    ")\n",
    "\n",
    "customer_360 = customer_360.join(activity_metrics, \"customer_id\", \"left\")\n",
    "\n",
    "# Create customer segments based on multiple data sources\n",
    "customer_360_segmented = customer_360.withColumn(\n",
    "    \"customer_tier\",\n",
    "    when(col(\"total_spent\") > 50000, \"VIP\")\n",
    "    .when(col(\"total_spent\") > 20000, \"Premium\")\n",
    "    .when(col(\"total_spent\") > 5000, \"Standard\")\n",
    "    .otherwise(\"Basic\")\n",
    ").withColumn(\n",
    "    \"digital_engagement\",\n",
    "    when(col(\"channels_used\") >= 3, \"High\")\n",
    "    .when(col(\"channels_used\") >= 2, \"Medium\")\n",
    "    .otherwise(\"Low\")\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Customer 360 view created\")\n",
    "print(f\"   üìä Integrated customers: {customer_360_segmented.count():,}\")\n",
    "\n",
    "# Show sample of integrated data\n",
    "print(\"\\nüìã Sample Customer 360 View:\")\n",
    "customer_360_segmented.select(\n",
    "    \"customer_id\", \"customer_segment\", \"customer_tier\", \"digital_engagement\",\n",
    "    \"total_spent\", \"total_transactions\", \"system_interactions\"\n",
    ").show(10)\n",
    "\n",
    "# 3. Market-influenced analytics\n",
    "print(\"\\nüìà Market Data Integration for Risk Analysis...\")\n",
    "\n",
    "# Get market data for risk correlation\n",
    "market_stats = market_data_df.agg(\n",
    "    avg(\"dax_index\").alias(\"avg_dax\"),\n",
    "    avg(\"market_volatility\").alias(\"avg_volatility\"),\n",
    "    avg(\"interest_rate_ecb\").alias(\"avg_interest_rate\")\n",
    ").collect()[0]\n",
    "\n",
    "print(f\"üìä Market Context:\")\n",
    "print(f\"   Average DAX: {market_stats.avg_dax:.2f}\")\n",
    "print(f\"   Average Volatility: {market_stats.avg_volatility:.2f}%\")\n",
    "print(f\"   Average Interest Rate: {market_stats.avg_interest_rate:.2f}%\")\n",
    "\n",
    "# Classify customers based on market conditions and behavior\n",
    "risk_profile = customer_360_segmented.withColumn(\n",
    "    \"market_risk_profile\",\n",
    "    when((col(\"max_transaction\") > 10000) & (market_stats.avg_volatility > 20), \"High Risk\")\n",
    "    .when((col(\"total_spent\") > 30000) & (col(\"channels_used\") == 1), \"Medium Risk\")\n",
    "    .otherwise(\"Low Risk\")\n",
    ")\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è Customer Risk Distribution:\")\n",
    "risk_distribution = risk_profile.groupBy(\"market_risk_profile\").count().orderBy(desc(\"count\"))\n",
    "risk_distribution.show()\n",
    "\n",
    "# 4. Performance comparison: Integrated vs Single-format queries\n",
    "print(\"\\n‚ö° Performance Analysis: Multi-Format Integration\")\n",
    "\n",
    "def single_format_analysis():\n",
    "    \"\"\"Query using only transaction data\"\"\"\n",
    "    return transactions_df.groupBy(\"customer_id\") \\\n",
    "        .agg(sum(\"amount\").alias(\"total_spent\")) \\\n",
    "        .orderBy(desc(\"total_spent\")) \\\n",
    "        .limit(100) \\\n",
    "        .collect()\n",
    "\n",
    "def integrated_analysis():\n",
    "    \"\"\"Query using integrated multi-format data\"\"\"\n",
    "    return risk_profile.select(\"customer_id\", \"total_spent\", \"market_risk_profile\") \\\n",
    "        .orderBy(desc(\"total_spent\")) \\\n",
    "        .limit(100) \\\n",
    "        .collect()\n",
    "\n",
    "# Benchmark both approaches\n",
    "single_result = performance_monitor.benchmark_function(\n",
    "    single_format_analysis, \n",
    "    \"Single Format Analysis\"\n",
    ")\n",
    "\n",
    "integrated_result = performance_monitor.benchmark_function(\n",
    "    integrated_analysis,\n",
    "    \"Multi-Format Integration\"\n",
    ")\n",
    "\n",
    "print(f\"\\nüìä Integration Performance Impact:\")\n",
    "integration_overhead = integrated_result[\"metrics\"][\"execution_time\"] / single_result[\"metrics\"][\"execution_time\"]\n",
    "print(f\"   Integration overhead: {integration_overhead:.2f}x\")\n",
    "print(f\"   Trade-off: {integration_overhead:.1f}x slower but {len(risk_profile.columns) - len(transactions_df.columns)} additional insights\")\n",
    "\n",
    "# 5. Data Quality Assessment Across Formats\n",
    "print(f\"\\nüîç Data Quality Assessment Across Formats:\")\n",
    "\n",
    "formats_quality = {\n",
    "    \"Structured (Parquet)\": {\n",
    "        \"completeness\": transactions_df.count() / transactions_df.count() * 100,\n",
    "        \"consistency\": \"High - Schema enforced\",\n",
    "        \"timeliness\": \"Real-time capable\"\n",
    "    },\n",
    "    \"Semi-Structured (JSON)\": {\n",
    "        \"completeness\": market_data_df.count() / 365 * 100,  # Expected daily records\n",
    "        \"consistency\": \"Medium - Schema flexibility\",\n",
    "        \"timeliness\": \"Daily updates\"\n",
    "    },\n",
    "    \"Unstructured (Text)\": {\n",
    "        \"completeness\": feedback_sentiment_df.count() / feedback_sentiment_df.count() * 100,\n",
    "        \"consistency\": \"Low - Free text\",\n",
    "        \"timeliness\": \"Batch processing\"\n",
    "    }\n",
    "}\n",
    "\n",
    "for format_type, quality in formats_quality.items():\n",
    "    print(f\"\\n   üìä {format_type}:\")\n",
    "    print(f\"      Completeness: {quality['completeness']:.1f}%\")\n",
    "    print(f\"      Consistency: {quality['consistency']}\")\n",
    "    print(f\"      Timeliness: {quality['timeliness']}\")\n",
    "\n",
    "print(f\"\\nüí° Integration Best Practices:\")\n",
    "print(f\"   ‚Ä¢ Use Parquet for analytical workloads\")\n",
    "print(f\"   ‚Ä¢ JSON for flexible schema evolution\")\n",
    "print(f\"   ‚Ä¢ Process unstructured data into structured insights\")\n",
    "print(f\"   ‚Ä¢ Cache frequently joined datasets\")\n",
    "print(f\"   ‚Ä¢ Monitor data quality across all sources\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52870eb9",
   "metadata": {},
   "source": [
    "## üéØ Lab Summary & Key Takeaways\n",
    "\n",
    "### üìä **What We Accomplished**\n",
    "\n",
    "1. **Multi-Format Data Processing**: Successfully processed structured (CSV/Parquet), semi-structured (JSON), and unstructured (text) banking data\n",
    "2. **Performance Analysis**: Compared processing speeds across different formats\n",
    "3. **Schema Management**: Understood schema inference, enforcement, and evolution\n",
    "4. **Text Analytics**: Applied basic NLP techniques to customer feedback\n",
    "5. **Data Integration**: Created unified customer views combining multiple data sources\n",
    "\n",
    "### üè¶ **Banking Industry Insights**\n",
    "\n",
    "- **70%** of banking data is unstructured (customer communications, documents)\n",
    "- **Parquet format** provides 2-5x better performance for analytics workloads\n",
    "- **Multi-format integration** enables comprehensive customer 360¬∞ views\n",
    "- **Real-time processing** requires different strategies for different data types\n",
    "\n",
    "### üõ† **Technical Skills Developed**\n",
    "\n",
    "- ‚úÖ Multi-format data ingestion with Spark\n",
    "- ‚úÖ Schema inference and management\n",
    "- ‚úÖ Text processing and sentiment analysis\n",
    "- ‚úÖ Cross-format data joins and integration\n",
    "- ‚úÖ Performance optimization strategies\n",
    "\n",
    "### üéØ **Next Steps**\n",
    "\n",
    "In **Lab 01.3**, we'll focus on data quality assessment and cleansing techniques for banking data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf17a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup and Resource Management\n",
    "print(\"üßπ Cleaning up resources...\")\n",
    "\n",
    "# Cache cleanup\n",
    "spark.catalog.clearCache()\n",
    "print(\"‚úÖ Spark cache cleared\")\n",
    "\n",
    "# Unpersist any cached DataFrames\n",
    "try:\n",
    "    if 'customer_360_segmented' in locals():\n",
    "        customer_360_segmented.unpersist()\n",
    "    if 'risk_profile' in locals():\n",
    "        risk_profile.unpersist()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "print(\"‚úÖ DataFrames unpersisted\")\n",
    "\n",
    "# Performance monitoring summary\n",
    "print(f\"\\nüìä Lab Performance Summary:\")\n",
    "print(f\"   Total benchmarks run: {len(performance_monitor.results)}\")\n",
    "\n",
    "if performance_monitor.results:\n",
    "    avg_time = np.mean([r[\"execution_time\"] for r in performance_monitor.results if r[\"success\"]])\n",
    "    print(f\"   Average execution time: {avg_time:.4f}s\")\n",
    "    \n",
    "    successful_runs = len([r for r in performance_monitor.results if r[\"success\"]])\n",
    "    print(f\"   Successful operations: {successful_runs}/{len(performance_monitor.results)}\")\n",
    "\n",
    "print(f\"\\nüéì Lab 01.2 Complete!\")\n",
    "print(f\"   ‚úÖ Multi-format data processing\")\n",
    "print(f\"   ‚úÖ Performance analysis across formats\")\n",
    "print(f\"   ‚úÖ Banking data integration patterns\")\n",
    "print(f\"   ‚úÖ Text analytics fundamentals\")\n",
    "\n",
    "print(f\"\\nüîÑ Ready for Lab 01.3: Data Quality Assessment\")\n",
    "\n",
    "# Keep Spark session running for next lab\n",
    "# spark.stop()  # Uncomment if you want to stop Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca6639d",
   "metadata": {},
   "source": [
    "# Lab 01.2: Data Variety - Multiple Formats in Banking\n",
    "## Banking Analytics Workshop - The 3rd V of Big Data\n",
    "\n",
    "**Duration:** 45 minutes  \n",
    "**Learning Objectives:**\n",
    "- Understand data variety challenges in banking\n",
    "- Work with structured, semi-structured, and unstructured data\n",
    "- Demonstrate schema-on-read vs schema-on-write\n",
    "- Integrate multiple data sources for comprehensive analytics\n",
    "\n",
    "### üìä Banking Data Variety Reality\n",
    "\n",
    "**Modern banking data comes in many formats:**\n",
    "- **Structured (20%):** Core banking transactions, customer tables\n",
    "- **Semi-structured (30%):** JSON APIs, XML messages, log files\n",
    "- **Unstructured (50%):** Emails, chat logs, documents, social media\n",
    "\n",
    "Let's explore how Big Data tools handle this variety!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
