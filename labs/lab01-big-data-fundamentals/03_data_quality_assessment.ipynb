{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2545d035",
   "metadata": {},
   "source": [
    "# Lab 01.3: Data Quality Assessment & Cleansing\n",
    "## Big Data Analytics Workshop - Banking Data Integrity\n",
    "\n",
    "### üéØ **Learning Objectives**\n",
    "After completing this lab, you will understand:\n",
    "- Data quality dimensions and their importance in banking\n",
    "- Common data quality issues in financial datasets\n",
    "- Spark-based data profiling and quality assessment techniques\n",
    "- Automated data cleansing and validation strategies\n",
    "- Regulatory compliance through data quality management\n",
    "\n",
    "### üè¶ **Banking Context: Why Data Quality Matters**\n",
    "In banking, data quality is critical for:\n",
    "- **Regulatory Compliance**: Basel III, GDPR, PSD2 requirements\n",
    "- **Risk Management**: Accurate credit scoring and fraud detection\n",
    "- **Customer Experience**: Personalized services and recommendations\n",
    "- **Operational Efficiency**: Automated decision-making systems\n",
    "\n",
    "Poor data quality costs banks an average of **$15M annually**!\n",
    "\n",
    "### üîç **Data Quality Dimensions**\n",
    "- **Completeness**: Are all required fields populated?\n",
    "- **Accuracy**: Is the data correct and valid?\n",
    "- **Consistency**: Is data uniform across systems?\n",
    "- **Timeliness**: Is the data current and up-to-date?\n",
    "- **Uniqueness**: Are there duplicate records?\n",
    "- **Validity**: Does data conform to business rules?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1562b1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment Setup and Imports\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add utils to path\n",
    "sys.path.append('../utils')\n",
    "from banking_data_generator import BankingDataGenerator\n",
    "from performance_monitor import PerformanceMonitor\n",
    "from banking_analytics import BankingAnalytics\n",
    "\n",
    "print(\"üîç Lab 01.3: Data Quality Assessment & Cleansing\")\n",
    "print(\"=\" * 60)\n",
    "print(\"üéØ Focus: Banking Data Integrity & Validation\")\n",
    "print(\"üìä Skills: Profiling, Cleansing, Validation\")\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
