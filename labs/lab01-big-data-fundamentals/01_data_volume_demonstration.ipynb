{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca4e2ddd",
   "metadata": {},
   "source": [
    "# Lab 01: Big Data Volume Demonstration\n",
    "## Banking Analytics Workshop - Data Volume Challenges\n",
    "\n",
    "**Duration:** 90 minutes  \n",
    "**Learning Objectives:**\n",
    "- Understand the Volume challenge in Big Data (5 V's)\n",
    "- Experience performance differences between traditional and distributed processing\n",
    "- Work with realistic banking transaction volumes\n",
    "- Explore memory limitations and scaling solutions\n",
    "\n",
    "### üìä Real Banking Volume Context\n",
    "\n",
    "**Industry Examples:**\n",
    "- **Deutsche Bank:** ~1 Billion transactions/day\n",
    "- **PayPal:** 19 million transactions/day  \n",
    "- **Visa:** 65,000 transactions/second\n",
    "- **Our Lab:** 1M+ synthetic banking transactions\n",
    "\n",
    "Let's explore how traditional tools break down and Big Data solutions scale!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d0211b",
   "metadata": {},
   "source": [
    "## üöÄ Environment Setup and Data Loading\n",
    "\n",
    "First, let's set up our environment and understand the tools we'll be working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788f1f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essential imports for Big Data processing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import time\n",
    "import psutil\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "from faker import Faker\n",
    "\n",
    "# PySpark imports\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import findspark\n",
    "\n",
    "# Initialize findspark to locate Spark installation\n",
    "findspark.init()\n",
    "\n",
    "print(\"‚úÖ All imports successful!\")\n",
    "print(f\"üìä Pandas version: {pd.__version__}\")\n",
    "print(f\"üî• NumPy version: {np.__version__}\")\n",
    "\n",
    "# Check system resources\n",
    "memory_gb = psutil.virtual_memory().total / (1024**3)\n",
    "cpu_cores = psutil.cpu_count()\n",
    "print(f\"üíª System Memory: {memory_gb:.1f} GB\")\n",
    "print(f\"‚ö° CPU Cores: {cpu_cores}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d1edf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure Spark Session for optimal performance\n",
    "def create_spark_session():\n",
    "    \"\"\"Create optimally configured Spark session for banking analytics\"\"\"\n",
    "    \n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"Banking-BigData-Volume-Lab\") \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .config(\"spark.driver.memory\", \"4g\") \\\n",
    "        .config(\"spark.executor.memory\", \"2g\") \\\n",
    "        .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "        .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "        .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "        .config(\"spark.sql.adaptive.advisoryPartitionSizeInBytes\", \"128MB\") \\\n",
    "        .getOrCreate()\n",
    "    \n",
    "    # Set log level to reduce noise\n",
    "    spark.sparkContext.setLogLevel(\"WARN\")\n",
    "    \n",
    "    return spark\n",
    "\n",
    "# Create Spark session\n",
    "spark = create_spark_session()\n",
    "\n",
    "print(\"üî• Spark Session Created Successfully!\")\n",
    "print(f\"üìä Spark Version: {spark.version}\")\n",
    "print(f\"‚ö° Available Cores: {spark.sparkContext.defaultParallelism}\")\n",
    "print(f\"üíæ Driver Memory: {spark.conf.get('spark.driver.memory')}\")\n",
    "\n",
    "# Display Spark UI URL for monitoring\n",
    "print(f\"üåê Spark Web UI: {spark.sparkContext.uiWebUrl}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81ce26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic banking data for volume demonstration\n",
    "fake = Faker()\n",
    "Faker.seed(42)  # For reproducible results\n",
    "\n",
    "def generate_banking_transactions(num_records):\n",
    "    \"\"\"\n",
    "    Generate synthetic banking transaction data\n",
    "    \n",
    "    Args:\n",
    "        num_records (int): Number of transaction records to generate\n",
    "    \n",
    "    Returns:\n",
    "        list: List of dictionaries containing transaction data\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"üè¶ Generating {num_records:,} synthetic banking transactions...\")\n",
    "    \n",
    "    # Banking-specific data pools\n",
    "    merchants = [\n",
    "        \"REWE Supermarket\", \"Amazon\", \"Deutsche Bahn\", \"Shell Tankstelle\",\n",
    "        \"MediaMarkt\", \"H&M Fashion\", \"McDonald's\", \"Sparkasse ATM\",\n",
    "        \"Edeka\", \"Zalando\", \"Netflix\", \"Spotify\", \"Apple Store\",\n",
    "        \"Google Play\", \"Vodafone\", \"BMW Service\", \"IKEA\", \"Booking.com\"\n",
    "    ]\n",
    "    \n",
    "    merchant_categories = [\n",
    "        \"Grocery\", \"Online Shopping\", \"Transport\", \"Gas Station\",\n",
    "        \"Electronics\", \"Fashion\", \"Fast Food\", \"ATM Withdrawal\",\n",
    "        \"Grocery\", \"Online Fashion\", \"Streaming\", \"Music\",\n",
    "        \"Technology\", \"Apps\", \"Telecommunications\", \"Automotive\",\n",
    "        \"Home & Garden\", \"Travel\"\n",
    "    ]\n",
    "    \n",
    "    channels = [\"card\", \"online\", \"mobile\", \"atm\", \"bank_transfer\"]\n",
    "    currencies = [\"EUR\", \"USD\", \"GBP\"]\n",
    "    countries = [\"DE\", \"US\", \"GB\", \"FR\", \"IT\", \"ES\", \"NL\"]\n",
    "    \n",
    "    transactions = []\n",
    "    customer_ids = [f\"CUST_{i:08d}\" for i in range(1, min(100000, num_records//10) + 1)]\n",
    "    \n",
    "    for i in range(num_records):\n",
    "        # Realistic transaction patterns\n",
    "        if i < num_records * 0.8:  # 80% normal transactions\n",
    "            amount = round(fake.random.uniform(5.0, 500.0), 2)\n",
    "        elif i < num_records * 0.95:  # 15% higher value transactions\n",
    "            amount = round(fake.random.uniform(500.0, 5000.0), 2)\n",
    "        else:  # 5% very high value transactions\n",
    "            amount = round(fake.random.uniform(5000.0, 50000.0), 2)\n",
    "        \n",
    "        merchant_idx = fake.random.randint(0, len(merchants) - 1)\n",
    "        \n",
    "        # Create transaction record\n",
    "        transaction = {\n",
    "            \"transaction_id\": f\"TXN_{fake.year()}_{i+1:08d}\",\n",
    "            \"customer_id\": fake.random.choice(customer_ids),\n",
    "            \"amount\": amount,\n",
    "            \"currency\": fake.random.choice(currencies),\n",
    "            \"merchant\": merchants[merchant_idx],\n",
    "            \"merchant_category\": merchant_categories[merchant_idx],\n",
    "            \"transaction_type\": fake.random.choice([\"purchase\", \"withdrawal\", \"transfer\", \"payment\"]),\n",
    "            \"channel\": fake.random.choice(channels),\n",
    "            \"timestamp\": fake.date_time_between(start_date='-2y', end_date='now'),\n",
    "            \"location_country\": fake.random.choice(countries),\n",
    "            \"location_city\": fake.city(),\n",
    "            \"is_weekend\": fake.random.choice([True, False]),\n",
    "            \"description\": f\"Transaction at {merchants[merchant_idx]}\"\n",
    "        }\n",
    "        \n",
    "        transactions.append(transaction)\n",
    "        \n",
    "        # Progress indicator\n",
    "        if (i + 1) % 100000 == 0:\n",
    "            print(f\"   Generated {i+1:,} transactions...\")\n",
    "    \n",
    "    print(f\"‚úÖ Generated {len(transactions):,} transactions successfully!\")\n",
    "    return transactions\n",
    "\n",
    "# Start with smaller dataset for comparison\n",
    "print(\"Creating datasets of different sizes for comparison...\")\n",
    "small_transactions = generate_banking_transactions(10000)  # 10K\n",
    "print(\"Small dataset created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4975986",
   "metadata": {},
   "source": [
    "## üìä Big Data Fundamentals: Pandas vs Spark Comparison\n",
    "\n",
    "Now let's experience the fundamental difference between traditional data processing (Pandas) and Big Data processing (Spark). We'll analyze the same banking data using both approaches and measure performance differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c6bd5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def performance_benchmark(func, *args, **kwargs):\n",
    "    \"\"\"Benchmark function execution time and memory usage\"\"\"\n",
    "    \n",
    "    # Memory before execution\n",
    "    process = psutil.Process()\n",
    "    mem_before = process.memory_info().rss / 1024 / 1024  # MB\n",
    "    \n",
    "    # Time execution\n",
    "    start_time = time.time()\n",
    "    result = func(*args, **kwargs)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    # Memory after execution\n",
    "    mem_after = process.memory_info().rss / 1024 / 1024  # MB\n",
    "    \n",
    "    execution_time = end_time - start_time\n",
    "    memory_used = mem_after - mem_before\n",
    "    \n",
    "    return result, execution_time, memory_used\n",
    "\n",
    "def analyze_transactions_pandas(transactions_data):\n",
    "    \"\"\"Traditional pandas-based analysis of banking transactions\"\"\"\n",
    "    \n",
    "    print(\"üêº Starting Pandas Analysis...\")\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(transactions_data)\n",
    "    \n",
    "    # Convert timestamp to datetime\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    df['date'] = df['timestamp'].dt.date\n",
    "    df['hour'] = df['timestamp'].dt.hour\n",
    "    \n",
    "    # Banking analytics queries\n",
    "    results = {}\n",
    "    \n",
    "    # 1. Daily transaction volume\n",
    "    daily_volume = df.groupby('date').agg({\n",
    "        'transaction_id': 'count',\n",
    "        'amount': ['sum', 'mean']\n",
    "    }).reset_index()\n",
    "    results['daily_volume'] = daily_volume.shape[0]\n",
    "    \n",
    "    # 2. Customer transaction patterns\n",
    "    customer_patterns = df.groupby('customer_id').agg({\n",
    "        'amount': ['sum', 'mean', 'count'],\n",
    "        'merchant_category': 'nunique'\n",
    "    }).reset_index()\n",
    "    results['customers'] = customer_patterns.shape[0]\n",
    "    \n",
    "    # 3. Fraud indicators (high-value, unusual patterns)\n",
    "    fraud_indicators = df[\n",
    "        (df['amount'] > df['amount'].quantile(0.95)) |\n",
    "        (df['hour'].isin([0, 1, 2, 3, 4, 5]))\n",
    "    ]\n",
    "    results['fraud_indicators'] = fraud_indicators.shape[0]\n",
    "    \n",
    "    # 4. Merchant analysis\n",
    "    merchant_stats = df.groupby(['merchant', 'merchant_category']).agg({\n",
    "        'amount': ['sum', 'count', 'mean'],\n",
    "        'customer_id': 'nunique'\n",
    "    }).reset_index()\n",
    "    results['merchants'] = merchant_stats.shape[0]\n",
    "    \n",
    "    print(f\"   ‚úÖ Processed {len(df):,} transactions\")\n",
    "    print(f\"   üìà Generated {len(results)} analytical views\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def analyze_transactions_spark(spark_session, transactions_data):\n",
    "    \"\"\"Distributed Spark-based analysis of banking transactions\"\"\"\n",
    "    \n",
    "    print(\"üî• Starting Spark Analysis...\")\n",
    "    \n",
    "    # Create DataFrame from data\n",
    "    df = spark_session.createDataFrame(transactions_data)\n",
    "    \n",
    "    # Convert timestamp and add derived columns\n",
    "    df = df.withColumn(\"timestamp\", to_timestamp(col(\"timestamp\"))) \\\n",
    "          .withColumn(\"date\", to_date(col(\"timestamp\"))) \\\n",
    "          .withColumn(\"hour\", hour(col(\"timestamp\")))\n",
    "    \n",
    "    # Cache for multiple operations\n",
    "    df.cache()\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # 1. Daily transaction volume using Spark SQL\n",
    "    df.createOrReplaceTempView(\"transactions\")\n",
    "    daily_volume = spark_session.sql(\"\"\"\n",
    "        SELECT date,\n",
    "               COUNT(*) as transaction_count,\n",
    "               SUM(amount) as total_amount,\n",
    "               AVG(amount) as avg_amount\n",
    "        FROM transactions\n",
    "        GROUP BY date\n",
    "    \"\"\")\n",
    "    results['daily_volume'] = daily_volume.count()\n",
    "    \n",
    "    # 2. Customer transaction patterns\n",
    "    customer_patterns = spark_session.sql(\"\"\"\n",
    "        SELECT customer_id,\n",
    "               SUM(amount) as total_spent,\n",
    "               AVG(amount) as avg_transaction,\n",
    "               COUNT(*) as transaction_count,\n",
    "               COUNT(DISTINCT merchant_category) as categories_used\n",
    "        FROM transactions\n",
    "        GROUP BY customer_id\n",
    "    \"\"\")\n",
    "    results['customers'] = customer_patterns.count()\n",
    "    \n",
    "    # 3. Fraud indicators using DataFrame API\n",
    "    quantile_95 = df.select(expr(\"percentile_approx(amount, 0.95)\").alias(\"q95\")).collect()[0][\"q95\"]\n",
    "    fraud_indicators = df.filter(\n",
    "        (col(\"amount\") > quantile_95) |\n",
    "        (col(\"hour\").isin([0, 1, 2, 3, 4, 5]))\n",
    "    )\n",
    "    results['fraud_indicators'] = fraud_indicators.count()\n",
    "    \n",
    "    # 4. Merchant analysis\n",
    "    merchant_stats = df.groupBy(\"merchant\", \"merchant_category\").agg(\n",
    "        sum(\"amount\").alias(\"total_amount\"),\n",
    "        count(\"*\").alias(\"transaction_count\"),\n",
    "        avg(\"amount\").alias(\"avg_amount\"),\n",
    "        countDistinct(\"customer_id\").alias(\"unique_customers\")\n",
    "    )\n",
    "    results['merchants'] = merchant_stats.count()\n",
    "    \n",
    "    print(f\"   ‚úÖ Processed {df.count():,} transactions\")\n",
    "    print(f\"   üìà Generated {len(results)} analytical views\")\n",
    "    \n",
    "    # Unpersist cache\n",
    "    df.unpersist()\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run performance comparison\n",
    "print(\"üèÅ Starting Performance Comparison...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Pandas benchmark\n",
    "pandas_result, pandas_time, pandas_memory = performance_benchmark(\n",
    "    analyze_transactions_pandas, small_transactions\n",
    ")\n",
    "\n",
    "print(f\"üêº Pandas Results:\")\n",
    "print(f\"   ‚è±Ô∏è  Execution Time: {pandas_time:.2f} seconds\")\n",
    "print(f\"   üíæ Memory Used: {pandas_memory:.1f} MB\")\n",
    "print()\n",
    "\n",
    "# Spark benchmark\n",
    "spark_result, spark_time, spark_memory = performance_benchmark(\n",
    "    analyze_transactions_spark, spark, small_transactions\n",
    ")\n",
    "\n",
    "print(f\"üî• Spark Results:\")\n",
    "print(f\"   ‚è±Ô∏è  Execution Time: {spark_time:.2f} seconds\")\n",
    "print(f\"   üíæ Memory Used: {spark_memory:.1f} MB\")\n",
    "print()\n",
    "\n",
    "# Performance comparison\n",
    "if pandas_time > 0:\n",
    "    speedup = pandas_time / spark_time\n",
    "    print(f\"üìä Performance Comparison:\")\n",
    "    print(f\"   üöÄ Spark is {speedup:.1f}x {'faster' if speedup > 1 else 'slower'} than Pandas\")\n",
    "    print(f\"   üí° Memory efficiency: {(pandas_memory/spark_memory):.1f}x\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Performance comparison not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8be1943",
   "metadata": {},
   "source": [
    "## üìà Scaling to Real Banking Volumes\n",
    "\n",
    "Now let's experience what happens when we scale up to more realistic banking volumes. We'll generate progressively larger datasets and observe how each system performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f7e8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Volume scaling experiment\n",
    "dataset_sizes = [10_000, 50_000, 100_000, 500_000]  # Start conservative for demo\n",
    "performance_results = []\n",
    "\n",
    "print(\"üéØ Volume Scaling Experiment\")\n",
    "print(\"=\"*50)\n",
    "print(\"Testing both Pandas and Spark with increasing data volumes...\")\n",
    "print()\n",
    "\n",
    "for size in dataset_sizes:\n",
    "    print(f\"üìä Testing with {size:,} transactions...\")\n",
    "    \n",
    "    # Generate dataset of current size\n",
    "    transactions = generate_banking_transactions(size)\n",
    "    \n",
    "    result_dict = {\n",
    "        'dataset_size': size,\n",
    "        'data_size_mb': len(str(transactions)) / 1024 / 1024  # Rough estimate\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Test Pandas\n",
    "        pandas_result, pandas_time, pandas_memory = performance_benchmark(\n",
    "            analyze_transactions_pandas, transactions\n",
    "        )\n",
    "        result_dict['pandas_time'] = pandas_time\n",
    "        result_dict['pandas_memory'] = pandas_memory\n",
    "        result_dict['pandas_success'] = True\n",
    "        print(f\"   üêº Pandas: {pandas_time:.2f}s, {pandas_memory:.1f}MB\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   üêº Pandas: Failed - {str(e)[:50]}...\")\n",
    "        result_dict['pandas_time'] = None\n",
    "        result_dict['pandas_memory'] = None\n",
    "        result_dict['pandas_success'] = False\n",
    "    \n",
    "    try:\n",
    "        # Test Spark\n",
    "        spark_result, spark_time, spark_memory = performance_benchmark(\n",
    "            analyze_transactions_spark, spark, transactions\n",
    "        )\n",
    "        result_dict['spark_time'] = spark_time\n",
    "        result_dict['spark_memory'] = spark_memory\n",
    "        result_dict['spark_success'] = True\n",
    "        print(f\"   üî• Spark: {spark_time:.2f}s, {spark_memory:.1f}MB\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   üî• Spark: Failed - {str(e)[:50]}...\")\n",
    "        result_dict['spark_time'] = None\n",
    "        result_dict['spark_memory'] = None\n",
    "        result_dict['spark_success'] = False\n",
    "    \n",
    "    performance_results.append(result_dict)\n",
    "    print()\n",
    "\n",
    "# Create performance visualization\n",
    "results_df = pd.DataFrame(performance_results)\n",
    "print(\"üìà Performance Results Summary:\")\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e94f3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive performance visualization\n",
    "fig = make_subplots(\n",
    "    rows=2, cols=2,\n",
    "    subplot_titles=('Execution Time vs Dataset Size', 'Memory Usage vs Dataset Size',\n",
    "                   'Performance Comparison', 'Banking Volume Context'),\n",
    "    specs=[[{\"secondary_y\": False}, {\"secondary_y\": False}],\n",
    "           [{\"secondary_y\": False}, {\"secondary_y\": False}]]\n",
    ")\n",
    "\n",
    "# Filter successful results for plotting\n",
    "successful_results = results_df[results_df['pandas_success'] | results_df['spark_success']]\n",
    "\n",
    "if not successful_results.empty:\n",
    "    # Execution Time Comparison\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=successful_results['dataset_size'],\n",
    "            y=successful_results['pandas_time'],\n",
    "            mode='lines+markers',\n",
    "            name='Pandas Time',\n",
    "            line=dict(color='blue', width=2),\n",
    "            marker=dict(size=8)\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=successful_results['dataset_size'],\n",
    "            y=successful_results['spark_time'],\n",
    "            mode='lines+markers',\n",
    "            name='Spark Time',\n",
    "            line=dict(color='red', width=2),\n",
    "            marker=dict(size=8)\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # Memory Usage Comparison\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=successful_results['dataset_size'],\n",
    "            y=successful_results['pandas_memory'],\n",
    "            mode='lines+markers',\n",
    "            name='Pandas Memory',\n",
    "            line=dict(color='lightblue', width=2),\n",
    "            marker=dict(size=8),\n",
    "            showlegend=False\n",
    "        ),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=successful_results['dataset_size'],\n",
    "            y=successful_results['spark_memory'],\n",
    "            mode='lines+markers',\n",
    "            name='Spark Memory',\n",
    "            line=dict(color='lightcoral', width=2),\n",
    "            marker=dict(size=8),\n",
    "            showlegend=False\n",
    "        ),\n",
    "        row=1, col=2\n",
    "    )\n",
    "\n",
    "# Performance efficiency chart\n",
    "if len(successful_results) > 0:\n",
    "    # Calculate efficiency (records/second)\n",
    "    latest_result = successful_results.iloc[-1]\n",
    "    if latest_result['pandas_time'] and latest_result['spark_time']:\n",
    "        pandas_efficiency = latest_result['dataset_size'] / latest_result['pandas_time']\n",
    "        spark_efficiency = latest_result['dataset_size'] / latest_result['spark_time']\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Bar(\n",
    "                x=['Pandas', 'Spark'],\n",
    "                y=[pandas_efficiency, spark_efficiency],\n",
    "                name='Records/Second',\n",
    "                marker_color=['blue', 'red'],\n",
    "                showlegend=False\n",
    "            ),\n",
    "            row=2, col=1\n",
    "        )\n",
    "\n",
    "# Banking industry context\n",
    "banking_volumes = {\n",
    "    'Our Lab': successful_results['dataset_size'].max() if not successful_results.empty else 100_000,\n",
    "    'Small Bank': 1_000_000,\n",
    "    'Regional Bank': 10_000_000,\n",
    "    'PayPal (Daily)': 19_000_000,\n",
    "    'Deutsche Bank (Daily)': 1_000_000_000\n",
    "}\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Bar(\n",
    "        x=list(banking_volumes.keys()),\n",
    "        y=list(banking_volumes.values()),\n",
    "        name='Daily Transactions',\n",
    "        marker_color='green',\n",
    "        showlegend=False\n",
    "    ),\n",
    "    row=2, col=2\n",
    ")\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    height=800,\n",
    "    title_text=\"Big Data Volume Performance Analysis - Banking Context\",\n",
    "    title_x=0.5\n",
    ")\n",
    "\n",
    "# Update axis labels\n",
    "fig.update_xaxes(title_text=\"Dataset Size (Records)\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"Time (Seconds)\", row=1, col=1)\n",
    "fig.update_xaxes(title_text=\"Dataset Size (Records)\", row=1, col=2)\n",
    "fig.update_yaxes(title_text=\"Memory (MB)\", row=1, col=2)\n",
    "fig.update_xaxes(title_text=\"Technology\", row=2, col=1)\n",
    "fig.update_yaxes(title_text=\"Records/Second\", row=2, col=1)\n",
    "fig.update_xaxes(title_text=\"Banking Institution\", row=2, col=2)\n",
    "fig.update_yaxes(title_text=\"Daily Transactions\", row=2, col=2, type=\"log\")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "# Print key insights\n",
    "print(\"\\nüîç Key Insights from Volume Analysis:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "if not successful_results.empty:\n",
    "    max_size = successful_results['dataset_size'].max()\n",
    "    pandas_worked = successful_results['pandas_success'].any()\n",
    "    spark_worked = successful_results['spark_success'].any()\n",
    "    \n",
    "    print(f\"‚úÖ Maximum dataset size processed: {max_size:,} records\")\n",
    "    print(f\"üêº Pandas handled: {'‚úÖ' if pandas_worked else '‚ùå'}\")\n",
    "    print(f\"üî• Spark handled: {'‚úÖ' if spark_worked else '‚ùå'}\")\n",
    "    \n",
    "    if pandas_worked and spark_worked:\n",
    "        # Compare efficiency at largest common size\n",
    "        last_common = successful_results[\n",
    "            successful_results['pandas_success'] & successful_results['spark_success']\n",
    "        ].iloc[-1] if any(successful_results['pandas_success'] & successful_results['spark_success']) else None\n",
    "        \n",
    "        if last_common is not None:\n",
    "            pandas_rate = last_common['dataset_size'] / last_common['pandas_time']\n",
    "            spark_rate = last_common['dataset_size'] / last_common['spark_time']\n",
    "            print(f\"‚ö° Pandas processing rate: {pandas_rate:,.0f} records/second\")\n",
    "            print(f\"üöÄ Spark processing rate: {spark_rate:,.0f} records/second\")\n",
    "            \n",
    "            if spark_rate > pandas_rate:\n",
    "                print(f\"üèÜ Spark is {spark_rate/pandas_rate:.1f}x faster for processing\")\n",
    "            else:\n",
    "                print(f\"üèÜ Pandas is {pandas_rate/spark_rate:.1f}x faster for processing\")\n",
    "\n",
    "print(\"\\nüí° Real-World Banking Context:\")\n",
    "print(f\"üè¶ A major bank processes ~1B transactions/day\")\n",
    "print(f\"üìà At current rates, that would take:\")\n",
    "if not successful_results.empty and successful_results['spark_success'].any():\n",
    "    spark_rate = successful_results[successful_results['spark_success']].iloc[-1]['dataset_size'] / \\\n",
    "                 successful_results[successful_results['spark_success']].iloc[-1]['spark_time']\n",
    "    days_needed = 1_000_000_000 / spark_rate / 86400  # Convert to days\n",
    "    print(f\"   üî• Spark: {days_needed:.1f} days (needs distributed cluster!)\")\n",
    "else:\n",
    "    print(\"   üî• Spark: Unable to estimate (processing failed)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867f9089",
   "metadata": {},
   "source": [
    "## üèóÔ∏è Hadoop Ecosystem Demo\n",
    "\n",
    "Let's explore the foundational concepts of the Hadoop ecosystem, including distributed file storage concepts and the MapReduce programming paradigm applied to banking data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3dae7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate MapReduce paradigm with banking fraud detection\n",
    "def mapreduce_fraud_detection_demo(transactions_data):\n",
    "    \"\"\"\n",
    "    Demonstrate MapReduce thinking for fraud detection\n",
    "    \n",
    "    MapReduce Pattern:\n",
    "    1. MAP: Extract key-value pairs from each transaction\n",
    "    2. SHUFFLE & SORT: Group by key\n",
    "    3. REDUCE: Aggregate values for each key\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"üó∫Ô∏è  MapReduce Fraud Detection Demo\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # STEP 1: MAP PHASE\n",
    "    print(\"1Ô∏è‚É£ MAP Phase: Extract suspicious patterns from each transaction\")\n",
    "    \n",
    "    mapped_data = []\n",
    "    \n",
    "    for transaction in transactions_data[:1000]:  # Demo with first 1000\n",
    "        # Map function: identify potential fraud indicators\n",
    "        amount = transaction['amount']\n",
    "        hour = pd.to_datetime(transaction['timestamp']).hour\n",
    "        \n",
    "        # Fraud indicators as key-value pairs\n",
    "        if amount > 1000:  # High value transaction\n",
    "            mapped_data.append((\"high_value_fraud\", 1))\n",
    "        \n",
    "        if hour in [0, 1, 2, 3, 4, 5]:  # Late night transaction\n",
    "            mapped_data.append((\"late_night_fraud\", 1))\n",
    "        \n",
    "        if transaction['channel'] == 'atm' and amount > 500:\n",
    "            mapped_data.append((\"suspicious_atm\", 1))\n",
    "        \n",
    "        # Normal transaction counter\n",
    "        mapped_data.append((\"normal_transaction\", 1))\n",
    "    \n",
    "    print(f\"   üìä Generated {len(mapped_data)} key-value pairs\")\n",
    "    print(f\"   üîç Sample mappings: {mapped_data[:10]}\")\n",
    "    \n",
    "    # STEP 2: SHUFFLE & SORT PHASE\n",
    "    print(\"\\n2Ô∏è‚É£ SHUFFLE & SORT Phase: Group by fraud type\")\n",
    "    \n",
    "    from collections import defaultdict\n",
    "    grouped_data = defaultdict(list)\n",
    "    \n",
    "    for key, value in mapped_data:\n",
    "        grouped_data[key].append(value)\n",
    "    \n",
    "    print(f\"   üìö Grouped into {len(grouped_data)} categories\")\n",
    "    for key, values in grouped_data.items():\n",
    "        print(f\"   üè∑Ô∏è  {key}: {len(values)} occurrences\")\n",
    "    \n",
    "    # STEP 3: REDUCE PHASE\n",
    "    print(\"\\n3Ô∏è‚É£ REDUCE Phase: Calculate final fraud statistics\")\n",
    "    \n",
    "    fraud_summary = {}\n",
    "    for key, values in grouped_data.items():\n",
    "        fraud_summary[key] = sum(values)\n",
    "    \n",
    "    print(f\"   üìà Fraud Detection Results:\")\n",
    "    for fraud_type, count in fraud_summary.items():\n",
    "        percentage = (count / len(transactions_data[:1000])) * 100\n",
    "        print(f\"      {fraud_type}: {count} ({percentage:.1f}%)\")\n",
    "    \n",
    "    return fraud_summary\n",
    "\n",
    "# Run MapReduce demo\n",
    "fraud_results = mapreduce_fraud_detection_demo(small_transactions)\n",
    "\n",
    "# Visualize MapReduce results\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 6))\n",
    "\n",
    "fraud_types = list(fraud_results.keys())\n",
    "fraud_counts = list(fraud_results.values())\n",
    "\n",
    "colors = ['red' if 'fraud' in ft or 'suspicious' in ft else 'green' for ft in fraud_types]\n",
    "\n",
    "bars = ax.bar(fraud_types, fraud_counts, color=colors, alpha=0.7)\n",
    "ax.set_title('MapReduce Fraud Detection Results\\nDistributed Pattern Recognition', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Fraud Pattern Type')\n",
    "ax.set_ylabel('Number of Occurrences')\n",
    "ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height + 0.5,\n",
    "           f'{int(height)}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° MapReduce Key Concepts Demonstrated:\")\n",
    "print(\"‚úÖ Distributed processing: Each transaction processed independently\")\n",
    "print(\"‚úÖ Scalability: Same logic works for millions of transactions\")\n",
    "print(\"‚úÖ Fault tolerance: If one node fails, others continue\")\n",
    "print(\"‚úÖ Data locality: Processing happens where data is stored\")\n",
    "\n",
    "# Compare with traditional approach\n",
    "print(\"\\n‚öñÔ∏è Traditional vs MapReduce Approach:\")\n",
    "print(\"üêå Traditional: Process all data on single machine\")\n",
    "print(\"üöÄ MapReduce: Distribute processing across cluster\")\n",
    "print(\"üíæ Traditional: Limited by single machine memory\")\n",
    "print(\"üåê MapReduce: Scales with cluster size\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a3d85a",
   "metadata": {},
   "source": [
    "## üî• Apache Spark Core Concepts\n",
    "\n",
    "Now let's dive deep into Spark's core concepts: RDDs, DataFrames, lazy evaluation, and in-memory computing. We'll demonstrate these with practical banking examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a4106c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate Spark Core Concepts with Banking Data\n",
    "\n",
    "print(\"üî• Apache Spark Core Concepts Demo\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# 1. RDDs (Resilient Distributed Datasets) - The Foundation\n",
    "print(\"\\n1Ô∏è‚É£ RDDs: Resilient Distributed Datasets\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Create RDD from transaction data\n",
    "transaction_amounts = [t['amount'] for t in small_transactions[:1000]]\n",
    "amounts_rdd = spark.sparkContext.parallelize(transaction_amounts)\n",
    "\n",
    "print(f\"‚úÖ Created RDD with {amounts_rdd.count()} transaction amounts\")\n",
    "print(f\"üìä Partitions: {amounts_rdd.getNumPartitions()}\")\n",
    "print(f\"üéØ Sample amounts: {amounts_rdd.take(5)}\")\n",
    "\n",
    "# RDD transformations (lazy evaluation)\n",
    "high_value_rdd = amounts_rdd.filter(lambda x: x > 100)\n",
    "squared_amounts_rdd = high_value_rdd.map(lambda x: x * x)\n",
    "\n",
    "print(f\"\\nüîÑ RDD Transformations (Lazy):\")\n",
    "print(f\"   üìà High value transactions: {high_value_rdd.count()}\")\n",
    "print(f\"   ‚ö° Squared amounts sample: {squared_amounts_rdd.take(3)}\")\n",
    "\n",
    "# RDD actions (trigger execution)\n",
    "total_high_value = high_value_rdd.reduce(lambda x, y: x + y)\n",
    "print(f\"   üí∞ Total high value amount: ‚Ç¨{total_high_value:,.2f}\")\n",
    "\n",
    "# 2. DataFrames - Structured Data Processing\n",
    "print(\"\\n2Ô∏è‚É£ DataFrames: Structured Data with Schema\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "# Create DataFrame from transactions\n",
    "df_transactions = spark.createDataFrame(small_transactions[:1000])\n",
    "\n",
    "# DataFrame schema\n",
    "print(\"üìã DataFrame Schema:\")\n",
    "df_transactions.printSchema()\n",
    "\n",
    "# DataFrame operations\n",
    "print(\"\\nüîç DataFrame Analysis:\")\n",
    "print(f\"   üìä Total rows: {df_transactions.count()}\")\n",
    "print(f\"   üìà Columns: {len(df_transactions.columns)}\")\n",
    "\n",
    "# Statistical summary\n",
    "print(\"\\nüìà Statistical Summary of Transaction Amounts:\")\n",
    "df_transactions.select(\"amount\").describe().show()\n",
    "\n",
    "# 3. Lazy Evaluation Demonstration\n",
    "print(\"\\n3Ô∏è‚É£ Lazy Evaluation: Building Execution Plan\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "# Build complex transformation chain (no execution yet)\n",
    "processed_df = df_transactions \\\n",
    "    .filter(col(\"amount\") > 50) \\\n",
    "    .withColumn(\"amount_category\", \n",
    "               when(col(\"amount\") < 100, \"small\")\n",
    "               .when(col(\"amount\") < 500, \"medium\")\n",
    "               .otherwise(\"large\")) \\\n",
    "    .groupBy(\"merchant_category\", \"amount_category\") \\\n",
    "    .agg(count(\"*\").alias(\"transaction_count\"),\n",
    "         avg(\"amount\").alias(\"avg_amount\")) \\\n",
    "    .orderBy(\"merchant_category\", \"amount_category\")\n",
    "\n",
    "print(\"üîÑ Transformation chain created (not executed yet)\")\n",
    "print(\"üìã Execution plan:\")\n",
    "processed_df.explain(True)\n",
    "\n",
    "# Trigger execution\n",
    "print(\"\\nüöÄ Triggering execution with .show():\")\n",
    "processed_df.show(10, truncate=False)\n",
    "\n",
    "# 4. In-Memory Computing with Caching\n",
    "print(\"\\n4Ô∏è‚É£ In-Memory Computing: Caching for Performance\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Create a DataFrame that will be used multiple times\n",
    "active_df = df_transactions.filter(col(\"amount\") > 10)\n",
    "\n",
    "# Time without caching\n",
    "start_time = time.time()\n",
    "count1 = active_df.count()\n",
    "avg_amount1 = active_df.select(avg(\"amount\")).collect()[0][0]\n",
    "max_amount1 = active_df.select(max(\"amount\")).collect()[0][0]\n",
    "time_without_cache = time.time() - start_time\n",
    "\n",
    "print(f\"‚è±Ô∏è Operations without cache: {time_without_cache:.3f} seconds\")\n",
    "\n",
    "# Cache the DataFrame in memory\n",
    "active_df.cache()\n",
    "print(\"üíæ DataFrame cached in memory\")\n",
    "\n",
    "# Time with caching (first operation will still be slow due to caching)\n",
    "start_time = time.time()\n",
    "count2 = active_df.count()  # This triggers caching\n",
    "cache_build_time = time.time() - start_time\n",
    "\n",
    "# Subsequent operations should be faster\n",
    "start_time = time.time()\n",
    "avg_amount2 = active_df.select(avg(\"amount\")).collect()[0][0]\n",
    "max_amount2 = active_df.select(max(\"amount\")).collect()[0][0]\n",
    "time_with_cache = time.time() - start_time\n",
    "\n",
    "print(f\"‚è±Ô∏è Cache building time: {cache_build_time:.3f} seconds\")\n",
    "print(f\"‚ö° Operations with cache: {time_with_cache:.3f} seconds\")\n",
    "\n",
    "if time_without_cache > 0:\n",
    "    speedup = time_without_cache / max(time_with_cache, 0.001)\n",
    "    print(f\"üöÄ Speedup from caching: {speedup:.1f}x\")\n",
    "\n",
    "# Show cache statistics\n",
    "print(f\"\\nüìä Results verification:\")\n",
    "print(f\"   Count: {count1} vs {count2} ({'‚úÖ' if count1 == count2 else '‚ùå'})\")\n",
    "print(f\"   Avg amount: ‚Ç¨{avg_amount1:.2f} vs ‚Ç¨{avg_amount2:.2f}\")\n",
    "print(f\"   Max amount: ‚Ç¨{max_amount1:.2f} vs ‚Ç¨{max_amount2:.2f}\")\n",
    "\n",
    "# Clean up cache\n",
    "active_df.unpersist()\n",
    "print(\"üßπ Cache cleared\")\n",
    "\n",
    "# 5. Spark SQL Integration\n",
    "print(\"\\n5Ô∏è‚É£ Spark SQL: SQL Interface to DataFrames\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "# Register DataFrame as SQL table\n",
    "df_transactions.createOrReplaceTempView(\"banking_transactions\")\n",
    "\n",
    "# Use SQL to query the data\n",
    "sql_result = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        merchant_category,\n",
    "        COUNT(*) as transaction_count,\n",
    "        AVG(amount) as avg_amount,\n",
    "        MAX(amount) as max_amount,\n",
    "        MIN(amount) as min_amount\n",
    "    FROM banking_transactions\n",
    "    WHERE amount > 20\n",
    "    GROUP BY merchant_category\n",
    "    ORDER BY transaction_count DESC\n",
    "    LIMIT 5\n",
    "\"\"\")\n",
    "\n",
    "print(\"üíª SQL Query Results:\")\n",
    "sql_result.show(truncate=False)\n",
    "\n",
    "# 6. Partitioning and Distribution\n",
    "print(\"\\n6Ô∏è‚É£ Data Partitioning: Distribution Strategy\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "print(f\"üìä Original partitions: {df_transactions.rdd.getNumPartitions()}\")\n",
    "\n",
    "# Repartition data\n",
    "repartitioned_df = df_transactions.repartition(4, col(\"merchant_category\"))\n",
    "print(f\"üîÑ After repartitioning: {repartitioned_df.rdd.getNumPartitions()}\")\n",
    "\n",
    "# Show partition distribution\n",
    "partition_info = repartitioned_df.rdd.mapPartitionsWithIndex(\n",
    "    lambda idx, iterator: [(idx, len(list(iterator)))]\n",
    ").collect()\n",
    "\n",
    "print(\"üìà Records per partition:\")\n",
    "for partition_id, record_count in partition_info:\n",
    "    print(f\"   Partition {partition_id}: {record_count} records\")\n",
    "\n",
    "print(\"\\nüéØ Key Spark Concepts Demonstrated:\")\n",
    "print(\"‚úÖ RDDs: Low-level distributed data structures\")\n",
    "print(\"‚úÖ DataFrames: Structured data with schema optimization\")\n",
    "print(\"‚úÖ Lazy Evaluation: Builds execution plan before running\")\n",
    "print(\"‚úÖ In-Memory Computing: Caching for iterative algorithms\")\n",
    "print(\"‚úÖ Spark SQL: SQL interface for familiar querying\")\n",
    "print(\"‚úÖ Partitioning: Data distribution across cluster\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cdbd39f",
   "metadata": {},
   "source": [
    "## üéØ Lab 01 Summary & Key Takeaways\n",
    "\n",
    "### What We've Learned\n",
    "\n",
    "**Big Data Volume Challenges:**\n",
    "- Traditional tools (Pandas) have memory limitations (~8GB RAM)\n",
    "- Real banking volumes (1B+ transactions/day) require distributed solutions\n",
    "- Performance degrades exponentially with traditional approaches\n",
    "\n",
    "**Technology Comparison:**\n",
    "- **Pandas:** Great for small-medium datasets, familiar interface, single-machine limited\n",
    "- **Spark:** Designed for Big Data, distributed processing, in-memory computing, unified platform\n",
    "\n",
    "**Hadoop Ecosystem:**\n",
    "- **MapReduce:** Foundational distributed computing paradigm\n",
    "- **HDFS:** Distributed file system for massive datasets\n",
    "- **Fault Tolerance:** Automatic recovery from hardware failures\n",
    "\n",
    "**Spark Advantages:**\n",
    "- **RDDs:** Fault-tolerant distributed datasets\n",
    "- **DataFrames:** Structured data with schema optimization\n",
    "- **Lazy Evaluation:** Optimized execution plans\n",
    "- **In-Memory Computing:** 10-100x faster than disk-based processing\n",
    "- **Unified Platform:** Batch, streaming, ML, and graph processing\n",
    "\n",
    "### Real-World Banking Applications\n",
    "\n",
    "1. **Fraud Detection:** Process millions of transactions in real-time\n",
    "2. **Customer Analytics:** Segment customers using behavioral patterns\n",
    "3. **Risk Assessment:** Monte Carlo simulations with billions of scenarios\n",
    "4. **Regulatory Reporting:** Process historical data for compliance\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- **Lab 02:** Dive deeper into Spark SQL for complex analytics\n",
    "- **Lab 03:** Build real-time fraud detection systems\n",
    "- **Lab 04:** Customer segmentation with machine learning\n",
    "- **Lab 05:** Data integration and ETL pipelines\n",
    "\n",
    "### üîß Clean Up Resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd7d9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up Spark resources\n",
    "print(\"üßπ Cleaning up resources...\")\n",
    "\n",
    "# Stop Spark session\n",
    "spark.stop()\n",
    "\n",
    "print(\"‚úÖ Spark session stopped\")\n",
    "print(\"üí° Lab 01 completed successfully!\")\n",
    "\n",
    "print(\"\\nüìö Additional Resources:\")\n",
    "print(\"   üìñ Spark Documentation: https://spark.apache.org/docs/latest/\")\n",
    "print(\"   üéì Banking Analytics Best Practices: See Day 2 materials\")\n",
    "print(\"   üîß Performance Tuning: Check Spark UI during exercises\")\n",
    "print(\"\\nüéâ Ready for Lab 02: Spark SQL for Banking Analytics!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
